{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "B_PZKEtmCUSc"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Sxb2DaDmUB60"
   },
   "outputs": [],
   "source": [
    "!wget -q https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vB7IxU0zUDOu"
   },
   "outputs": [],
   "source": [
    "!tar xf spark-3.3.1-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tC-zB8OiUEU3"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rhktzC10UFk2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I2CNGXdtUG4F"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OV5NEzRbUH98",
    "outputId": "bfc6bd17-1402-40b3-bee2-6d1247debb84"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QJrYiDDzUJ3b"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"Thesis\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "mrvGoUWCUL0r",
    "outputId": "36b4fb02-41fa-40a6-a920-368871b424e7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.10.84:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f37e4cadd60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Uy6RuNHKSsF",
    "outputId": "2115cd8d-3c80-47c1-addf-6d7243b729f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 4.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 41.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 65.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s17AcUhMKT6M",
    "outputId": "8fe8945a-6d9d-4824-cc9a-1b41d0d312a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy) (0.2.5)\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.1.1\n"
     ]
    }
   ],
   "source": [
    "# thu vien chinh sua cac loi unicode\n",
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZubCjaEGXYg",
    "outputId": "b2a1a837-0a4f-4938-88ca-412df9da1eea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "import string    \n",
    "import random\n",
    "import json\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import TFRobertaModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spYtj8KIGsnQ",
    "outputId": "c066e719-af3b-4b63-f370-60c282886b48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-07 17:11:07--  http://setup.johnsnowlabs.com/colab.sh\n",
      "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
      "--2022-12-07 17:11:08--  https://setup.johnsnowlabs.com/colab.sh\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
      "--2022-12-07 17:11:08--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1191 (1.2K) [text/plain]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-12-07 17:11:08 (41.1 MB/s) - written to stdout [1191/1191]\n",
      "\n",
      "Installing PySpark 3.2.1 and Spark NLP 4.2.4\n",
      "setup Colab for PySpark 3.2.1 and Spark NLP 4.2.4\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
      "\u001b[K     |████████████████████████████████| 448 kB 56.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 198 kB 55.3 MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AEWLGIpdGWpU"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CwoQMhRNO1vI"
   },
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04C4kmPtUNd-",
    "outputId": "e40b0836-c3e4-4b4a-c8aa-421b38e2399b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# connect ggdrive\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# connect ggdrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tZKAXqFaJqjK"
   },
   "outputs": [],
   "source": [
    "\n",
    "#save_model_address = '/content/drive/MyDrive/LUẬN VĂN-K18_CQ/02. Bình-Ngọc/Data/Dataset/conll/b_simp'\n",
    "#save_model_address = '/content'\n",
    "\n",
    "save_model_address = '/content/drive/MyDrive/Data Science/thesis/ML_NER/NERModel_config'\n",
    "\n",
    "#save_model = BertForTokenClassification.from_pretrained(save_model_address, num_labels=20)\n",
    "#tokenizer = BertTokenizer.from_pretrained(save_model_address,do_lower_case=True)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(save_model_address)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_model_address, do_lower_case=True, model_max_length=256)\n",
    "\n",
    "#nlp = pipeline(\"ner\", model=save_model, tokenizer=tokenizer, aggregation_strategy='simple',ignore_labels =['X','O'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3Z8qWSyNcPd"
   },
   "outputs": [],
   "source": [
    "loaded_ner_model = NerDLModel.load(\"/content/drive/MyDrive/Data Science/thesis/ML_NER/NERModel_config/pytorch_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CfbE7I_vQAXJ"
   },
   "outputs": [],
   "source": [
    "df = spark.read.option('header','true').csv('/home/tyeson/Desktop/K18/File K18/sample_crawl_dataset/Coursera_DataScience.csv',inferSchema=True)\n",
    "#df = spark.read.option('header','true').csv('/content/drive/MyDrive/Data Science/thesis/K18/File K18/sample_crawl_dataset/Coursera_DataScience.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WP10Z8RdZc-9",
    "outputId": "4b880528-0f76-4e88-e2ac-68b5bf7debde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, link: string, rating: string, enroll: string, instructor: string, time: string, levelrequirement: string, skillrequirement: string, SkillWillLearn: string, SkillGain: string, Subject: string, organization: string, fee: string, program: string, RelationInsOrg: string, Subtitle: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hn0KjyRJ3lTx"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df = df.withColumn(\"enroll\", f.regexp_replace(f.col(\"enroll\"), \",\", \"\").alias(\"enroll\")) \\\n",
    "    .withColumn(\"time\", f.regexp_replace(f.col(\"time\"), \" hours\", \"\").alias(\"time\")) \\\n",
    "    .withColumn(\"time\", f.regexp_replace(f.col(\"time\"), \" hour\", \"\").alias(\"time\")) \\\n",
    "    .withColumn(\"fee\", f.regexp_replace(f.col(\"fee\"), \"[a-zA-Z]+\", \"\").alias(\"fee\"))\n",
    "    #.withColumn(\"fee\", f.regexp_replace(f.col(\"fee\"), \"Enroll\", \"\").alias(\"fee\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6qNrE7RNZviY"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, LongType, IntegerType\n",
    "\n",
    "df = df \\\n",
    "  .withColumn(\"rating\" ,\n",
    "              df[\"rating\"]\n",
    "              .cast(FloatType()))   \\\n",
    "  .withColumn(\"enroll\",\n",
    "              df[\"enroll\"]\n",
    "              .cast(LongType())) \\\n",
    "  .withColumn(\"time\",df['time'].cast(IntegerType())) \\\n",
    "  .withColumn(\"fee\", df['fee'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFIsHiWI1gtc",
    "outputId": "4a028c63-784b-46f3-c805-cf10f4f3c3b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, link: string, rating: float, enroll: bigint, instructor: string, time: int, levelrequirement: string, skillrequirement: string, SkillWillLearn: string, SkillGain: string, Subject: string, organization: string, fee: int, program: string, RelationInsOrg: string, Subtitle: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jXLUeLApT1yu"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "il4aoBz8YqwW"
   },
   "outputs": [],
   "source": [
    "df = df.na.fill(value=0, subset=['rating','enroll', 'fee','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GQ2S1Jpz4cQy"
   },
   "outputs": [],
   "source": [
    "df = df.na.fill(value='None', subset=['link','instructor', 'levelrequirement', 'skillrequirement', 'SkillWillLearn','SkillGain', 'Subject', 'organization', 'program', 'RelationInsOrg', 'Subtitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6QRcVRJRKJsa"
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"levelrequirement\", f.regexp_replace(f.col(\"levelrequirement\"), \"None\", \"Beginener\").alias(\"levelrequirement\")) \\\n",
    "    .withColumn(\"levelrequirement\", f.regexp_replace(f.col(\"levelrequirement\"), \"Advanced\", \"Expert\").alias(\"levelrequirement\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NR3L4y2_L5XV",
    "outputId": "4014fa34-fd68-4362-9a11-1c2caffe28d2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col, monotonically_increasing_id\n",
    "\n",
    "df2 = df.select(col('name'),col('link'),col('rating'),col('enroll'),col('instructor'),col('time'),col('levelrequirement'),col('skillrequirement'),\n",
    "                col('SkillWillLearn'),col('SkillGain'),col('Subject'),col('organization'),col('fee'),col('program'),col('RelationInsOrg'),split(col(\"Subtitle\"),\", \").alias(\"Subtitle\"))\n",
    "df2 = df2.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+---+--------------------+\n",
      "|                name|                link|rating| enroll|          instructor|time|levelrequirement|    skillrequirement|      SkillWillLearn|           SkillGain|             Subject|        organization|fee|             program|      RelationInsOrg|            Subtitle| id|           timestamp|\n",
      "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+---+--------------------+\n",
      "|    Machine Learning|https://www.cours...|   4.9|4491564|           Andrew Ng|  61|       Beginener|                None|Machine learning ...|Logistic Regressi...|        Data Science| Stanford University|  0|                None|Andrew Ng work fo...|[Arabic, French, ...|  0|2023-01-03 13:38:...|\n",
      "|Data Visualizatio...|https://www.cours...|   4.9|  12564|Nicky Bull,Dr Pra...|  18|    Intermediate|We expect that yo...|\"In an age now dr...| we need to cut t...| but they must al...|        tell a story|  0| but it is still ...| particularly if ...|[ so the newer ve...|  1|2023-01-03 13:38:...|\n",
      "|Spatial Analysis ...|https://www.cours...|   4.9|  18321|           Don Boyes|  14|        Beginner|                None|In this course, y...|Geographic Inform...|Physical Science ...|University of Tor...|  0|GIS, Mapping, and...|Don Boyes work fo...|[Arabic, French, ...|  2|2023-01-03 13:38:...|\n",
      "|Python Data Struc...|https://www.cours...|   4.9| 800203|Charles Russell S...|  19|       Beginener|                None|Explain the princ...|Python Syntax And...|    Computer Science|University of Mic...|  0|Python for Everyb...|Charles Russell S...|[Arabic, French, ...|  3|2023-01-03 13:38:...|\n",
      "|Neural Networks a...|https://www.cours...|   4.9|1013093|Andrew Ng,Kian Ka...|  27|    Intermediate|                None|In the first cour...|Deep Learning    ...|        Data Science|     DeepLearning.AI|  0|Deep Learning Spe...|Andrew Ng work fo...|[Chinese (Traditi...|  4|2023-01-03 13:38:...|\n",
      "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df2 = df2.select(col('id'), col('name'), col('rating'), col('enroll'), col('time'), col('fee'), col('link'))\n",
    "df2 = df2.withColumn(\"timestamp\", f.current_timestamp()).alias(\"timestamp\")\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+-------+----+---+--------------------+--------------------+\n",
      "| id|                name|rating| enroll|time|fee|                link|           timestamp|\n",
      "+---+--------------------+------+-------+----+---+--------------------+--------------------+\n",
      "|  0|    Machine Learning|   4.9|4491564|  61|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "|  1|Data Visualizatio...|   4.9|  12564|  18|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "|  2|Spatial Analysis ...|   4.9|  18321|  14|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "|  3|Python Data Struc...|   4.9| 800203|  19|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "|  4|Neural Networks a...|   4.9|1013093|  27|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "|  5|What is Data Scie...|   4.7| 516658|   9|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "|  6|         Web of Data|   4.1|   4104|  18|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "|  7|Share Data Throug...|   4.5|  68559|  24|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "|  8|Visualization for...|   4.4|   6089|  18|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "|  9|Using Data for Ge...|   4.8|   7270|   5|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "| 10|Understanding and...|   4.7|  92827|  21|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "| 11|Tools for Explora...|   0.0|   2271|  18|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "| 12|Importing Data in...|   4.6|      0|  15|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "| 13|Visualizing Data ...|   0.0|      0|  17|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "| 14|Wrangling Data in...|   4.8|      0|  14|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "| 15|Modeling Data in ...|   0.0|      0|  21|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "| 16|The Data Science ...|   4.7|  12002|  10|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "| 17|The Fundamental o...|   0.0|      0|  19|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "| 18|Teaching Impacts ...|   4.6|   2623|  13|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "| 19|Survey Data Colle...|   0.0|      0|  14|  0|https://www.cours...|2023-01-03 13:38:...|\n",
      "+---+--------------------+------+-------+----+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "course = df2.select(col('id'), col('name'), col('rating'), col('enroll'), col('time'), col('fee'), col('link'), col('timestamp'))\n",
    "course.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1hDk2ZYMhvWt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df2 = df2.select(col('id'), col('name'), col('rating'), col('enroll'), col('time'), col('fee'), col('link'))\n",
    "#df2  = course_data.withColumn(\"timestamp\", f.current_timestamp()).alias(\"timestamp\")\n",
    "\n",
    "#df2 = df2.select(col('id'), col('name'), col('rating'), col('enroll'), col('time'), col('fee'), col('link'), col('timestamp'))\n",
    "\n",
    "#query = \"merge (course:Course{courseIdentifyKey: event.id , courseName: event.name ,courseEnrolled:[event.enroll], courseRating: event.rating, courseTime: event.time,courseFee: event.fee, courseUpdateDate: [event.timestamp], courselink: event.link})\"\n",
    "#query = \"merge (course:Course{courseIdentifyKey: event.id, courseName: event.name,courseEnrolled:[event.enroll], courseUpdateDate: event.timestamp, courseTime: event.time})\"\n",
    "course.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", \"neo4j+s://c4ae1994.databases.neo4j.io\")\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\")\\\n",
    "    .option(\"node.keys\", \"id\")\\\n",
    "    .option(\"labels\",\":Cousse\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+-------+----+---+--------------------+--------------------+\n",
      "| id|                name|rating| enroll|time|fee|                link|           timestamp|\n",
      "+---+--------------------+------+-------+----+---+--------------------+--------------------+\n",
      "|  0|    Machine Learning|   4.9|4491564|  61|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|  1|Data Visualizatio...|   4.9|  12564|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|  2|Spatial Analysis ...|   4.9|  18321|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|  3|Python Data Struc...|   4.9| 800203|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|  4|Neural Networks a...|   4.9|1013093|  27|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|  5|What is Data Scie...|   4.7| 516658|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|  6|         Web of Data|   4.1|   4104|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|  7|Share Data Throug...|   4.5|  68559|  24|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|  8|Visualization for...|   4.4|   6089|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|  9|Using Data for Ge...|   4.8|   7270|   5|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 10|Understanding and...|   4.7|  92827|  21|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 11|Tools for Explora...|   0.0|   2271|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 12|Importing Data in...|   4.6|      0|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 13|Visualizing Data ...|   0.0|      0|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 14|Wrangling Data in...|   4.8|      0|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 15|Modeling Data in ...|   0.0|      0|  21|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 16|The Data Science ...|   4.7|  12002|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 17|The Fundamental o...|   0.0|      0|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 18|Teaching Impacts ...|   4.6|   2623|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 19|Survey Data Colle...|   0.0|      0|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 20|Statistics for Da...|   4.6|   9567|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 21|Statistical Infer...|   0.0|      0|  27|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 22|SQL for Data Scie...|   4.0|   2470|  16|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 23|SQL for Data Science|   4.6| 358409|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 24|SQL for Data Scie...|   4.1|  18019|  35|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 25|Databases and SQL...|   4.6| 220195|  20|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 26|Spatial Data Scie...|   4.4|  18504|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 27|Social Media Data...|   4.1|  35265|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 28|Serverless Data P...|   4.1|   2583|   3|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 29|Serverless Data P...|   0.0|      0|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 30|Security and Priv...|   4.6|  12139|   1|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 31|Security and Priv...|   4.7|   6166|   1|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 32|Big Data Analysis...|   4.7|  90075|  28|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 33|Population Health...|   4.6|   3351|  21|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 34|Relational Databa...|   0.0|      0|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 35|Relational databa...|   4.4|  19169|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 36|Data Science in R...|   4.4|  47030|   7|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 37|Building Data Vis...|   3.9|  11369|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 38|Qualitative Data ...|   4.6|   2209|  16|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 39|Qualitative Data ...|   4.9|   2607|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 40|Python Scripting:...|   0.0|      0|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 41|,Develop computer...|   0.0|      0|null|  0|                None|2022-12-09 01:33:...|\n",
      "| 42|Python Project fo...|   4.6|   6453|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 43|Python Project fo...|   4.5|  41061|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 44|Python per la Dat...|   0.0|      0|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 45|Using Python to A...|   4.8| 529528|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 46|Data Visualizatio...|   4.5| 137200|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 47|Python for Data S...|   4.6| 348943|  20|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 48|Using Databases w...|   4.8| 396073|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 49|Python استخدام قو...|   0.0|      0|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 50|Data Processing U...|   4.1|  51360|  29|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 51|Capstone: Retriev...|   4.7| 210393|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 52|Python Data Struc...|   4.9| 803936|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 53|Promote the Ethic...|   4.6|   5805|  21|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 54|Process Data from...|   4.8|  95625|  22|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 55|Probability Theor...|   4.2|   5228|  48|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 56|Privacy Law and D...|   4.8|  13470|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 57|Preparing for the...|   4.6|  35423|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 58|Getting Started w...|   4.7|  20252|   5|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 59|Predictive Analyt...|   4.4|   9498|  24|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 60|Prediction Models...|   0.0|      0|  33|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 61|Tools for Data Sc...|   4.5| 241729|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 62|Explain how IBM W...|   0.0|      0|null|  0|Data Science     ...|2022-12-09 01:33:...|\n",
      "| 63|       NoSQL systems|   4.3|   9529|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 64|Data Privacy Fund...|   4.8|   8168|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 65|Network Security ...|   4.6|  41248|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 66|Exploring NCAA Da...|   0.0|      0|  45|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 67|Dealing With Miss...|   3.8|  10056|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 68|Foundations of mi...|   4.0|   2362|   7|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 69|Measurement – Tur...|   4.8|      0|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 70|Managing, Describ...|   4.3|   2170|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 71|Managing Data Ana...|   4.6|  61583|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 72|Machine Learning ...|   0.0|      0|  31|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 73|Machine Learning ...|   4.5|  13254|  22|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 74|Scalable Machine ...|   3.8|  19464|   7|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 75|Python and Machin...|   4.4|  10338|  20|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 76|Big Data Applicat...|   3.8|  10925|  28|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 77|Getting Started w...|   4.8|   6409|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 78|Data Visualizatio...|   4.9|   3013|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 79|Advanced Data Vis...|   4.8|   2070|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 80|Data Manipulation...|   0.0|      0|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 81|Inheritance and D...|   4.5|   2052|  21|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 82|Java Programming:...|   4.7| 124477|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 83|Introduzione alla...|   0.0|      0|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 84|Introduction to R...|   4.6|   6265|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 85|Introduction to N...|   4.6|   3975|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 86|Introduction to R...|   4.6|   5533|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 87|Introduction to D...|   4.8| 120550|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 88|Introduction to D...|   4.7|   6360|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 89|Introduction to D...|   4.7|  19897|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 90|Introduction to B...|   4.1|   3524|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 91|Introduction to S...|   4.7|  31159|  16|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 92|Introduction to C...|   4.6|  12315|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 93|Introduction to C...|   4.6|   7448|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 94|Intro to Analytic...|   4.0|   3112|   7|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 95|Introduction to A...|   4.8|  18899|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 96|Innovating with D...|   4.8|   5245|   2|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 97|Information Secur...|   4.7|  64840|  24|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 98|   IBM Data Topology|   4.8|      0|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "| 99|IBM Data Privacy ...|   4.7|      0|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|100|IBM Data Analyst ...|   4.6|  10946|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|101|Exploratory Data ...|   4.6|  23135|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|102|AI Workflow: Data...|   4.2|   2800|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|103|AI Workflow: Busi...|   4.3|   4410|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|104|Healthcare Data Q...|   4.5|   4824|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|105|Healthcare Data L...|   4.5|   9950|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|106|Using clinical he...|   4.7|   6107|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|107|Healthcare Data M...|   4.5|   4155|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|108|Google Data Analy...|   4.8|  56382|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|109|GIS Data Acquisit...|   4.9|  18752|  20|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|110|GIS Data Formats,...|   4.9|  33082|  28|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|111|Getting Started w...|   0.0|      0|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|112|Getting Started w...|   4.6|   7276|   4|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|113|Genomic Data Scie...|   4.2|  12919|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|114|Exploring ​and ​P...|   4.7|  52002|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|115|Google Cloud Big ...|   4.7| 228702|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|116|Creating New BigQ...|   4.6|  18663|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|117|Fundamentals of D...|   0.0|      0|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|118|Fundamentals of D...|   0.0|      0|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|119|Fundamentals of D...|   0.0|      0|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|120|Foundations for B...|   4.8|  38812|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|121|Foundations: Data...|   4.8| 512051|  20|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|122|Fitting Statistic...|   4.4|  24959|  16|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|123|Explore Core Data...|   4.8|   3373|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|124|Exploratory Data ...|   4.8|  30459|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|125|Exploratory Data ...|   4.7| 163189|  55|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|126|Executive Data Sc...|   4.7|  13477|   2|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|127|Excel Fundamental...|   4.8|  67987|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|128|Data Visualizatio...|   4.9|  13013|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|129|Introduction to D...|   4.7| 209592|  20|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|130|Excel Basics for ...|   4.8|  66437|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|131|ETL and Data Pipe...|   4.8|   2936|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|132|Ethical Issues in...|   4.4|      0|  23|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|133|Communicate Effec...|   4.4|   3008|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|134|Create and Lead a...|   4.5|   2951|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|135|Enterprise Databa...|   0.0|   2078|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|136|Amazon DynamoDB: ...|   4.7|  10277|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|137|Web Application T...|   4.7|  57056|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|138|Developing Applic...|   4.5|   4370|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|139|. Understand Obje...|   0.0|      0|null|  0|Django (Web Frame...|2022-12-09 01:33:...|\n",
      "|140|Serverless Data P...|   4.0|      0|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|141|Design Thinking a...|   4.5|   7810|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|142|  Data Visualization|   4.5| 106764|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|143|Essential Design ...|   4.5|  44724|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|144|Data Visualizatio...|   4.6|  21711|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|145|Visual Analytics ...|   4.6|  40234|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|146|Creating Dashboar...|   4.6|  37540|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|147|Data Wrangling, A...|   3.5|  30012|  16|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|148|Data Science in S...|   4.6|  16288|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|149|Data Science at S...|   4.1|   2239|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|150|Database Manageme...|   4.6| 160973|  36|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|151|Database Design a...|   4.8|  10016|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|152|Database Architec...|   4.3|   2351|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|153|Data Science Math...|   4.5| 289819|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|154|Building Database...|   4.9|  39477|  24|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|155|Modern Data Wareh...|   4.5|      0|   4|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|156|Fundamentals of V...|   4.5| 139052|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|157|Publishing Visual...|   4.9|   1961|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|158|3D Data Visualiza...|   0.0|   3010|  32|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|159|Design and Build ...|   4.6|   8335|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|160|Data Visualizatio...|   0.0|   1852|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|161|Data Visualizatio...|   4.7|  27985|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|162|Data Management a...|   4.4|  75026|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|163|Data Structures a...|   4.6|  13249|  22|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|164|Data Visualizatio...|   0.0|      0|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|165|     Data Structures|   4.6| 205479|  25|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|166|Data Structures a...|   4.8|  84794|  42|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|167|Data Visualizatio...|   4.9|      0|  22|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|168|Data, Security, a...|   0.0|      0|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|169|Data Science Meth...|   4.6| 164737|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|170|Data Science on G...|   0.0|      0|   7|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|171|The Data Scientis...|   4.6| 653843|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|172|Data Science Caps...|   4.5|  33998|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|173|Data Science with...|   0.0|      0|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|174| Data Science Ethics|   4.8|  23952|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|175|Introduction to D...|   0.0|      0|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|176|Data Science for ...|   4.3|   7850|   7|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|177|Data Science on G...|   0.0|      0|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|178|Data Science as a...|   0.0|      0|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|179|A Crash Course in...|   4.5| 179037|   7|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|180|Data Science Fund...|   3.7|   3085|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|181|Foundations of Da...|   4.7|  23163|  29|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|182|Data and Health I...|   4.7|  26176|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|183|Data Processing w...|   3.7|   6391|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|184|Communicating Dat...|   3.6|  15491|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|185|Prepare Data for ...|   4.8| 119518|  22|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|186|Developing Data P...|   4.6|  78785|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|187|Data Pipelines wi...|   4.3|  15282|  16|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|188|Data Modeling and...|   4.3|   6636|  25|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|189|Pattern Discovery...|   4.3|  36522|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|190|Research Data Man...|   4.7|  27693|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|191| Data Mining Project|   4.6|   6166|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|192|Data Manipulation...|   4.3|  57771|  20|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|193| Data Mining Project|   0.0|      0|  37|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|194| Data Mining Methods|   0.0|      0|  22|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|195|Data Mining Pipeline|   0.0|      0|  21|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|196|Designing data-in...|   4.4|   5990|   7|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|197|Data Engineering ...|   3.7|   1882|   7|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|198|Big Data, Genes, ...|   4.3|  27100|  40|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|199|Modernizing Data ...|   4.7|  33872|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|200|Applying Machine ...|   4.7|  14778|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|201|Data Literacy Cap...|   0.0|      0|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|202|Data for Machine ...|   4.4|   6297|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|203|Data Collection: ...|   4.6|  11262|  21|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|204|Data Collection a...|   4.7|  59044|  16|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|205|Data-Driven Proce...|   4.4|   3041|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|206|Data-driven Astro...|   4.8|  25151|  24|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|207|Framework for Dat...|   4.2|  23665|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|208|Getting and Clean...|   4.6| 189906|  20|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|209|Combining and Ana...|   4.2|   7132|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|210|Data Analytics fo...|   4.8|  47764|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|211|Data Analysis and...|   4.4|   3384|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|212|Data Analytics Fo...|   0.0|   2933|  70|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|213|Advanced Business...|   4.3|   6978|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|214|Data Analytics Fo...|   4.1|   7244|  67|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|215|Data Analysis with R|   4.9|   2608|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|216|Data Analysis and...|   4.8|  10784|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|217|Data Analysis wit...|   4.7| 213336|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|218|Data Analysis Usi...|   4.6|   5223|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|219|Data Analysis and...|   4.7|   3834|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|220| Data Analysis Tools|   4.5|  42602|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|221|Data Analysis wit...|   4.8|  71091|  37|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|222|Data Analysis and...|   4.8|   3551|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|223|Data Analysis and...|   4.7|   3404|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|224|Cybersecurity for...|   0.0|      0|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|225|Contemporary Data...|   0.0|      0|  16|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|226|How to Win a Data...|   4.7| 105047|  54|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|227|Code Free Data Sc...|   4.3|  10394|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|228|Analyzing Big Dat...|   4.9|  20566|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|229|Managing Big Data...|   4.7|   9115|  21|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|230|Cloud Data Engine...|   4.5|   1587|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|231|Building Advanced...|   4.3|      0|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|232| Cloud Data Security|   0.0|      0|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|233|Clinical Data Mod...|   4.2|   4996|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|234|Data Management f...|   4.7|  72267|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|235|Calculus through ...|   4.8|      0|   5|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|236|Calculus through ...|   0.0|      0|   5|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|237|Calculus through ...|   4.8|      0|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|238|Calculus through ...|   5.0|      0|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|239|Calculus through ...|   4.7|      0|   7|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|240|Calculus through ...|   4.8|   2543|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|241|Calculus through ...|   4.8|      0|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|242|Calculus through ...|   0.0|      0|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|243|Business intellig...|   3.9|  10094|  10|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|244|Business intellig...|   4.5|  11685|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|245|Exploring and Pro...|   4.8|  33594|  23|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|246|Building a Data S...|   4.5|  42405|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|247|Big Data Services...|   0.0|   1792|   3|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|248|BigQuery Basics f...|   4.7|   3033|   5|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|249|Big Data Modeling...|   4.4|  83258|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|250|Machine Learning ...|   4.6|  61519|  22|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|251|Big Data - Capsto...|   4.4|  13455|  21|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|252|Big data and Lang...|   0.0|   2254|   5|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|253|Graph Analytics f...|   4.3|  42389|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|254|Introduction to B...|   4.6| 272795|  17|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|255|Big Data Emerging...|   4.7|  18651|  30|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|256|Big Data Essentia...|   3.9|  51942|  41|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|257|Big data and Lang...|   4.5|   5125|   4|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|258|Big Data Analysis...|   0.0|      0|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|259|Big Data Integrat...|   4.4|  64488|  18|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|260|Building Batch Da...|   4.5|  30298|  13|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|261|Big Data, Artific...|   4.6|  18760|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|262|Basic Data Proces...|   4.3|  17086|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|263|Analyze Datasets ...|   4.5|  11284|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|264|Using Asylo to Pr...|   0.0|      0|   1|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|265|Applying Data Ana...|   4.5|  15065|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|266|Applying Data Ana...|   4.5|  15054|  23|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|267|Applied Data Scie...|   4.2|   1910|  16|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|268|Applied Data Scie...|   4.7|  87418|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|269|Select a location...|   0.0|      0|null|  0|Data Science     ...|2022-12-09 01:33:...|\n",
      "|270|Apache Spark (TM)...|   4.6|   9642|  14|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|271|Applied Analytics...|   4.5|      0|  11|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|272|Analyze Data to A...|   4.6|  78275|  26|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|273|Big Data Analytic...|   4.4|      0|   8|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|274|Graph Search, Sho...|   4.8|  66570|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|275|Algorithms, Data ...|   4.6|  16586|  15|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|276|Artificial Intell...|   4.8|   3247|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|277|Advanced Clinical...|   0.0|      0|   4|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|278|Advanced Data Sci...|   4.6|  13148|   9|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|279|Advanced Data Str...|   4.8|  73266|  29|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|280|Accounting Data A...|   4.1|   8065|  32|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|281|Data Analytics in...|   0.0|   1781|  19|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|282|Big Data Analysis...|   4.0|  29461|  37|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|283|Fundamentals of B...|   0.0|      0|  12|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|284|Healthcare Data S...|   4.7|   2473|   6|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "|285|Introduction to D...|   4.5| 634691|  31|  0|https://www.cours...|2022-12-09 01:33:...|\n",
      "+---+--------------------+------+-------+----+---+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#ourse_data = df2.select(col('id'), col('name'), col('rating'), col('enroll'), col('time'), col('fee'), col('link'))\n",
    "#course_data_time = course_data.withColumn(\"timestamp\", f.current_timestamp()).alias(\"timestamp\")\n",
    "course_data_time.show(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"instructor\", f.regexp_replace(f.col(\"instructor\"), \", PhD\", \"\").alias(\"instructor\")) \\\n",
    "    .withColumn(\"instructor\", f.regexp_replace(f.col(\"instructor\"), \", Ph.D.\", \"\").alias(\"instructor\")) \\\n",
    "    .withColumn(\"instructor\", f.regexp_replace(f.col(\"instructor\"), chr(34), \"\").alias(\"instructor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+---+--------------------+\n",
      "|                name|                link|rating| enroll|          instructor|time|levelrequirement|    skillrequirement|      SkillWillLearn|           SkillGain|             Subject|        organization|fee|             program|      RelationInsOrg|            Subtitle| id|           timestamp|\n",
      "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+---+--------------------+\n",
      "|    Machine Learning|https://www.cours...|   4.9|4491564|           Andrew Ng|  61|       Beginener|                None|Machine learning ...|Logistic Regressi...|        Data Science| Stanford University|  0|                None|Andrew Ng work fo...|[Arabic, French, ...|  0|2023-01-03 13:40:...|\n",
      "|Data Visualizatio...|https://www.cours...|   4.9|  12564|Nicky Bull,Dr Pra...|  18|    Intermediate|We expect that yo...|\"In an age now dr...| we need to cut t...| but they must al...|        tell a story|  0| but it is still ...| particularly if ...|[ so the newer ve...|  1|2023-01-03 13:40:...|\n",
      "|Spatial Analysis ...|https://www.cours...|   4.9|  18321|           Don Boyes|  14|        Beginner|                None|In this course, y...|Geographic Inform...|Physical Science ...|University of Tor...|  0|GIS, Mapping, and...|Don Boyes work fo...|[Arabic, French, ...|  2|2023-01-03 13:40:...|\n",
      "|Python Data Struc...|https://www.cours...|   4.9| 800203|Charles Russell S...|  19|       Beginener|                None|Explain the princ...|Python Syntax And...|    Computer Science|University of Mic...|  0|Python for Everyb...|Charles Russell S...|[Arabic, French, ...|  3|2023-01-03 13:40:...|\n",
      "|Neural Networks a...|https://www.cours...|   4.9|1013093|Andrew Ng,Kian Ka...|  27|    Intermediate|                None|In the first cour...|Deep Learning    ...|        Data Science|     DeepLearning.AI|  0|Deep Learning Spe...|Andrew Ng work fo...|[Chinese (Traditi...|  4|2023-01-03 13:40:...|\n",
      "|What is Data Scie...|https://www.cours...|   4.7| 516658|Rav Ahuja,Alex Ak...|   9|        Beginner|                None|Define data scien...|Data Science     ...|        Data Science|                 IBM|  0|                None|Rav Ahuja work fo...|[Arabic, French, ...|  5|2023-01-03 13:40:...|\n",
      "|         Web of Data|https://www.cours...|   4.1|   4104|Catherine Faron Z...|  18|    Intermediate|                None|This MOOC – a joi...|search for occurr...|        Data Science|        EIT Digital |  0|                None|Catherine Faron Z...|           [English]|  6|2023-01-03 13:40:...|\n",
      "|Share Data Throug...|https://www.cours...|   4.5|  68559|Google Career Cer...|  24|        Beginner|No prior experien...|Describe the use ...|Data Analysis    ...|        Data Science|              Google|  0|Google Data Analy...|Google Career Cer...|           [English]|  7|2023-01-03 13:40:...|\n",
      "|Visualization for...|https://www.cours...|   4.4|   6089|         Margaret Ng|  18|        Beginner|                None|While telling sto...|Python Libraries ...|        Data Science|University of Ill...|  0|                None|Margaret Ng work ...|[French, Portugue...|  8|2023-01-03 13:40:...|\n",
      "|Using Data for Ge...|https://www.cours...|   4.8|   7270|         Nicole Ball|   5|    Intermediate|                None|In this course, y...|                None|        Data Science|                 SAS|  0|SAS Visual Busine...|Nicole Ball work ...|[Subtitles: English]|  9|2023-01-03 13:40:...|\n",
      "|Understanding and...|https://www.cours...|   4.7|  92827|Brenda Gunderson,...|  21|        Beginner| High school algebra|Properly identify...|Statistics       ...|        Data Science|University of Mic...|  0|Statistics with P...|Brenda Gunderson ...|[Arabic, French, ...| 10|2023-01-03 13:40:...|\n",
      "|Tools for Explora...|https://www.cours...|   0.0|   2271|Jessen Hobson,Ron...|  18|        Beginner|Some prior experi...|Development of an...|R and RStudio    ...|        Data Science|University of Ill...|  0|Business Analytic...|Jessen Hobson wor...|           [English]| 11|2023-01-03 13:40:...|\n",
      "|Importing Data in...|https://www.cours...|   4.6|      0|Carrie Wright,Sha...|  15|        Beginner|F​amiliarity with...|Describe differen...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|[Subtitles: English]| 12|2023-01-03 13:40:...|\n",
      "|Visualizing Data ...|https://www.cours...|   0.0|      0|Carrie Wright,Sha...|  17|       Beginener|                None|Distinguish betwe...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|[Subtitles: English]| 13|2023-01-03 13:40:...|\n",
      "|Wrangling Data in...|https://www.cours...|   4.8|      0|Carrie Wright,Sha...|  14|        Manually|                None|Apply Tidyverse f...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|[Subtitles: English]| 14|2023-01-03 13:40:...|\n",
      "|Modeling Data in ...|https://www.cours...|   0.0|      0|Carrie Wright,Sha...|  21|       Beginener|                None|Describe differen...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|[Subtitles: English]| 15|2023-01-03 13:40:...|\n",
      "|The Data Science ...|https://www.cours...|   4.7|  12002|Hadi H. K. Kharra...|  10|        Beginner|While there are n...|Articulate differ...|Data Science     ...|              Health|Johns Hopkins Uni...|  0|Health Informatic...|Hadi H. K. Kharra...|[Arabic, French, ...| 16|2023-01-03 13:40:...|\n",
      "|The Fundamental o...|https://www.cours...|   0.0|      0|     Youngju Nielsen|  19|        Beginner|                None|Build an investme...|Build an investme...|        Data Science|Sungkyunkwan Univ...|  0|                None|Youngju Nielsen w...|           [English]| 17|2023-01-03 13:40:...|\n",
      "|Teaching Impacts ...|https://www.cours...|   4.6|   2623|          Beth Simon|  13|        Beginner|                None|In this course yo...|                None|     Social Sciences|University of Cal...|  0|Teaching Impacts ...|Beth Simon work f...|[Subtitles: English]| 18|2023-01-03 13:40:...|\n",
      "|Survey Data Colle...|https://www.cours...|   0.0|      0|Frauke Kreuter,Fr...|  14|       Beginener|                None|The Capstone Proj...|                None|              Health|University of Mic...|  0|Survey Data Colle...|Frauke Kreuter, P...|[Subtitles: English]| 19|2023-01-03 13:40:...|\n",
      "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          instructor|\n",
      "+---+--------------------+\n",
      "|  0|         [Andrew Ng]|\n",
      "|  1|[Nicky Bull, Dr P...|\n",
      "|  2|         [Don Boyes]|\n",
      "|  3|[Charles Russell ...|\n",
      "|  4|[Andrew Ng, Kian ...|\n",
      "|  5|[Rav Ahuja, Alex ...|\n",
      "|  6|[Catherine Faron ...|\n",
      "|  7|[Google Career Ce...|\n",
      "|  8|       [Margaret Ng]|\n",
      "|  9|       [Nicole Ball]|\n",
      "| 10|[Brenda Gunderson...|\n",
      "| 11|[Jessen Hobson, R...|\n",
      "| 12|[Carrie Wright, S...|\n",
      "| 13|[Carrie Wright, S...|\n",
      "| 14|[Carrie Wright, S...|\n",
      "| 15|[Carrie Wright, S...|\n",
      "| 16|[Hadi H. K. Kharr...|\n",
      "| 17|   [Youngju Nielsen]|\n",
      "| 18|        [Beth Simon]|\n",
      "| 19|[Frauke Kreuter, ...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instructor = df2.select(col(\"id\"), split(col(\"instructor\"),\",\").alias(\"instructor\"))\n",
    "instructor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "instructor = instructor.select(instructor.id, explode(\"instructor\").alias(\"instructor\"))\n",
    "instructor.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          instructor|\n",
      "+---+--------------------+\n",
      "|  0|           Andrew Ng|\n",
      "|  1|          Nicky Bull|\n",
      "|  1|Dr Prashan S. M. ...|\n",
      "|  2|           Don Boyes|\n",
      "|  3|Charles Russell S...|\n",
      "|  4|           Andrew Ng|\n",
      "|  4|   Kian Katanforoosh|\n",
      "|  4|Younes Bensouda M...|\n",
      "|  5|           Rav Ahuja|\n",
      "|  5|         Alex Aklson|\n",
      "|  6|Catherine Faron Z...|\n",
      "|  6|       Fabien Gandon|\n",
      "|  6|       Olivier Corby|\n",
      "|  7|Google Career Cer...|\n",
      "|  8|         Margaret Ng|\n",
      "|  9|         Nicole Ball|\n",
      "| 10|    Brenda Gunderson|\n",
      "| 10|       Brady T. West|\n",
      "| 10|       Kerby Shedden|\n",
      "| 11|       Jessen Hobson|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instructorNode = instructor.select(instructor.instructor)\n",
    "instructor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructorNode.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#query = \"merge (course:Course{courseIdentifyKey: event.id , courseName: event.name ,courseEnrolled:[event.enroll], courseRating: event.rating, courseTime: event.time,courseFee: event.fee, courseUpdateDate: [event.timestamp], courselink: event.link})\"\n",
    "#query = \"merge (course:Course{courseIdentifyKey: event.id, courseName: event.name,courseEnrolled:[event.enroll], courseUpdateDate: event.timestamp, courseTime: event.time})\"\n",
    "instructorNode.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", \"neo4j+s://c4ae1994.databases.neo4j.io\")\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\")\\\n",
    "    .option(\"node.keys\", \"instructor\")\\\n",
    "    .option(\"labels\",\":Instructor\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/03 15:30:47 ERROR Utils: Aborting task\n",
      "java.lang.IllegalArgumentException: Write failed due to the following errors:\n",
      " - Schema is missing target from option `relationship.target.node.keys`\n",
      " - Schema is missing source from option `relationship.source.node.keys`\n",
      "\n",
      "The option key and value might be inverted.\n",
      "\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n",
      "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n",
      "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n",
      "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
      "\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n",
      "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:92)\n",
      "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:18)\n",
      "\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:235)\n",
      "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n",
      "\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/01/03 15:30:47 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 22, attempt 0, stage 25.0)\n",
      "23/01/03 15:30:47 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 22, attempt 0, stage 25.0)\n",
      "23/01/03 15:30:47 ERROR Executor: Exception in task 0.0 in stage 25.0 (TID 22)\n",
      "java.lang.IllegalArgumentException: Write failed due to the following errors:\n",
      " - Schema is missing target from option `relationship.target.node.keys`\n",
      " - Schema is missing source from option `relationship.source.node.keys`\n",
      "\n",
      "The option key and value might be inverted.\n",
      "\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n",
      "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n",
      "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n",
      "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
      "\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n",
      "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:92)\n",
      "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:18)\n",
      "\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:235)\n",
      "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n",
      "\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/01/03 15:30:47 WARN TaskSetManager: Lost task 0.0 in stage 25.0 (TID 22) (192.168.10.84 executor driver): java.lang.IllegalArgumentException: Write failed due to the following errors:\n",
      " - Schema is missing target from option `relationship.target.node.keys`\n",
      " - Schema is missing source from option `relationship.source.node.keys`\n",
      "\n",
      "The option key and value might be inverted.\n",
      "\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n",
      "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n",
      "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n",
      "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
      "\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n",
      "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:92)\n",
      "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:18)\n",
      "\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:235)\n",
      "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n",
      "\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/01/03 15:30:47 ERROR TaskSetManager: Task 0 in stage 25.0 failed 1 times; aborting job\n",
      "23/01/03 15:30:47 ERROR OverwriteByExpressionExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@5a62e3e2 is aborting.\n",
      "23/01/03 15:30:47 ERROR OverwriteByExpressionExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@5a62e3e2 aborted.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o299.save.\n: org.apache.spark.SparkException: Writing job aborted\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:749)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:409)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:353)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:262)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:332)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:331)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:262)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:318)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 22) (192.168.10.84 executor driver): java.lang.IllegalArgumentException: Write failed due to the following errors:\n - Schema is missing target from option `relationship.target.node.keys`\n - Schema is missing source from option `relationship.source.node.keys`\n\nThe option key and value might be inverted.\n\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:92)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:18)\n\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:235)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:377)\n\t... 44 more\nCaused by: java.lang.IllegalArgumentException: Write failed due to the following errors:\n - Schema is missing target from option `relationship.target.node.keys`\n - Schema is missing source from option `relationship.source.node.keys`\n\nThe option key and value might be inverted.\n\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:92)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:18)\n\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:235)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m instructor\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.neo4j.spark.DataSource\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j+s://c4ae1994.databases.neo4j.io\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthentication.type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthentication.basic.username\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthentication.basic.password\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelationship\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEACH_BY\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationship.save.strategy\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeys\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationship.source.labels\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:Course\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationship.source.node.keys\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource:instructor\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationship.source.save.mode\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationship.target.labels\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:instructor\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationship.target.node.keys\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget:instructor\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationship.target.save.mode\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:966\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o299.save.\n: org.apache.spark.SparkException: Writing job aborted\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:749)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:409)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:353)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:262)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:332)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:331)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:262)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:318)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 22) (192.168.10.84 executor driver): java.lang.IllegalArgumentException: Write failed due to the following errors:\n - Schema is missing target from option `relationship.target.node.keys`\n - Schema is missing source from option `relationship.source.node.keys`\n\nThe option key and value might be inverted.\n\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:92)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:18)\n\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:235)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:377)\n\t... 44 more\nCaused by: java.lang.IllegalArgumentException: Write failed due to the following errors:\n - Schema is missing target from option `relationship.target.node.keys`\n - Schema is missing source from option `relationship.source.node.keys`\n\nThe option key and value might be inverted.\n\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:92)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:18)\n\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:235)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "instructor.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", \"neo4j+s://c4ae1994.databases.neo4j.io\")\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\")\\\n",
    "    .option(\"relationship\", 'TEACH_BY')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':Course')\\\n",
    "    .option('relationship.source.node.keys','source:instructor')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':instructor')\\\n",
    "    .option('relationship.target.node.keys','target:instructor')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          instructor|\n",
      "+---+--------------------+\n",
      "|  0|           Andrew Ng|\n",
      "|  1|          Nicky Bull|\n",
      "|  1|Dr Prashan S. M. ...|\n",
      "|  2|           Don Boyes|\n",
      "|  3|Charles Russell S...|\n",
      "|  4|           Andrew Ng|\n",
      "|  4|   Kian Katanforoosh|\n",
      "|  4|Younes Bensouda M...|\n",
      "|  5|           Rav Ahuja|\n",
      "|  5|         Alex Aklson|\n",
      "|  6|Catherine Faron Z...|\n",
      "|  6|       Fabien Gandon|\n",
      "|  6|       Olivier Corby|\n",
      "|  7|Google Career Cer...|\n",
      "|  8|         Margaret Ng|\n",
      "|  9|         Nicole Ball|\n",
      "| 10|    Brenda Gunderson|\n",
      "| 10|       Brady T. West|\n",
      "| 10|       Kerby Shedden|\n",
      "| 11|       Jessen Hobson|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instructor.select(instructor.id,instructor.instructor).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor = df2.select(col(\"id\"), col('name'),col('link'),col('rating'),col('enroll'),split(col(\"instructor\"),\",\").alias(\"instructor\"),col('time'),col('levelrequirement'),col('skillrequirement'),\n",
    "                col('SkillWillLearn'),col('SkillGain'),col('Subject'),col('organization'),col('fee'),col('program'),col('RelationInsOrg'),col(\"Subtitle\"),col('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = df2.select(col(\"id\"), col('levelrequirement'))\n",
    "level.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", \"neo4j+s://c4ae1994.databases.neo4j.io\")\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\")\\\n",
    "    .option(\"node.keys\", \"level\")\\\n",
    "    .option(\"labels\",\":Level\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle = df2.select(col(\"id\"), split(col(\"Subtitle\"),\",\").alias(\"Subtitle\"))\n",
    "subtitle.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle = subtitle.select(subtitle.id, explode(\"Subtitle\").alias(\"language\"))\n",
    "subtitle.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", \"neo4j+s://c4ae1994.databases.neo4j.io\")\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\")\\\n",
    "    .option(\"node.keys\", \"language\")\\\n",
    "    .option(\"labels\",\":Language\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", \"neo4j+s://c4ae1994.databases.neo4j.io\")\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\")\\\n",
    "    .option(\"relationship\", 'HAS_LEVEL')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':Course')\\\n",
    "    .option('relationship.source.node.keys','source:level')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Level')\\\n",
    "    .option('relationship.target.node.keys','target:level')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", \"neo4j+s://c4ae1994.databases.neo4j.io\")\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\")\\\n",
    "    .option(\"relationship\", 'TEACH_BY')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':Course')\\\n",
    "    .option('relationship.source.node.keys','source:instructor')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':instructor')\\\n",
    "    .option('relationship.target.node.keys','target:instructor')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
