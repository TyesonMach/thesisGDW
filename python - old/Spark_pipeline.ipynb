{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_PZKEtmCUSc"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "Sxb2DaDmUB60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.3.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "vB7IxU0zUDOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "tC-zB8OiUEU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "rhktzC10UFk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "I2CNGXdtUG4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OV5NEzRbUH98",
        "outputId": "bfc6bd17-1402-40b3-bee2-6d1247debb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.3.1-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "QJrYiDDzUJ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "mrvGoUWCUL0r",
        "outputId": "36b4fb02-41fa-40a6-a920-368871b424e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd9258afeb0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://697de085432f:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uy6RuNHKSsF",
        "outputId": "2115cd8d-3c80-47c1-addf-6d7243b729f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 65.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# thu vien chinh sua cac loi unicode\n",
        "!pip install ftfy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s17AcUhMKT6M",
        "outputId": "8fe8945a-6d9d-4824-cc9a-1b41d0d312a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pprint import pprint\n",
        "import string    \n",
        "import random\n",
        "import json\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "from transformers import TFRobertaModel\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZubCjaEGXYg",
        "outputId": "b2a1a837-0a4f-4938-88ca-412df9da1eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spYtj8KIGsnQ",
        "outputId": "c066e719-af3b-4b63-f370-60c282886b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-07 17:11:07--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
            "--2022-12-07 17:11:08--  https://setup.johnsnowlabs.com/colab.sh\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2022-12-07 17:11:08--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-07 17:11:08 (41.1 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "Installing PySpark 3.2.1 and Spark NLP 4.2.4\n",
            "setup Colab for PySpark 3.2.1 and Spark NLP 4.2.4\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[K     |████████████████████████████████| 448 kB 56.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 55.3 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import json\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "AEWLGIpdGWpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "CwoQMhRNO1vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect ggdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04C4kmPtUNd-",
        "outputId": "e40b0836-c3e4-4b4a-c8aa-421b38e2399b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#save_model_address = '/content/drive/MyDrive/LUẬN VĂN-K18_CQ/02. Bình-Ngọc/Data/Dataset/conll/b_simp'\n",
        "#save_model_address = '/content'\n",
        "\n",
        "save_model_address = '/content/drive/MyDrive/Data Science/thesis/ML_NER/NERModel_config'\n",
        "\n",
        "#save_model = BertForTokenClassification.from_pretrained(save_model_address, num_labels=20)\n",
        "#tokenizer = BertTokenizer.from_pretrained(save_model_address,do_lower_case=True)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(save_model_address)\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_model_address, do_lower_case=True, model_max_length=256)\n",
        "\n",
        "#nlp = pipeline(\"ner\", model=save_model, tokenizer=tokenizer, aggregation_strategy='simple',ignore_labels =['X','O'])\n"
      ],
      "metadata": {
        "id": "tZKAXqFaJqjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_ner_model = NerDLModel.load(\"/content/drive/MyDrive/Data Science/thesis/ML_NER/NERModel_config/pytorch_model\")"
      ],
      "metadata": {
        "id": "d3Z8qWSyNcPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option('header','true').csv('/content/drive/MyDrive/Data Science/thesis/K18/File K18/sample_crawl_dataset/Coursera_DataScience.csv',inferSchema=True)"
      ],
      "metadata": {
        "id": "CfbE7I_vQAXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP10Z8RdZc-9",
        "outputId": "4b880528-0f76-4e88-e2ac-68b5bf7debde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[name: string, link: string, rating: string, enroll: string, instructor: string, time: string, levelrequirement: string, skillrequirement: string, SkillWillLearn: string, SkillGain: string, Subject: string, organization: string, fee: string, program: string, RelationInsOrg: string, Subtitle: string]"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as f\n",
        "df = df.withColumn(\"enroll\", f.regexp_replace(f.col(\"enroll\"), \",\", \"\").alias(\"enroll\")) \\\n",
        "    .withColumn(\"time\", f.regexp_replace(f.col(\"time\"), \" hours\", \"\").alias(\"time\")) \\\n",
        "    .withColumn(\"time\", f.regexp_replace(f.col(\"time\"), \" hour\", \"\").alias(\"time\")) \\\n",
        "    .withColumn(\"fee\", f.regexp_replace(f.col(\"fee\"), \"[a-zA-Z]+\", \"\").alias(\"fee\"))\n",
        "    #.withColumn(\"fee\", f.regexp_replace(f.col(\"fee\"), \"Enroll\", \"\").alias(\"fee\"))"
      ],
      "metadata": {
        "id": "hn0KjyRJ3lTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import FloatType, LongType, IntegerType\n",
        "\n",
        "df = df \\\n",
        "  .withColumn(\"rating\" ,\n",
        "              df[\"rating\"]\n",
        "              .cast(FloatType()))   \\\n",
        "  .withColumn(\"enroll\",\n",
        "              df[\"enroll\"]\n",
        "              .cast(LongType())) \\\n",
        "  .withColumn(\"time\",df['time'].cast(IntegerType())) \\\n",
        "  .withColumn(\"fee\", df['fee'].cast(IntegerType()))"
      ],
      "metadata": {
        "id": "6qNrE7RNZviY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFIsHiWI1gtc",
        "outputId": "4a028c63-784b-46f3-c805-cf10f4f3c3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[name: string, link: string, rating: float, enroll: bigint, instructor: string, time: int, levelrequirement: string, skillrequirement: string, SkillWillLearn: string, SkillGain: string, Subject: string, organization: string, fee: int, program: string, RelationInsOrg: string, Subtitle: string]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1IBFnvHQAuu",
        "outputId": "db37c49d-76ff-48e9-e780-b59b8a294603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n",
            "|                name|                link|rating| enroll|          instructor|time|levelrequirement|    skillrequirement|      SkillWillLearn|           SkillGain|             Subject|        organization| fee|             program|      RelationInsOrg|            Subtitle|\n",
            "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n",
            "|    Machine Learning|https://www.cours...|   4.9|4491564|           Andrew Ng|  61|            null|                null|Machine learning ...|Logistic Regressi...|        Data Science| Stanford University|null|                null|Andrew Ng work fo...|Arabic, French, P...|\n",
            "|Data Visualizatio...|https://www.cours...|   4.9|  12564|Nicky Bull,Dr Pra...|  18|    Intermediate|We expect that yo...|\"In an age now dr...| we need to cut t...| but they must al...|        tell a story|   0| but it is still ...| particularly if ...| so the newer ver...|\n",
            "|Spatial Analysis ...|https://www.cours...|   4.9|  18321|           Don Boyes|  14|        Beginner|                null|In this course, y...|Geographic Inform...|Physical Science ...|University of Tor...|null|GIS, Mapping, and...|Don Boyes work fo...|Arabic, French, P...|\n",
            "|Python Data Struc...|https://www.cours...|   4.9| 800203|Charles Russell S...|  19|            null|                null|Explain the princ...|Python Syntax And...|    Computer Science|University of Mic...|null|Python for Everyb...|Charles Russell S...|Arabic, French, P...|\n",
            "|Neural Networks a...|https://www.cours...|   4.9|1013093|Andrew Ng,Kian Ka...|  27|    Intermediate|                null|In the first cour...|Deep Learning    ...|        Data Science|     DeepLearning.AI|null|Deep Learning Spe...|Andrew Ng work fo...|Chinese (Traditio...|\n",
            "|What is Data Scie...|https://www.cours...|   4.7| 516658|Rav Ahuja,Alex Ak...|   9|        Beginner|                null|Define data scien...|Data Science     ...|        Data Science|                 IBM|null|                null|Rav Ahuja work fo...|Arabic, French, P...|\n",
            "|         Web of Data|https://www.cours...|   4.1|   4104|Catherine Faron Z...|  18|    Intermediate|                null|This MOOC – a joi...|search for occurr...|        Data Science|        EIT Digital |null|                null|Catherine Faron Z...|             English|\n",
            "|Share Data Throug...|https://www.cours...|   4.5|  68559|Google Career Cer...|  24|        Beginner|No prior experien...|Describe the use ...|Data Analysis    ...|        Data Science|              Google|null|Google Data Analy...|Google Career Cer...|             English|\n",
            "|Visualization for...|https://www.cours...|   4.4|   6089|         Margaret Ng|  18|        Beginner|                null|While telling sto...|Python Libraries ...|        Data Science|University of Ill...|null|                null|Margaret Ng work ...|French, Portugues...|\n",
            "|Using Data for Ge...|https://www.cours...|   4.8|   7270|         Nicole Ball|   5|    Intermediate|                null|In this course, y...|                null|        Data Science|                 SAS|null|SAS Visual Busine...|Nicole Ball work ...|  Subtitles: English|\n",
            "|Understanding and...|https://www.cours...|   4.7|  92827|Brenda Gunderson,...|  21|        Beginner| High school algebra|Properly identify...|Statistics       ...|        Data Science|University of Mic...|null|Statistics with P...|Brenda Gunderson ...|Arabic, French, P...|\n",
            "|Tools for Explora...|https://www.cours...|  null|   2271|Jessen Hobson,Ron...|  18|        Beginner|Some prior experi...|Development of an...|R and RStudio    ...|        Data Science|University of Ill...|null|Business Analytic...|Jessen Hobson wor...|             English|\n",
            "|Importing Data in...|https://www.cours...|   4.6|   null|Carrie Wright, Ph...|  15|        Beginner|F​amiliarity with...|Describe differen...|                null|        Data Science|Johns Hopkins Uni...|null|Tidyverse Skills ...|Carrie Wright, Ph...|  Subtitles: English|\n",
            "|Visualizing Data ...|https://www.cours...|  null|   null|Carrie Wright, Ph...|  17|            null|                null|Distinguish betwe...|                null|        Data Science|Johns Hopkins Uni...|null|Tidyverse Skills ...|Carrie Wright, Ph...|  Subtitles: English|\n",
            "|Wrangling Data in...|https://www.cours...|   4.8|   null|Carrie Wright, Ph...|  14|        Manually|                null|Apply Tidyverse f...|                null|        Data Science|Johns Hopkins Uni...|null|Tidyverse Skills ...|Carrie Wright, Ph...|  Subtitles: English|\n",
            "|Modeling Data in ...|https://www.cours...|  null|   null|Carrie Wright, Ph...|  21|            null|                null|Describe differen...|                null|        Data Science|Johns Hopkins Uni...|null|Tidyverse Skills ...|Carrie Wright, Ph...|  Subtitles: English|\n",
            "|The Data Science ...|https://www.cours...|   4.7|  12002|Hadi H. K. Kharra...|  10|        Beginner|While there are n...|Articulate differ...|Data Science     ...|              Health|Johns Hopkins Uni...|null|Health Informatic...|Hadi H. K. Kharra...|Arabic, French, P...|\n",
            "|The Fundamental o...|https://www.cours...|  null|   null|     Youngju Nielsen|  19|        Beginner|                null|Build an investme...|Build an investme...|        Data Science|Sungkyunkwan Univ...|null|                null|Youngju Nielsen w...|             English|\n",
            "|Teaching Impacts ...|https://www.cours...|   4.6|   2623|          Beth Simon|  13|        Beginner|                null|In this course yo...|                null|     Social Sciences|University of Cal...|null|Teaching Impacts ...|Beth Simon work f...|  Subtitles: English|\n",
            "|Survey Data Colle...|https://www.cours...|  null|   null|Frauke Kreuter, P...|  14|            null|                null|The Capstone Proj...|                null|              Health|University of Mic...|null|Survey Data Colle...|Frauke Kreuter, P...|  Subtitles: English|\n",
            "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,when"
      ],
      "metadata": {
        "id": "jXLUeLApT1yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.na.fill(value=0, subset=['rating','enroll', 'fee'])"
      ],
      "metadata": {
        "id": "il4aoBz8YqwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.na.fill(value='None', subset=['link','instructor', 'levelrequirement', 'skillrequirement', 'SkillWillLearn','SkillGain', 'Subject', 'organization', 'program', 'RelationInsOrg', 'Subtitle'])"
      ],
      "metadata": {
        "id": "GQ2S1Jpz4cQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"levelrequirement\", f.regexp_replace(f.col(\"levelrequirement\"), \"None\", \"Beginener\").alias(\"levelrequirement\")) \\\n",
        "    .withColumn(\"levelrequirement\", f.regexp_replace(f.col(\"levelrequirement\"), \"Advanced\", \"Expert\").alias(\"levelrequirement\")) \n"
      ],
      "metadata": {
        "id": "6QRcVRJRKJsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJWXnE6vL2kk",
        "outputId": "343efd29-c39f-47e4-f645-a1bb606b6fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|                name|                link|rating| enroll|          instructor|time|levelrequirement|    skillrequirement|      SkillWillLearn|           SkillGain|             Subject|        organization|fee|             program|      RelationInsOrg|            Subtitle|\n",
            "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|    Machine Learning|https://www.cours...|   4.9|4491564|           Andrew Ng|  61|       Beginener|                None|Machine learning ...|Logistic Regressi...|        Data Science| Stanford University|  0|                None|Andrew Ng work fo...|Arabic, French, P...|\n",
            "|Data Visualizatio...|https://www.cours...|   4.9|  12564|Nicky Bull,Dr Pra...|  18|    Intermediate|We expect that yo...|\"In an age now dr...| we need to cut t...| but they must al...|        tell a story|  0| but it is still ...| particularly if ...| so the newer ver...|\n",
            "|Spatial Analysis ...|https://www.cours...|   4.9|  18321|           Don Boyes|  14|        Beginner|                None|In this course, y...|Geographic Inform...|Physical Science ...|University of Tor...|  0|GIS, Mapping, and...|Don Boyes work fo...|Arabic, French, P...|\n",
            "|Python Data Struc...|https://www.cours...|   4.9| 800203|Charles Russell S...|  19|       Beginener|                None|Explain the princ...|Python Syntax And...|    Computer Science|University of Mic...|  0|Python for Everyb...|Charles Russell S...|Arabic, French, P...|\n",
            "|Neural Networks a...|https://www.cours...|   4.9|1013093|Andrew Ng,Kian Ka...|  27|    Intermediate|                None|In the first cour...|Deep Learning    ...|        Data Science|     DeepLearning.AI|  0|Deep Learning Spe...|Andrew Ng work fo...|Chinese (Traditio...|\n",
            "|What is Data Scie...|https://www.cours...|   4.7| 516658|Rav Ahuja,Alex Ak...|   9|        Beginner|                None|Define data scien...|Data Science     ...|        Data Science|                 IBM|  0|                None|Rav Ahuja work fo...|Arabic, French, P...|\n",
            "|         Web of Data|https://www.cours...|   4.1|   4104|Catherine Faron Z...|  18|    Intermediate|                None|This MOOC – a joi...|search for occurr...|        Data Science|        EIT Digital |  0|                None|Catherine Faron Z...|             English|\n",
            "|Share Data Throug...|https://www.cours...|   4.5|  68559|Google Career Cer...|  24|        Beginner|No prior experien...|Describe the use ...|Data Analysis    ...|        Data Science|              Google|  0|Google Data Analy...|Google Career Cer...|             English|\n",
            "|Visualization for...|https://www.cours...|   4.4|   6089|         Margaret Ng|  18|        Beginner|                None|While telling sto...|Python Libraries ...|        Data Science|University of Ill...|  0|                None|Margaret Ng work ...|French, Portugues...|\n",
            "|Using Data for Ge...|https://www.cours...|   4.8|   7270|         Nicole Ball|   5|    Intermediate|                None|In this course, y...|                None|        Data Science|                 SAS|  0|SAS Visual Busine...|Nicole Ball work ...|  Subtitles: English|\n",
            "|Understanding and...|https://www.cours...|   4.7|  92827|Brenda Gunderson,...|  21|        Beginner| High school algebra|Properly identify...|Statistics       ...|        Data Science|University of Mic...|  0|Statistics with P...|Brenda Gunderson ...|Arabic, French, P...|\n",
            "|Tools for Explora...|https://www.cours...|   0.0|   2271|Jessen Hobson,Ron...|  18|        Beginner|Some prior experi...|Development of an...|R and RStudio    ...|        Data Science|University of Ill...|  0|Business Analytic...|Jessen Hobson wor...|             English|\n",
            "|Importing Data in...|https://www.cours...|   4.6|      0|Carrie Wright, Ph...|  15|        Beginner|F​amiliarity with...|Describe differen...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|  Subtitles: English|\n",
            "|Visualizing Data ...|https://www.cours...|   0.0|      0|Carrie Wright, Ph...|  17|       Beginener|                None|Distinguish betwe...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|  Subtitles: English|\n",
            "|Wrangling Data in...|https://www.cours...|   4.8|      0|Carrie Wright, Ph...|  14|        Manually|                None|Apply Tidyverse f...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|  Subtitles: English|\n",
            "|Modeling Data in ...|https://www.cours...|   0.0|      0|Carrie Wright, Ph...|  21|       Beginener|                None|Describe differen...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|  Subtitles: English|\n",
            "|The Data Science ...|https://www.cours...|   4.7|  12002|Hadi H. K. Kharra...|  10|        Beginner|While there are n...|Articulate differ...|Data Science     ...|              Health|Johns Hopkins Uni...|  0|Health Informatic...|Hadi H. K. Kharra...|Arabic, French, P...|\n",
            "|The Fundamental o...|https://www.cours...|   0.0|      0|     Youngju Nielsen|  19|        Beginner|                None|Build an investme...|Build an investme...|        Data Science|Sungkyunkwan Univ...|  0|                None|Youngju Nielsen w...|             English|\n",
            "|Teaching Impacts ...|https://www.cours...|   4.6|   2623|          Beth Simon|  13|        Beginner|                None|In this course yo...|                None|     Social Sciences|University of Cal...|  0|Teaching Impacts ...|Beth Simon work f...|  Subtitles: English|\n",
            "|Survey Data Colle...|https://www.cours...|   0.0|      0|Frauke Kreuter, P...|  14|       Beginener|                None|The Capstone Proj...|                None|              Health|University of Mic...|  0|Survey Data Colle...|Frauke Kreuter, P...|  Subtitles: English|\n",
            "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "df2 = df.select(col('name'),col('link'),col('rating'),col('enroll'),col('instructor'),col('time'),col('levelrequirement'),col('skillrequirement'),\n",
        "                col('SkillWillLearn'),col('SkillGain'),col('Subject'),col('organization'),col('fee'),col('program'),col('RelationInsOrg'),split(col(\"Subtitle\"),\", \").alias(\"Subtitle\"))\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR3L4y2_L5XV",
        "outputId": "4014fa34-fd68-4362-9a11-1c2caffe28d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|                name|                link|rating| enroll|          instructor|time|levelrequirement|    skillrequirement|      SkillWillLearn|           SkillGain|             Subject|        organization|fee|             program|      RelationInsOrg|            Subtitle|\n",
            "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|    Machine Learning|https://www.cours...|   4.9|4491564|           Andrew Ng|  61|       Beginener|                None|Machine learning ...|Logistic Regressi...|        Data Science| Stanford University|  0|                None|Andrew Ng work fo...|[Arabic, French, ...|\n",
            "|Data Visualizatio...|https://www.cours...|   4.9|  12564|Nicky Bull,Dr Pra...|  18|    Intermediate|We expect that yo...|\"In an age now dr...| we need to cut t...| but they must al...|        tell a story|  0| but it is still ...| particularly if ...|[ so the newer ve...|\n",
            "|Spatial Analysis ...|https://www.cours...|   4.9|  18321|           Don Boyes|  14|        Beginner|                None|In this course, y...|Geographic Inform...|Physical Science ...|University of Tor...|  0|GIS, Mapping, and...|Don Boyes work fo...|[Arabic, French, ...|\n",
            "|Python Data Struc...|https://www.cours...|   4.9| 800203|Charles Russell S...|  19|       Beginener|                None|Explain the princ...|Python Syntax And...|    Computer Science|University of Mic...|  0|Python for Everyb...|Charles Russell S...|[Arabic, French, ...|\n",
            "|Neural Networks a...|https://www.cours...|   4.9|1013093|Andrew Ng,Kian Ka...|  27|    Intermediate|                None|In the first cour...|Deep Learning    ...|        Data Science|     DeepLearning.AI|  0|Deep Learning Spe...|Andrew Ng work fo...|[Chinese (Traditi...|\n",
            "|What is Data Scie...|https://www.cours...|   4.7| 516658|Rav Ahuja,Alex Ak...|   9|        Beginner|                None|Define data scien...|Data Science     ...|        Data Science|                 IBM|  0|                None|Rav Ahuja work fo...|[Arabic, French, ...|\n",
            "|         Web of Data|https://www.cours...|   4.1|   4104|Catherine Faron Z...|  18|    Intermediate|                None|This MOOC – a joi...|search for occurr...|        Data Science|        EIT Digital |  0|                None|Catherine Faron Z...|           [English]|\n",
            "|Share Data Throug...|https://www.cours...|   4.5|  68559|Google Career Cer...|  24|        Beginner|No prior experien...|Describe the use ...|Data Analysis    ...|        Data Science|              Google|  0|Google Data Analy...|Google Career Cer...|           [English]|\n",
            "|Visualization for...|https://www.cours...|   4.4|   6089|         Margaret Ng|  18|        Beginner|                None|While telling sto...|Python Libraries ...|        Data Science|University of Ill...|  0|                None|Margaret Ng work ...|[French, Portugue...|\n",
            "|Using Data for Ge...|https://www.cours...|   4.8|   7270|         Nicole Ball|   5|    Intermediate|                None|In this course, y...|                None|        Data Science|                 SAS|  0|SAS Visual Busine...|Nicole Ball work ...|[Subtitles: English]|\n",
            "|Understanding and...|https://www.cours...|   4.7|  92827|Brenda Gunderson,...|  21|        Beginner| High school algebra|Properly identify...|Statistics       ...|        Data Science|University of Mic...|  0|Statistics with P...|Brenda Gunderson ...|[Arabic, French, ...|\n",
            "|Tools for Explora...|https://www.cours...|   0.0|   2271|Jessen Hobson,Ron...|  18|        Beginner|Some prior experi...|Development of an...|R and RStudio    ...|        Data Science|University of Ill...|  0|Business Analytic...|Jessen Hobson wor...|           [English]|\n",
            "|Importing Data in...|https://www.cours...|   4.6|      0|Carrie Wright, Ph...|  15|        Beginner|F​amiliarity with...|Describe differen...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|[Subtitles: English]|\n",
            "|Visualizing Data ...|https://www.cours...|   0.0|      0|Carrie Wright, Ph...|  17|       Beginener|                None|Distinguish betwe...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|[Subtitles: English]|\n",
            "|Wrangling Data in...|https://www.cours...|   4.8|      0|Carrie Wright, Ph...|  14|        Manually|                None|Apply Tidyverse f...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|[Subtitles: English]|\n",
            "|Modeling Data in ...|https://www.cours...|   0.0|      0|Carrie Wright, Ph...|  21|       Beginener|                None|Describe differen...|                None|        Data Science|Johns Hopkins Uni...|  0|Tidyverse Skills ...|Carrie Wright, Ph...|[Subtitles: English]|\n",
            "|The Data Science ...|https://www.cours...|   4.7|  12002|Hadi H. K. Kharra...|  10|        Beginner|While there are n...|Articulate differ...|Data Science     ...|              Health|Johns Hopkins Uni...|  0|Health Informatic...|Hadi H. K. Kharra...|[Arabic, French, ...|\n",
            "|The Fundamental o...|https://www.cours...|   0.0|      0|     Youngju Nielsen|  19|        Beginner|                None|Build an investme...|Build an investme...|        Data Science|Sungkyunkwan Univ...|  0|                None|Youngju Nielsen w...|           [English]|\n",
            "|Teaching Impacts ...|https://www.cours...|   4.6|   2623|          Beth Simon|  13|        Beginner|                None|In this course yo...|                None|     Social Sciences|University of Cal...|  0|Teaching Impacts ...|Beth Simon work f...|[Subtitles: English]|\n",
            "|Survey Data Colle...|https://www.cours...|   0.0|      0|Frauke Kreuter, P...|  14|       Beginener|                None|The Capstone Proj...|                None|              Health|University of Mic...|  0|Survey Data Colle...|Frauke Kreuter, P...|[Subtitles: English]|\n",
            "+--------------------+--------------------+------+-------+--------------------+----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dbutils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzVVa0g612mW",
        "outputId": "99945f8a-57e7-47a7-905b-96812afe02b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dbutils\n",
            "  Downloading DBUtils-3.0.2-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: dbutils\n",
            "Successfully installed dbutils-3.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dbutils\n",
        "\n",
        "dbutils.widgets.text(\"url\", \"neo4j+s://c4ae1994.databases.neo4j.io\")\n",
        "dbutils.widgets.text(\"user\", \"neo4j\")\n",
        "dbutils.widgets.text(\"user\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "QAMQGRlgwFXt",
        "outputId": "fc79bea7-f2dd-4227-a85c-6bea32908df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2d815e695941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdbutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neo4j+s://c4ae1994.databases.neo4j.io\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neo4j\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'dbutils' has no attribute 'widgets'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"Cases Report.csv\")\n",
        "df = spark.read.option('header','true').csv('Cases Report.csv',inferSchema=True)\n",
        "#.option(\"query\",\"merge (course:Course{courseIdentifyKey: apoc.util.sha512([]) , courseName:'\" + data['name'] + \"',courseEnrolled:[\" + str(data['enroll']) + \"], courseRating:\" + str(data['rating']) + \",courseFee:\" + str(data['fee']) + \",courseTime:\" + str(data['time']) + \", courseUpdateDate: [datetime()], courselink: '\" + data['link'] + \"'})\")"
      ],
      "metadata": {
        "id": "brUa6WP30JTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$SPARK_HOME/bin/pyspark --jars neo4j-connector-apache-spark_2.12-5.0.0_for_spark_3.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq-zFqId3Hih",
        "outputId": "4fc41988-2bf6-4eff-e560-b9815c2e59cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.15 (default, Oct 12 2022, 19:14:39) \n",
            "[GCC 7.5.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            "22/12/07 20:33:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.1\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.8.15 (default, Oct 12 2022 19:14:39)\n",
            "Spark context Web UI available at http://697de085432f:4040\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1670445192994).\n",
            "SparkSession available as 'spark'.\n",
            "rue)\n",
            ">>> from pyspark.sql.functions import col,when\n",
            ">>> course_data = df.select(col('Outcome'),col('Age'))\n",
            ", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\").save()\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/readwriter.py\", line 966, in save\n",
            "    self._jwrite.save()\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py\", line 196, in deco\n",
            "    raise converted from None\n",
            "pyspark.sql.utils.AnalysisException: TableProvider implementation org.neo4j.spark.DataSource cannot be written with ErrorIfExists mode, please use Append or Overwrite modes instead.\n",
            "tion.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\").save()\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "TypeError: option() missing 1 required positional argument: 'value'\n",
            "()\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/readwriter.py\", line 966, in save\n",
            "    self._jwrite.save()\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py\", line 196, in deco\n",
            "    raise converted from None\n",
            "pyspark.sql.utils.AnalysisException: TableProvider implementation org.neo4j.spark.DataSource cannot be written with ErrorIfExists mode, please use Append or Overwrite modes instead.\n",
            "on.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\").save()\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/readwriter.py\", line 966, in save\n",
            "    self._jwrite.save()\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py\", line 196, in deco\n",
            "    raise converted from None\n",
            "pyspark.sql.utils.IllegalArgumentException: No valid option found. One of `query`, `labels`, `relationship` is required\n",
            "s\", \"Course\").option(\"node.keys\",\"id\").save()\n",
            "22/12/07 20:36:29 ERROR Utils: Aborting task\n",
            "java.lang.IllegalArgumentException: Write failed due to the following errors:\n",
            " - Schema is missing id from option `node.keys`\n",
            "\n",
            "The option key and value might be inverted.\n",
            "\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n",
            "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
            "\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:22)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:18)\n",
            "\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:234)\n",
            "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n",
            "\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "22/12/07 20:36:29 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 3, attempt 0, stage 2.0)\n",
            "22/12/07 20:36:29 ERROR Utils: Aborting task\n",
            "java.lang.IllegalArgumentException: Write failed due to the following errors:\n",
            " - Schema is missing id from option `node.keys`\n",
            "\n",
            "The option key and value might be inverted.\n",
            "\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n",
            "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
            "\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:22)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:18)\n",
            "\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:234)\n",
            "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n",
            "\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "22/12/07 20:36:29 ERROR DataWritingSparkTask: Aborting commit for partition 1 (task 4, attempt 0, stage 2.0)\n",
            "22/12/07 20:36:29 ERROR DataWritingSparkTask: Aborted commit for partition 1 (task 4, attempt 0, stage 2.0)\n",
            "22/12/07 20:36:29 ERROR Executor: Exception in task 1.0 in stage 2.0 (TID 4)\n",
            "java.lang.IllegalArgumentException: Write failed due to the following errors:\n",
            " - Schema is missing id from option `node.keys`\n",
            "\n",
            "The option key and value might be inverted.\n",
            "\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n",
            "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
            "\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:22)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:18)\n",
            "\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:234)\n",
            "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n",
            "\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "22/12/07 20:36:29 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 3, attempt 0, stage 2.0)\n",
            "22/12/07 20:36:29 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 3)\n",
            "java.lang.IllegalArgumentException: Write failed due to the following errors:\n",
            " - Schema is missing id from option `node.keys`\n",
            "\n",
            "The option key and value might be inverted.\n",
            "\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n",
            "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
            "\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:22)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:18)\n",
            "\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:234)\n",
            "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n",
            "\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "22/12/07 20:36:29 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4) (697de085432f executor driver): java.lang.IllegalArgumentException: Write failed due to the following errors:\n",
            " - Schema is missing id from option `node.keys`\n",
            "\n",
            "The option key and value might be inverted.\n",
            "\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n",
            "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
            "\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:22)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:18)\n",
            "\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:234)\n",
            "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n",
            "\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "\n",
            "22/12/07 20:36:29 ERROR TaskSetManager: Task 1 in stage 2.0 failed 1 times; aborting job\n",
            "22/12/07 20:36:29 ERROR OverwriteByExpressionExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@6a89b8aa is aborting.\n",
            "22/12/07 20:36:29 ERROR OverwriteByExpressionExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@6a89b8aa aborted.\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/readwriter.py\", line 966, in save\n",
            "    self._jwrite.save()\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py\", line 190, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o86.save.\n",
            ": org.apache.spark.SparkException: Writing job aborted\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:767)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:409)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:353)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:262)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:332)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:331)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:262)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:318)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 4) (697de085432f executor driver): java.lang.IllegalArgumentException: Write failed due to the following errors:\n",
            " - Schema is missing id from option `node.keys`\n",
            "\n",
            "The option key and value might be inverted.\n",
            "\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n",
            "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
            "\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:22)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:18)\n",
            "\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:234)\n",
            "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n",
            "\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:377)\n",
            "\t... 44 more\n",
            "Caused by: java.lang.IllegalArgumentException: Write failed due to the following errors:\n",
            " - Schema is missing id from option `node.keys`\n",
            "\n",
            "The option key and value might be inverted.\n",
            "\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:38)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:12)\n",
            "\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:12)\n",
            "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
            "\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:12)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:22)\n",
            "\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.node(MappingService.scala:18)\n",
            "\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:234)\n",
            "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:43)\n",
            "\tat org.neo4j.spark.writer.Neo4jDataWriter.write(Neo4jDataWriter.scala:9)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:442)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\t... 1 more\n",
            "\n",
            "s\", \"Course\").save()\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/readwriter.py\", line 966, in save\n",
            "    self._jwrite.save()\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py\", line 196, in deco\n",
            "    raise converted from None\n",
            "pyspark.sql.utils.IllegalArgumentException: node.keys is required when Save Mode is Overwrite\n",
            "s\", \"Course\").option(\"node.keys\",\"Age\").save()\n",
            ">>> \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/context.py\", line 362, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/context.py\", line 1447, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1377, in __getattr__\n",
            "  File \"/content/spark-3.3.1-bin-hadoop3/python/pyspark/context.py\", line 363, in signal_handler\n",
            "    raise KeyboardInterrupt()\n",
            "KeyboardInterrupt\n",
            ">>> ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "course_data = df2.select(col('name'), col('rating'), col('enroll'), col('time'), col('fee'), col('link'))\n",
        "#course_data = df.select(col('Outcome'),col('Age'))\n",
        "course_data.write.format(\"org.neo4j.spark.DataSource\") \\\n",
        "  .mode(\"Overwrite\") \\\n",
        "  .option(\"url\",\"neo4j+s://c4ae1994.databases.neo4j.io\") \\\n",
        "  .option(\"authentication.basic.username\", \"neo4j\") \\\n",
        "  .option(\"authentication.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\") \\\n",
        "  .option(\"labels\", \"Course\") \\\n",
        "  .option(\"node.keys\",\"id\") \\\n",
        "  .save()"
      ],
      "metadata": {
        "id": "1hDk2ZYMhvWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "course_data.write.format(\"org.neo4j.spark.DataSource\").mode(\"Overwrite\").option(\"url\", \"neo4j+s://c4ae1994.databases.neo4j.io\").option(\"authentication.type\", \"basic\").option(\"authentication.basic.username\", \"neo4j\").option(\"authentication.basic.password\", \"_qALa8tQkC7Bg47nvzrRPhuPH38QJEUXeUw1NTs22J8\").option(\"labels\", \"Course\").option(\"node.keys\",\"Age\").save()"
      ],
      "metadata": {
        "id": "lAumVmtIPPnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "course_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v6-VR_c0-4l",
        "outputId": "20e6c070-ef9f-4278-f9d9-6ae39fc355f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+-------+----+---+--------------------+\n",
            "|                name|rating| enroll|time|fee|                link|\n",
            "+--------------------+------+-------+----+---+--------------------+\n",
            "|    Machine Learning|   4.9|4491564|  61|  0|https://www.cours...|\n",
            "|Data Visualizatio...|   4.9|  12564|  18|  0|https://www.cours...|\n",
            "|Spatial Analysis ...|   4.9|  18321|  14|  0|https://www.cours...|\n",
            "|Python Data Struc...|   4.9| 800203|  19|  0|https://www.cours...|\n",
            "|Neural Networks a...|   4.9|1013093|  27|  0|https://www.cours...|\n",
            "|What is Data Scie...|   4.7| 516658|   9|  0|https://www.cours...|\n",
            "|         Web of Data|   4.1|   4104|  18|  0|https://www.cours...|\n",
            "|Share Data Throug...|   4.5|  68559|  24|  0|https://www.cours...|\n",
            "|Visualization for...|   4.4|   6089|  18|  0|https://www.cours...|\n",
            "|Using Data for Ge...|   4.8|   7270|   5|  0|https://www.cours...|\n",
            "|Understanding and...|   4.7|  92827|  21|  0|https://www.cours...|\n",
            "|Tools for Explora...|   0.0|   2271|  18|  0|https://www.cours...|\n",
            "|Importing Data in...|   4.6|      0|  15|  0|https://www.cours...|\n",
            "|Visualizing Data ...|   0.0|      0|  17|  0|https://www.cours...|\n",
            "|Wrangling Data in...|   4.8|      0|  14|  0|https://www.cours...|\n",
            "|Modeling Data in ...|   0.0|      0|  21|  0|https://www.cours...|\n",
            "|The Data Science ...|   4.7|  12002|  10|  0|https://www.cours...|\n",
            "|The Fundamental o...|   0.0|      0|  19|  0|https://www.cours...|\n",
            "|Teaching Impacts ...|   4.6|   2623|  13|  0|https://www.cours...|\n",
            "|Survey Data Colle...|   0.0|      0|  14|  0|https://www.cours...|\n",
            "+--------------------+------+-------+----+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}