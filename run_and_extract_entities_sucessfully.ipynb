{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vXYNX2lQROB"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BertForTokenClassification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zva6MvJyLeWi"
      },
      "source": [
        "## Import BertForTokenClassification models from HuggingFace ü§ó  into Spark NLP üöÄ \n",
        "\n",
        "Let's keep in mind a few things before we start üòä \n",
        "\n",
        "- This feature is only in `Spark NLP 3.2.x` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import BERT models trained/fine-tuned for token classification via `BertForTokenClassification` or `TFBertForTokenClassification`. These models are usually under `Token Classification` category and have `bert` in their labels\n",
        "- Reference: [TFBertForTokenClassification](https://huggingface.co/transformers/model_doc/bert.html#tfbertfortokenclassification)\n",
        "- Some [example models](https://huggingface.co/models?filter=bert&pipeline_tag=token-classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzxB-Nq6cxOA"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNQkhyMHMgkE"
      },
      "source": [
        "- Let's install `HuggingFace` and `TensorFlow`. You don't need `TensorFlow` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock TensorFlow on `2.11.0` version and Transformers on `4.25.1`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.25.1 tensorflow==2.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX8rVEiBH-nj",
        "outputId": "697454f0-4c07-456e-ecec-bcfc6cdf7a3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hHXgqiWpMfCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299ae0b8-0322-43d3-ed18-ae95896bb7c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.19.0\n",
            "Cloning into 'onnx-tensorflow'...\n",
            "remote: Enumerating objects: 6516, done.\u001b[K\n",
            "remote: Counting objects: 100% (465/465), done.\u001b[K\n",
            "remote: Compressing objects: 100% (202/202), done.\u001b[K\n",
            "remote: Total 6516 (delta 323), reused 380 (delta 259), pack-reused 6051\u001b[K\n",
            "Receiving objects: 100% (6516/6516), 1.98 MiB | 18.09 MiB/s, done.\n",
            "Resolving deltas: 100% (5050/5050), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/onnx-tensorflow\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnx>=1.10.2\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from onnx-tf==1.10.0) (6.0)\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.8/dist-packages (from onnx-tf==1.10.0) (0.19.0)\n",
            "Requirement already satisfied: tensorflow_probability in /usr/local/lib/python3.8/dist-packages (from onnx-tf==1.10.0) (0.17.0)\n",
            "Collecting protobuf<4,>=3.20.2\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.10.2->onnx-tf==1.10.0) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.10.2->onnx-tf==1.10.0) (1.21.6)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons->onnx-tf==1.10.0) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons->onnx-tf==1.10.0) (21.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (0.1.8)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (2.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (1.3.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (1.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (4.4.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons->onnx-tf==1.10.0) (3.0.9)\n",
            "Installing collected packages: protobuf, onnx, onnx-tf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Running setup.py develop for onnx-tf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.13.0 onnx-tf-1.10.0 protobuf-3.20.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "Installing collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mRunning setup.py install for pytorch\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Running setup.py install for pytorch ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m Encountered error while trying to install package.\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m pytorch\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.13.1+cu116)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons\n",
        "!git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow && pip install -e . \n",
        "!pip install pytorch\n",
        "!pip install torchvision\n",
        "!pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect ggdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSJIEn9lk8BE",
        "outputId": "3df76f41-ed53-4faf-e316-b00f3b8c1473"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3AM6bj4P3NS"
      },
      "source": [
        "- HuggingFace comes with a native `saved_model` feature inside `save_pretrained` function for TensorFlow based models. We will use that to save it as TF `SavedModel`.\n",
        "- We'll use [dslim/bert-base-NER](https://huggingface.co/dslim/bert-base-NER) model from HuggingFace as an example\n",
        "- In addition to `TFBertForTokenClassification` we also need to save the `BertTokenizer`. This is the same for every model, these are assets needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pprint import pprint\n",
        "import string    \n",
        "import random\n",
        "import json\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "#from transformers import BertTokenizer, BertForTokenClassification\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TFAutoModelForTokenClassification"
      ],
      "metadata": {
        "id": "VXuGsv8Q1ibh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bedeb33f-6c54-46fa-ce9a-08c7ed88f2bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TFAutoModelForTokenClassification"
      ],
      "metadata": {
        "id": "W3lhTcEyk-ZZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import BertTokenizer, BertForTokenClassification\n",
        "save_model_address = '/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config'\n",
        "#save_model = BertForTokenClassification.from_pretrained(save_model_address)\n",
        "#tokenizer = BertTokenizer.from_pretrained(save_model_address,do_lower_case=True, model_max_length=256)\n",
        "\n",
        "save_model = TFAutoModelForTokenClassification.from_pretrained(save_model_address, from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_model_address, do_lower_case=True, model_max_length=256)\n",
        "\n",
        "nlp = pipeline(\"ner\", model=save_model, tokenizer=tokenizer, aggregation_strategy='simple',ignore_labels =['X','O'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fsAsozAFfsg",
        "outputId": "7e8a6626-8924-4864-9c98-321753c8b05a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test cau dai\n",
        "orig_string = '''Learn and Master Software Testing Quickly from the experts - GUARANTEED! THE IN-DEPTH SOFTWARE TESTING TRAINING - By SoftwareTestingHelp Team. \"TOP STUDENT PICK\" on Udemy in the Software Testing category! 26+ hours of HD content. Value for money! DON'T settle for other basic courses of less thanhours! Few Student reviews from hundreds ofstar reviews: \"The course is an eye opener into the world of IT. Theophilus. \"Money well spent, excellent delivery. Very informative and practical. Would highly recommend to anyone interested in pursuing software testing as a career. Olanrewaju. \"Truly the best software testing training I have come across both in dept and in substance. Kingsley. \"This is really \"The Best Software Training Course\". I hardly know anything regarding testing, instructor had taken utmost care in providing the knowledge starting from basics, the terminology etc...I am very much satisfied with this course. I strongly recommend this course. Vijaya. \"Great tutorials ..in detail ...learned a lot ...must see tutorial for all testers. Masud. \"The instructor is just a perfect lecturer! Entire course is very informative and useful for software testers as beginners with a lot of practical examples. Who wants to understand principles of testing and main techniques of it - enroll in this course. Oleksii. \"The instructor according to me.....God has gifted her a real talent to be one of the best tutors in this world. Biju. Introducing the Most Practical, Precise and Inexpensive Software Testing Course. It is going to include everything there is to know for you to become a perfect Software Tester. This software testing QA training course is designed by working professionals in a way that, course it will progress from introducing you to the basics of software testing to advanced topics like Software configuration management, creating a test plan, test estimations etc along with introduction and familiarity with Automation testing and test management tools like QTP (intro), QC, JIRA, and Bugzilla. Course Benefits: Syllabus: We came up with a unique list of topics that will help you gradually work your way into the testing world. Practice sessions: Assignments in a way that you will get to apply the theory you learnt immediately. Video sessions of Instructor led live training sessions. Practical learning experienc e with live project work and examples. Support: Our Team is going to be available to you via email or the website for you to reach out to us. Over Lectures and more than+ hours of HD content! Learn Software Testing and Automation basics from a professional trainer from your own desk. Information packed practical training starting from basics to advanced testing techniques. Best suitable for beginners to advanced level users and who learn faster when demonstrated. Get √¢‚Ç¨≈ìCertificate of completion. LIVE PROJECT End to End Software Testing Training Included. Learn Software Testing and Automation basics from a professional trainer from your own desk. Information packed practical training starting from basics to advanced testing techniques. Best suitable for beginners to advanced level users and who learn faster when demonstrated. Course content designed by considering current software testing technology and the job market. Practical assignments at the end of every session. Practical learning experience with live project work and examples. Lifetime enrollment - Pay one time fee and access video training sessions as many times as you want. Resume Preparation Guidance for Testers Included. Software Testing Interview Questions and Preparation Tips Included. Download Real Software Testing Templates like Test Plan, Test Cases and other important Templates. Software Testing Certification Guidance. Learn Test Management Tools like JIRA, and Bugzilla. Get all future course updates free!'''\n",
        "#results = nlp(sentences)\n",
        "#results\n",
        "#len(tokenizer.tokenize(sentences, truncation=True))\n",
        "list_of_lines = []\n",
        "max_length = 256*4\n",
        "while len(orig_string) > max_length:\n",
        "    line_length = max(orig_string[:max_length].rfind(i) for i in \".!?,\")\n",
        "    list_of_lines.append(orig_string[:line_length])\n",
        "    orig_string = orig_string[line_length + 1:]\n",
        "list_of_lines.append(orig_string)\n",
        "list_of_lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR0JRrEU1Wmj",
        "outputId": "829b15ca-55be-4e10-f66d-238533e9f1be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Learn and Master Software Testing Quickly from the experts - GUARANTEED! THE IN-DEPTH SOFTWARE TESTING TRAINING - By SoftwareTestingHelp Team. \"TOP STUDENT PICK\" on Udemy in the Software Testing category! 26+ hours of HD content. Value for money! DON\\'T settle for other basic courses of less thanhours! Few Student reviews from hundreds ofstar reviews: \"The course is an eye opener into the world of IT. Theophilus. \"Money well spent, excellent delivery. Very informative and practical. Would highly recommend to anyone interested in pursuing software testing as a career. Olanrewaju. \"Truly the best software testing training I have come across both in dept and in substance. Kingsley. \"This is really \"The Best Software Training Course\". I hardly know anything regarding testing, instructor had taken utmost care in providing the knowledge starting from basics, the terminology etc...I am very much satisfied with this course. I strongly recommend this course. Vijaya. \"Great tutorials ..in detail ...learned a lot ..',\n",
              " 'must see tutorial for all testers. Masud. \"The instructor is just a perfect lecturer! Entire course is very informative and useful for software testers as beginners with a lot of practical examples. Who wants to understand principles of testing and main techniques of it - enroll in this course. Oleksii. \"The instructor according to me.....God has gifted her a real talent to be one of the best tutors in this world. Biju. Introducing the Most Practical, Precise and Inexpensive Software Testing Course. It is going to include everything there is to know for you to become a perfect Software Tester. This software testing QA training course is designed by working professionals in a way that, course it will progress from introducing you to the basics of software testing to advanced topics like Software configuration management, creating a test plan, test estimations etc along with introduction and familiarity with Automation testing and test management tools like QTP (intro), QC, JIRA, and Bugzilla',\n",
              " ' Course Benefits: Syllabus: We came up with a unique list of topics that will help you gradually work your way into the testing world. Practice sessions: Assignments in a way that you will get to apply the theory you learnt immediately. Video sessions of Instructor led live training sessions. Practical learning experienc e with live project work and examples. Support: Our Team is going to be available to you via email or the website for you to reach out to us. Over Lectures and more than+ hours of HD content! Learn Software Testing and Automation basics from a professional trainer from your own desk. Information packed practical training starting from basics to advanced testing techniques. Best suitable for beginners to advanced level users and who learn faster when demonstrated. Get √¢‚Ç¨≈ìCertificate of completion. LIVE PROJECT End to End Software Testing Training Included. Learn Software Testing and Automation basics from a professional trainer from your own desk',\n",
              " ' Information packed practical training starting from basics to advanced testing techniques. Best suitable for beginners to advanced level users and who learn faster when demonstrated. Course content designed by considering current software testing technology and the job market. Practical assignments at the end of every session. Practical learning experience with live project work and examples. Lifetime enrollment - Pay one time fee and access video training sessions as many times as you want. Resume Preparation Guidance for Testers Included. Software Testing Interview Questions and Preparation Tips Included. Download Real Software Testing Templates like Test Plan, Test Cases and other important Templates. Software Testing Certification Guidance. Learn Test Management Tools like JIRA, and Bugzilla. Get all future course updates free!']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix ky tu bang thu vien fix that for you\n",
        "import ftfy\n",
        "results = nlp(ftfy.fix_text(list_of_lines[0]))\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA8Tq5eADEiv",
        "outputId": "f6cfd29a-a0ed-4870-b838-10306cbff504"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'KNOW',\n",
              "  'score': 0.9037937,\n",
              "  'word': 'software testing',\n",
              "  'start': 17,\n",
              "  'end': 33},\n",
              " {'entity_group': 'KNOW',\n",
              "  'score': 0.8402617,\n",
              "  'word': 'software testing',\n",
              "  'start': 86,\n",
              "  'end': 102},\n",
              " {'entity_group': 'KNOW',\n",
              "  'score': 0.8576295,\n",
              "  'word': 'software testing',\n",
              "  'start': 543,\n",
              "  'end': 559},\n",
              " {'entity_group': 'KNOW',\n",
              "  'score': 0.8752378,\n",
              "  'word': 'software testing',\n",
              "  'start': 601,\n",
              "  'end': 617}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fixed = ftfy.fix_text(list_of_lines[0])\n",
        "fixed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "Nyz_r2XvJx6a",
        "outputId": "5928c76d-2c0d-4a70-a94f-3bf192d2d31b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Learn and Master Software Testing Quickly from the experts - GUARANTEED! THE IN-DEPTH SOFTWARE TESTING TRAINING - By SoftwareTestingHelp Team. \"TOP STUDENT PICK\" on Udemy in the Software Testing category! 26+ hours of HD content. Value for money! DON\\'T settle for other basic courses of less thanhours! Few Student reviews from hundreds ofstar reviews: \"The course is an eye opener into the world of IT. Theophilus. \"Money well spent, excellent delivery. Very informative and practical. Would highly recommend to anyone interested in pursuing software testing as a career. Olanrewaju. \"Truly the best software testing training I have come across both in dept and in substance. Kingsley. \"This is really \"The Best Software Training Course\". I hardly know anything regarding testing, instructor had taken utmost care in providing the knowledge starting from basics, the terminology etc...I am very much satisfied with this course. I strongly recommend this course. Vijaya. \"Great tutorials ..in detail ...learned a lot ..'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = '/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config'\n",
        "try:\n",
        "  print('try downloading TF weights')\n",
        "  save_model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n",
        "except:\n",
        "  print('try downloading PyTorch weights')\n",
        "  save_model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, from_pt=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBAEYStmYmWj",
        "outputId": "fb9cd7dc-f197-4324-aea5-3d37e098d65a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "try downloading TF weights\n",
            "try downloading PyTorch weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(save_model_address, do_lower_case=True, model_max_length=256)\n",
        "tokenizer.save_pretrained('.{}'.format(MODEL_NAME))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkg4B4_dm5MI",
        "outputId": "39928ea1-ba9a-481d-9125-b4ade1f2eef3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config/tokenizer_config.json',\n",
              " './content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config/special_tokens_map.json',\n",
              " './content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config/vocab.txt',\n",
              " './content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config/added_tokens.json',\n",
              " './content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\n",
        "    fixed, \n",
        "    return_attention_mask=False,\n",
        "    truncation=True,\n",
        "    return_special_tokens_mask=True,\n",
        "    return_offsets_mapping=tokenizer.is_fast,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6b1vY1GNgO0",
        "outputId": "f6c95c63-6139-42d0-891b-0be077fd65cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  4553,  1998,  3040,  4007,  5604,  2855,  2013,  1996,  8519,\n",
              "          1011, 12361,   999,  1996,  1999,  1011,  5995,  4007,  5604,  2731,\n",
              "          1011,  2011,  4007, 22199,  2075, 16001,  2361,  2136,  1012,  1000,\n",
              "          2327,  3076,  4060,  1000,  2006, 20904, 26662,  1999,  1996,  4007,\n",
              "          5604,  4696,   999,  2656,  1009,  2847,  1997, 10751,  4180,  1012,\n",
              "          3643,  2005,  2769,   999,  2123,  1005,  1056,  7392,  2005,  2060,\n",
              "          3937,  5352,  1997,  2625,  2084,  6806,  9236,   999,  2261,  3076,\n",
              "          4391,  2013,  5606,  1997, 14117,  4391,  1024,  1000,  1996,  2607,\n",
              "          2003,  2019,  3239, 16181,  2046,  1996,  2088,  1997,  2009,  1012,\n",
              "         14833, 21850,  7393,  1012,  1000,  2769,  2092,  2985,  1010,  6581,\n",
              "          6959,  1012,  2200, 12367,  8082,  1998,  6742,  1012,  2052,  3811,\n",
              "         16755,  2000,  3087,  4699,  1999, 11828,  4007,  5604,  2004,  1037,\n",
              "          2476,  1012, 19330,  2319, 15603, 13006,  2226,  1012,  1000,  5621,\n",
              "          1996,  2190,  4007,  5604,  2731,  1045,  2031,  2272,  2408,  2119,\n",
              "          1999, 29466,  1998,  1999,  9415,  1012, 22819,  1012,  1000,  2023,\n",
              "          2003,  2428,  1000,  1996,  2190,  4007,  2731,  2607,  1000,  1012,\n",
              "          1045,  6684,  2113,  2505,  4953,  5604,  1010,  9450,  2018,  2579,\n",
              "         27917,  2729,  1999,  4346,  1996,  3716,  3225,  2013, 24078,  1010,\n",
              "          1996, 18444,  4385,  1012,  1012,  1012,  1045,  2572,  2200,  2172,\n",
              "          8510,  2007,  2023,  2607,  1012,  1045,  6118, 16755,  2023,  2607,\n",
              "          1012, 17027,  2050,  1012,  1000,  2307, 14924, 26340,  1012,  1012,\n",
              "          1999,  6987,  1012,  1012,  1012,  4342,  1037,  2843,  1012,  1012,\n",
              "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 1]]), 'offset_mapping': tensor([[[   0,    0],\n",
              "         [   0,    5],\n",
              "         [   6,    9],\n",
              "         [  10,   16],\n",
              "         [  17,   25],\n",
              "         [  26,   33],\n",
              "         [  34,   41],\n",
              "         [  42,   46],\n",
              "         [  47,   50],\n",
              "         [  51,   58],\n",
              "         [  59,   60],\n",
              "         [  61,   71],\n",
              "         [  71,   72],\n",
              "         [  73,   76],\n",
              "         [  77,   79],\n",
              "         [  79,   80],\n",
              "         [  80,   85],\n",
              "         [  86,   94],\n",
              "         [  95,  102],\n",
              "         [ 103,  111],\n",
              "         [ 112,  113],\n",
              "         [ 114,  116],\n",
              "         [ 117,  125],\n",
              "         [ 125,  129],\n",
              "         [ 129,  132],\n",
              "         [ 132,  135],\n",
              "         [ 135,  136],\n",
              "         [ 137,  141],\n",
              "         [ 141,  142],\n",
              "         [ 143,  144],\n",
              "         [ 144,  147],\n",
              "         [ 148,  155],\n",
              "         [ 156,  160],\n",
              "         [ 160,  161],\n",
              "         [ 162,  164],\n",
              "         [ 165,  167],\n",
              "         [ 167,  170],\n",
              "         [ 171,  173],\n",
              "         [ 174,  177],\n",
              "         [ 178,  186],\n",
              "         [ 187,  194],\n",
              "         [ 195,  203],\n",
              "         [ 203,  204],\n",
              "         [ 205,  207],\n",
              "         [ 207,  208],\n",
              "         [ 209,  214],\n",
              "         [ 215,  217],\n",
              "         [ 218,  220],\n",
              "         [ 221,  228],\n",
              "         [ 228,  229],\n",
              "         [ 230,  235],\n",
              "         [ 236,  239],\n",
              "         [ 240,  245],\n",
              "         [ 245,  246],\n",
              "         [ 247,  250],\n",
              "         [ 250,  251],\n",
              "         [ 251,  252],\n",
              "         [ 253,  259],\n",
              "         [ 260,  263],\n",
              "         [ 264,  269],\n",
              "         [ 270,  275],\n",
              "         [ 276,  283],\n",
              "         [ 284,  286],\n",
              "         [ 287,  291],\n",
              "         [ 292,  296],\n",
              "         [ 296,  298],\n",
              "         [ 298,  301],\n",
              "         [ 301,  302],\n",
              "         [ 303,  306],\n",
              "         [ 307,  314],\n",
              "         [ 315,  322],\n",
              "         [ 323,  327],\n",
              "         [ 328,  336],\n",
              "         [ 337,  339],\n",
              "         [ 339,  343],\n",
              "         [ 344,  351],\n",
              "         [ 351,  352],\n",
              "         [ 353,  354],\n",
              "         [ 354,  357],\n",
              "         [ 358,  364],\n",
              "         [ 365,  367],\n",
              "         [ 368,  370],\n",
              "         [ 371,  374],\n",
              "         [ 375,  381],\n",
              "         [ 382,  386],\n",
              "         [ 387,  390],\n",
              "         [ 391,  396],\n",
              "         [ 397,  399],\n",
              "         [ 400,  402],\n",
              "         [ 402,  403],\n",
              "         [ 404,  408],\n",
              "         [ 408,  411],\n",
              "         [ 411,  414],\n",
              "         [ 414,  415],\n",
              "         [ 416,  417],\n",
              "         [ 417,  422],\n",
              "         [ 423,  427],\n",
              "         [ 428,  433],\n",
              "         [ 433,  434],\n",
              "         [ 435,  444],\n",
              "         [ 445,  453],\n",
              "         [ 453,  454],\n",
              "         [ 455,  459],\n",
              "         [ 460,  466],\n",
              "         [ 466,  471],\n",
              "         [ 472,  475],\n",
              "         [ 476,  485],\n",
              "         [ 485,  486],\n",
              "         [ 487,  492],\n",
              "         [ 493,  499],\n",
              "         [ 500,  509],\n",
              "         [ 510,  512],\n",
              "         [ 513,  519],\n",
              "         [ 520,  530],\n",
              "         [ 531,  533],\n",
              "         [ 534,  542],\n",
              "         [ 543,  551],\n",
              "         [ 552,  559],\n",
              "         [ 560,  562],\n",
              "         [ 563,  564],\n",
              "         [ 565,  571],\n",
              "         [ 571,  572],\n",
              "         [ 573,  575],\n",
              "         [ 575,  577],\n",
              "         [ 577,  580],\n",
              "         [ 580,  582],\n",
              "         [ 582,  583],\n",
              "         [ 583,  584],\n",
              "         [ 585,  586],\n",
              "         [ 586,  591],\n",
              "         [ 592,  595],\n",
              "         [ 596,  600],\n",
              "         [ 601,  609],\n",
              "         [ 610,  617],\n",
              "         [ 618,  626],\n",
              "         [ 627,  628],\n",
              "         [ 629,  633],\n",
              "         [ 634,  638],\n",
              "         [ 639,  645],\n",
              "         [ 646,  650],\n",
              "         [ 651,  653],\n",
              "         [ 654,  658],\n",
              "         [ 659,  662],\n",
              "         [ 663,  665],\n",
              "         [ 666,  675],\n",
              "         [ 675,  676],\n",
              "         [ 677,  685],\n",
              "         [ 685,  686],\n",
              "         [ 687,  688],\n",
              "         [ 688,  692],\n",
              "         [ 693,  695],\n",
              "         [ 696,  702],\n",
              "         [ 703,  704],\n",
              "         [ 704,  707],\n",
              "         [ 708,  712],\n",
              "         [ 713,  721],\n",
              "         [ 722,  730],\n",
              "         [ 731,  737],\n",
              "         [ 737,  738],\n",
              "         [ 738,  739],\n",
              "         [ 740,  741],\n",
              "         [ 742,  748],\n",
              "         [ 749,  753],\n",
              "         [ 754,  762],\n",
              "         [ 763,  772],\n",
              "         [ 773,  780],\n",
              "         [ 780,  781],\n",
              "         [ 782,  792],\n",
              "         [ 793,  796],\n",
              "         [ 797,  802],\n",
              "         [ 803,  809],\n",
              "         [ 810,  814],\n",
              "         [ 815,  817],\n",
              "         [ 818,  827],\n",
              "         [ 828,  831],\n",
              "         [ 832,  841],\n",
              "         [ 842,  850],\n",
              "         [ 851,  855],\n",
              "         [ 856,  862],\n",
              "         [ 862,  863],\n",
              "         [ 864,  867],\n",
              "         [ 868,  879],\n",
              "         [ 880,  883],\n",
              "         [ 883,  884],\n",
              "         [ 884,  885],\n",
              "         [ 885,  886],\n",
              "         [ 886,  887],\n",
              "         [ 888,  890],\n",
              "         [ 891,  895],\n",
              "         [ 896,  900],\n",
              "         [ 901,  910],\n",
              "         [ 911,  915],\n",
              "         [ 916,  920],\n",
              "         [ 921,  927],\n",
              "         [ 927,  928],\n",
              "         [ 929,  930],\n",
              "         [ 931,  939],\n",
              "         [ 940,  949],\n",
              "         [ 950,  954],\n",
              "         [ 955,  961],\n",
              "         [ 961,  962],\n",
              "         [ 963,  968],\n",
              "         [ 968,  969],\n",
              "         [ 969,  970],\n",
              "         [ 971,  972],\n",
              "         [ 972,  977],\n",
              "         [ 978,  983],\n",
              "         [ 983,  987],\n",
              "         [ 988,  989],\n",
              "         [ 989,  990],\n",
              "         [ 990,  992],\n",
              "         [ 993,  999],\n",
              "         [1000, 1001],\n",
              "         [1001, 1002],\n",
              "         [1002, 1003],\n",
              "         [1003, 1010],\n",
              "         [1011, 1012],\n",
              "         [1013, 1016],\n",
              "         [1017, 1018],\n",
              "         [1018, 1019],\n",
              "         [   0,    0]]])}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.is_fast:\n",
        "    offset_mapping = tokens.pop(\"offset_mapping\").cpu().numpy()[0]\n",
        "elif offset_mappings:\n",
        "    offset_mapping = offset_mappings[i]\n",
        "else:\n",
        "    offset_mapping = None\n",
        "\n",
        "special_tokens_mask = tokens.pop(\"special_tokens_mask\").cpu().numpy()[0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    entities = save_model(**tokens)[0][0].cpu().numpy()\n",
        "    input_ids = tokens[\"input_ids\"].cpu().numpy()[0]\n"
      ],
      "metadata": {
        "id": "RwYvSV-JNrs8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6c6a9d6-47e1-4e85-e696-d232d2bc7937"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a1d79c9ab8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36minput_processing\u001b[0;34m(func, config, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Data of type {type(v)} is not allowed only {allowed_types} is accepted for {k}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_bert_for_token_classification_1' (type TFBertForTokenClassification).\n\nData of type <class 'torch.Tensor'> is not allowed only (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>, <class 'keras.engine.keras_tensor.KerasTensor'>) is accepted for token_type_ids.\n\nCall arguments received by layer 'tf_bert_for_token_classification_1' (type TFBertForTokenClassification):\n  ‚Ä¢ input_ids=tensor([[  101,  4553,  1998,  3040,  4007,  5604,  2855,  2013,  1996,  8519,\n          1011, 12361,   999,  1996,  1999,  1011,  5995,  4007,  5604,  2731,\n          1011,  2011,  4007, 22199,  2075, 16001,  2361,  2136,  1012,  1000,\n          2327,  3076,  4060,  1000,  2006, 20904, 26662,  1999,  1996,  4007,\n          5604,  4696,   999,  2656,  1009,  2847,  1997, 10751,  4180,  1012,\n          3643,  2005,  2769,   999,  2123,  1005,  1056,  7392,  2005,  2060,\n          3937,  5352,  1997,  2625,  2084,  6806,  9236,   999,  2261,  3076,\n          4391,  2013,  5606,  1997, 14117,  4391,  1024,  1000,  1996,  2607,\n          2003,  2019,  3239, 16181,  2046,  1996,  2088,  1997,  2009,  1012,\n         14833, 21850,  7393,  1012,  1000,  2769,  2092,  2985,  1010,  6581,\n          6959,  1012,  2200, 12367,  8082,  1998,  6742,  1012,  2052,  3811,\n         16755,  2000,  3087,  4699,  1999, 11828,  4007,  5604,  2004,  1037,\n          2476,  1012, 19330,  2319, 15603, 13006,  2226,  1012,  1000,  5621,\n          1996,  2190,  4007,  5604,  2731,  1045,  2031,  2272,  2408,  2119,\n          1999, 29466,  1998,  1999,  9415,  1012, 22819,  1012,  1000,  2023,\n          2003,  2428,  1000,  1996,  2190,  4007,  2731,  2607,  1000,  1012,\n          1045,  6684,  2113,  2505,  4953,  5604,  1010,  9450,  2018,  2579,\n         27917,  2729,  1999,  4346,  1996,  3716,  3225,  2013, 24078,  1010,\n          1996, 18444,  4385,  1012,  1012,  1012,  1045,  2572,  2200,  2172,\n          8510,  2007,  2023,  2607,  1012,  1045,  6118, 16755,  2023,  2607,\n          1012, 17027,  2050,  1012,  1000,  2307, 14924, 26340,  1012,  1012,\n          1999,  6987,  1012,  1012,  1012,  4342,  1037,  2843,  1012,  1012,\n           102]])\n  ‚Ä¢ attention_mask=None\n  ‚Ä¢ token_type_ids=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])\n  ‚Ä¢ position_ids=None\n  ‚Ä¢ head_mask=None\n  ‚Ä¢ inputs_embeds=None\n  ‚Ä¢ output_attentions=None\n  ‚Ä¢ output_hidden_states=None\n  ‚Ä¢ return_dict=None\n  ‚Ä¢ labels=None\n  ‚Ä¢ training=False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.onnx.export(save_model, **tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "b1w755NbCqrC",
        "outputId": "c1213f68-1af3-4c27-d4fe-f2d52bd3189e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-45f2d3eff90b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: export() got an unexpected keyword argument 'input_ids'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "MODEL_NAME = '/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config'\n",
        "\n",
        "# Define TF Signature\n",
        "@tf.function(\n",
        "  input_signature=[\n",
        "      {\n",
        "          \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n",
        "          \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n",
        "          \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n",
        "      }\n",
        "  ]\n",
        ")\n",
        "def serving_fn(input):\n",
        "    return save_model(input)\n",
        "\n",
        "save_model.save_pretrained(\"{}/converting\".format(MODEL_NAME), saved_model=True, signatures={\"serving_default\": serving_fn})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arqtR23l1380",
        "outputId": "238cbbb0-c395-4c8e-d6a0-be8000bd4cbd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, LayerNorm_layer_call_fn while saving (showing 5 of 416). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tree -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNhE3V7g88WR",
        "outputId": "a0223794-84b5-48eb-dfed-bf6c76a0b973"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 43.0 kB of archives.\n",
            "After this operation, 115 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tree amd64 1.8.0-1 [43.0 kB]\n",
            "Fetched 43.0 kB in 0s (352 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 129496 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.8.0-1_amd64.deb ...\n",
            "Unpacking tree (1.8.0-1) ...\n",
            "Setting up tree (1.8.0-1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree {MODEL_NAME}/converting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkPcuCBg8z6G",
        "outputId": "b880c4f4-5700-41bb-ce36-54e3bc3059b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config/converting\u001b[00m\n",
            "‚îú‚îÄ‚îÄ config.json\n",
            "‚îú‚îÄ‚îÄ \u001b[01;34msaved_model\u001b[00m\n",
            "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ \u001b[01;34m1\u001b[00m\n",
            "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ \u001b[01;34massets\u001b[00m\n",
            "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ fingerprint.pb\n",
            "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ keras_metadata.pb\n",
            "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ saved_model.pb\n",
            "‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ \u001b[01;34mvariables\u001b[00m\n",
            "‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ variables.data-00000-of-00001\n",
            "‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ variables.index\n",
            "‚îî‚îÄ‚îÄ tf_model.h5\n",
            "\n",
            "4 directories, 7 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree {MODEL_NAME}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxkmPxl0VPpv",
        "outputId": "ee11da57-7848-40da-fbdd-f64078851b3e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config\u001b[00m\n",
            "‚îú‚îÄ‚îÄ config.json\n",
            "‚îú‚îÄ‚îÄ \u001b[01;34mconverting\u001b[00m\n",
            "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.json\n",
            "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ \u001b[01;34msaved_model\u001b[00m\n",
            "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ \u001b[01;34m1\u001b[00m\n",
            "‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ \u001b[01;34massets\u001b[00m\n",
            "‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ fingerprint.pb\n",
            "‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ keras_metadata.pb\n",
            "‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ saved_model.pb\n",
            "‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ \u001b[01;34mvariables\u001b[00m\n",
            "‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ variables.data-00000-of-00001\n",
            "‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ variables.index\n",
            "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tf_model.h5\n",
            "‚îú‚îÄ‚îÄ eval_results.txt\n",
            "‚îú‚îÄ‚îÄ pytorch_model.bin\n",
            "‚îî‚îÄ‚îÄ vocab.txt\n",
            "\n",
            "5 directories, 11 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r {MODEL_NAME}/my_model_tf/saved_model/1/assets {MODEL_NAME}/converting/saved_model/1"
      ],
      "metadata": {
        "id": "Q_yqGgWKVRRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tree {MODEL_NAME}/converting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDPcaHZN3I2W",
        "outputId": "a5c973ee-0383-4d09-9271-1af464102f9a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config/converting\u001b[00m\n",
            "‚îú‚îÄ‚îÄ config.json\n",
            "‚îú‚îÄ‚îÄ \u001b[01;34msaved_model\u001b[00m\n",
            "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ \u001b[01;34m1\u001b[00m\n",
            "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ \u001b[01;34massets\u001b[00m\n",
            "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ fingerprint.pb\n",
            "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ keras_metadata.pb\n",
            "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ saved_model.pb\n",
            "‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ \u001b[01;34mvariables\u001b[00m\n",
            "‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ variables.data-00000-of-00001\n",
            "‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ variables.index\n",
            "‚îî‚îÄ‚îÄ tf_model.h5\n",
            "\n",
            "4 directories, 7 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asset_path = '{}/saved_model/1/assets'.format(MODEL_NAME)\n",
        "\n",
        "!cp {MODEL_NAME}/vocab.txt {asset_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccV_GlTMoLHL",
        "outputId": "45e56f0f-9613-4bec-f3b2-4e2449bc68a0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file '/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config/saved_model/1/assets': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get label2id dictionary \n",
        "labels = save_model.config.label2id\n",
        "# sort the dictionary based on the id\n",
        "labels = sorted(labels, key=labels.get)\n",
        "\n",
        "with open(asset_path+'/labels.txt', 'w') as f:\n",
        "    f.write('\\n'.join(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "MzDwOX4KoeEP",
        "outputId": "0ab28d56-4e13-455c-b12f-d00791c25834"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-abd42a5902ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/labels.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config/saved_model/1/assets/labels.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFcmW0fdVghJ",
        "outputId": "733a6f1d-fc2a-471d-f4b9-f3a0dc933c36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing PySpark 3.2.3 and Spark NLP 4.2.8\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 4.2.8\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m453.8/453.8 KB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Spark NLP from PyPI\n",
        "!pip install spark-nlp==4.2.8\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m50vR5uKUOT",
        "outputId": "903be705-7ad5-4f57-e2c6-42e1f730f190"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spark-nlp==4.2.8 in /usr/local/lib/python3.8/dist-packages (4.2.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=c79182a1832032d98bfd796278b6202b1f4dd7b4975e07fac6ac8a2eecd23f13\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "# let's start Spark with Spark NLP\n",
        "#spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "NRM3cr3QVioV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark NLP\")\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .config(\"spark.driver.memory\",\"16G\")\\\n",
        "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
        "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.8\")\\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "NHynUipGHp4j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import *\n",
        "from sparknlp.training import CoNLL\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *"
      ],
      "metadata": {
        "id": "2iOxLyeVmXie"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = CoNLL().readDataset(spark = spark, path = '/content/drive/MyDrive/Data_Science/thesis/ML_NER/Train_Dataset/*')\n",
        "training_date.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "FgCOmkKJqqg4",
        "outputId": "147cdd97-dd75-4646-aaf5-5375c676b027"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-27cb9bc9aff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoNLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Data_Science/thesis/ML_NER/Train_Dataset/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'training_date' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = '/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config'\n",
        "training_data = CoNLL().readDataset(spark = spark, path = '/content/drive/MyDrive/Data_Science/thesis/ML_NER/Train_Dataset/*')\n",
        "\n",
        "bert = BertEmbeddings.pretrained('bert_base_cased','en') \\\n",
        " .setInputCols([\"sentence\",'token'])\\\n",
        " .setOutputCol(\"bert\")\n",
        "\n",
        "nerTagger = NerDLApproach() \\\n",
        " .setInputCols([\"sentence\",'token','bert'])\\\n",
        " .setLabelColumn(\"label\") \\\n",
        " .setOutputCol(\"ner\") \\\n",
        " .setMaxEpochs(1) \\\n",
        " .setEnableMemoryOptimizer(True)\n",
        "\n",
        "pipeline = Pipeline(stages = [bert, nerTagger])\n",
        "\n",
        "ner_model = pipeline.fit(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzcnfa6WmnFG",
        "outputId": "acd59cb3-fae6-4de4-9798-69b8ebb59f86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model.stages[1].write().save('/content/drive/MyDrive/Data_Science/thesis/NERModel_embeddings/NER_bert_1')"
      ],
      "metadata": {
        "id": "qMdt6k_iQEXo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import *\n",
        "MODEL_NAME = '/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config'\n",
        "bert = BertForTokenClassification.loadSavedModel(\n",
        "     '{}/converting/saved_model/1'.format(MODEL_NAME),\n",
        "     spark\n",
        " )\\\n",
        " .setInputCols([\"document\",'token'])\\\n",
        " .setOutputCol(\"ner\")\\\n",
        " .setCaseSensitive(True)\\\n",
        " .setMaxSentenceLength(128)"
      ],
      "metadata": {
        "id": "Iz1p_xME3Q2F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "ceca4ce0-0ad6-451c-feab-10c56fd99e08"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionRefusedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b3a79322b039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Data_Science/thesis/NER_new/NERModel_config'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m berta = BertForTokenClassification.loadSavedModel(\n\u001b[0m\u001b[1;32m      4\u001b[0m      \u001b[0;34m'{}/converting/saved_model/1'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sparknlp/annotator/classifier_dl/bert_for_token_classification.py\u001b[0m in \u001b[0;36mloadSavedModel\u001b[0;34m(folder, spark_session)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_BertTokenClassifierLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mjModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_BertTokenClassifierLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBertForTokenClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sparknlp/internal/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, jspark)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_BertTokenClassifierLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtendedJavaWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         super(_BertTokenClassifierLoader, self).__init__(\n\u001b[0m\u001b[1;32m     71\u001b[0m             \"com.johnsnowlabs.nlp.annotators.classifier.dl.BertForTokenClassification.loadSavedModel\", path, jspark)\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sparknlp/internal/extended_java_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtendedJavaWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sparknlp/internal/extended_java_wrapper.py\u001b[0m in \u001b[0;36mnew_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpylist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1710\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert.write().overwrite().save(\"./{}\".format(MODEL_NAME))"
      ],
      "metadata": {
        "id": "5RZIx0Y7X0mR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {MODEL_NAME}_tokenizer {MODEL_NAME}"
      ],
      "metadata": {
        "id": "5vI0v-w6YLL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -l {MODEL_NAME}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCo5ZJcjYb3J",
        "outputId": "1de495a0-da45-4995-fcb3-370ea5eca5a0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 425712\n",
            "-rw------- 1 root root      1136 Sep 26 01:35 config.json\n",
            "drwx------ 3 root root      4096 Feb  2 05:33 converting\n",
            "-rw------- 1 root root       554 Sep 26 01:35 eval_results.txt\n",
            "-rw------- 1 root root 435689969 Sep 26 01:35 pytorch_model.bin\n",
            "-rw------- 1 root root    231508 Sep 26 01:35 vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenClassifier_loaded = BertForTokenClassification.load(\"./{}\".format(MODEL_NAME))\\\n",
        "  .setInputCols([\"document\",'token'])\\\n",
        "  .setOutputCol(\"ner\")"
      ],
      "metadata": {
        "id": "XwP2iZHQZ2Xz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenClassifier_loaded.getClasses()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSf7AsOtZ-uI",
        "outputId": "a0ce3dcb-dd62-49fd-b49e-857143461b10"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I-TOOL',\n",
              " 'B-TOOL',\n",
              " 'I-KNOW',\n",
              " '[SEP]',\n",
              " 'B-LANG',\n",
              " 'I-LANG',\n",
              " 'B-FRAM',\n",
              " 'I-FRAM',\n",
              " 'B-KNOW',\n",
              " 'I-PLAT',\n",
              " '[CLS]',\n",
              " 'O',\n",
              " 'B-PLAT']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import *\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol('text') \\\n",
        "    .setOutputCol('document')\n",
        "\n",
        "sentence = SentenceDetector() \\\n",
        "    .setInputCols(['document']) \\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(['document']) \\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "bert = BertEmbeddings.pretrained('bert_base_cased','en') \\\n",
        " .setInputCols([\"sentence\",'token'])\\\n",
        " .setOutputCol(\"bert\") \\\n",
        " .setCaseSensitive(True)\n",
        "\n",
        "loaded_ner_model = NerDLModel.load(\"/content/drive/MyDrive/Data_Science/thesis/NERModel_embeddings/NER_bert_1\")\\\n",
        "   .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
        "   .setOutputCol(\"ner\")\n",
        "\n",
        "converter = NerConverter()\\\n",
        "    .setInputCols([\"document\",\"token\",\"ner\"])\\\n",
        "    .setOutputCol(\"ner_span\")\n",
        "\n",
        "custom_ner_pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    sentence,\n",
        "    tokenizer,\n",
        "    bert,\n",
        "    loaded_ner_model,\n",
        "    converter\n",
        "])\n",
        "'''\n",
        "custom_ner_pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    tokenClassifier_loaded,\n",
        "    converter    \n",
        "])\n",
        "'''\n",
        "# couple of simple examples\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "P6eI7HX1aErb",
        "outputId": "2ff819ed-8e98-4fe5-853b-495c8fe2a3f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncustom_ner_pipeline = Pipeline(stages=[\\n    document_assembler,\\n    tokenizer,\\n    tokenClassifier_loaded,\\n    converter    \\n])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"software testing Quickly from the experts - GUARANTEED! THE IN-DEPTH software testing TRAINING - By SoftwareTestingHelp Team. on Udemy in the software testing category!\"\n",
        "prediction_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "prediction_model = custom_ner_pipeline.fit(prediction_data)\n",
        "preds = prediction_model.transform(prediction_data)\n",
        "preds.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVKmD1_dVaZT",
        "outputId": "f2376949-29a3-4b6c-8699-c3848ecceb7b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                bert|                 ner|            ner_span|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|software testing ...|[{document, 0, 16...|[{document, 0, 54...|[{token, 0, 7, so...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 15, s...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, col, monotonically_increasing_id\n",
        "preds = preds.withColumn(\"id\", monotonically_increasing_id())"
      ],
      "metadata": {
        "id": "GjgbuTWlbjPj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-soXojUXNx5",
        "outputId": "bf4b82b9-2bf6-4b56-998c-e1df14dfb067"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
            "|                text|            document|            sentence|               token|                bert|                 ner|            ner_span|         id|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
            "|software testing ...|[{document, 0, 16...|[{document, 0, 54...|[{token, 0, 7, so...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 15, s...|94489280512|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "test = test.withColumn(\"result\", col(\"result\").cast(\"string\")) \\\n",
        "  .withColumn(\"metadata\", col(\"metadata\").cast(\"string\")) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "qcqL7xlfXrhU",
        "outputId": "d3c413ea-ee0a-4bfe-e293-89ffb099ef75"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-c99cee11b57b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'withColumn'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.write.options(header='True', delimiter=',', quotes = '\"') \\\n",
        " .csv(\"/content/drive/MyDrive/Data_Science/thesis/result\")"
      ],
      "metadata": {
        "id": "esvl_7u0XliO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.select(\"ner_span.result\",\"ner_span.metadata\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPYCLctndPhe",
        "outputId": "2c7dbfcb-822f-4025-ae45-ca13d9104829"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|              result|            metadata|\n",
            "+--------------------+--------------------+\n",
            "|[software testing...|[{entity -> KNOW,...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "preds.select(F.explode(F.arrays_zip(\"ner_span.result\",\"ner_span.metadata\")).alias(\"entities\"), 'id') \\\n",
        ".select(F.expr(\"entities['result']\").alias(\"chunk\"), \n",
        "        F.expr(\"entities['metadata'].entity\").alias(\"entity\"), 'id'). \\\n",
        "        show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft3Kco18V3sp",
        "outputId": "81a53eb4-2255-431f-e486-49d61bda5513"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------+-----------+\n",
            "|chunk           |entity|id         |\n",
            "+----------------+------+-----------+\n",
            "|software testing|KNOW  |94489280512|\n",
            "|software testing|KNOW  |94489280512|\n",
            "+----------------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = spark.createDataFrame([[\"software testing Quickly from the experts - GUARANTEED! THE IN-DEPTH software testing TRAINING - By SoftwareTestingHelp Team. on Udemy in the software testing category!\"]]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(example).transform(example)\n",
        "\n",
        "# result is a DataFrame\n",
        "result.select(\"text\", \"ner.result\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "Ofra7dsdUxi3",
        "outputId": "aa846efb-6e30-4352-810c-532e6e48cc72"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7e483934a7c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"software testing Quickly from the experts - GUARANTEED! THE IN-DEPTH software testing TRAINING - By SoftwareTestingHelp Team. on Udemy in the software testing category!\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# result is a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Wrong or missing inputCols annotators in BERT_EMBEDDINGS_a6fae8f56338.\n\nCurrent inputCols: sentence,token. Dataset's columns:\n(column_name=text,is_nlp_annotator=false).\nMake sure such annotators exist in your pipeline, with the right output names and that they have following annotator types: document, token"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = spark.createDataFrame([[\"software testing Quickly from the experts - GUARANTEED! THE IN-DEPTH software testing TRAINING - By SoftwareTestingHelp Team. on Udemy in the software testing category!\"]]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(example).transform(example)\n",
        "\n",
        "# result is a DataFrame\n",
        "result.select(\"text\", \"ner.result\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtCsKbjTicaS",
        "outputId": "3b999ad7-2939-444b-cfcc-16b4fbe52964"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|              result|\n",
            "+--------------------+--------------------+\n",
            "|software testing ...|[B-KNOW, I-KNOW, ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "joQkoyL8idlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option('header','true').csv('/content/drive/MyDrive/Data_Science/thesis/K18/File K18/sample_crawl_dataset/Coursera_DataScience.csv',inferSchema=True, escape = '\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bjN0_6umbUbP",
        "outputId": "2ae51fcc-7763-4ee4-8991-705028dc4c4c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f5a7317f2dc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Data_Science/thesis/K18/File K18/sample_crawl_dataset/Coursera_DataScience.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o679.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error accessing configuration file\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:582)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1173)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.nextProviderClass(ServiceLoader.java:1206)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1221)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1265)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1300)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1385)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\n\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\n\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\n\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)\n\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:652)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.nio.file.NoSuchFileException: /usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-sql_2.12-3.2.3.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1764)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1259)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:831)\n\tat java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:857)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:846)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:248)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:177)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:350)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1165)\n\t... 34 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lower, col\n",
        "df = df.select(df.SkillWillLearn.alias('text'))\n",
        "df = df.withColumn('text', lower(col('text')))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "_HZCiXsIbXSV",
        "outputId": "282cc934-f534-47d7-8c8a-1fc3f23d91aa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1abf559df31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSkillWillLearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'SkillWillLearn'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipeline.fit(df).transform(df)\n",
        "\n",
        "# result is a DataFrame\n",
        "result.select(\"text\", \"ner\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYxZWziybbz5",
        "outputId": "e9ebfe70-f069-4ee1-d43f-2ab3a3e80ced"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|                 ner|\n",
            "+--------------------+--------------------+\n",
            "|machine learning ...|[{named_entity, 0...|\n",
            "|in an age now dri...|[{named_entity, 0...|\n",
            "|in this course, y...|[{named_entity, 0...|\n",
            "|explain the princ...|[{named_entity, 0...|\n",
            "|in the first cour...|[{named_entity, 0...|\n",
            "|define data scien...|[{named_entity, 0...|\n",
            "|this mooc ‚Äì a joi...|[{named_entity, 0...|\n",
            "|describe the use ...|[{named_entity, 0...|\n",
            "|while telling sto...|[{named_entity, 0...|\n",
            "|in this course, y...|[{named_entity, 0...|\n",
            "|properly identify...|[{named_entity, 0...|\n",
            "|development of an...|[{named_entity, 0...|\n",
            "|describe differen...|[{named_entity, 0...|\n",
            "|distinguish betwe...|[{named_entity, 0...|\n",
            "|apply tidyverse f...|[{named_entity, 0...|\n",
            "|describe differen...|[{named_entity, 0...|\n",
            "|articulate differ...|[{named_entity, 0...|\n",
            "|build an investme...|[{named_entity, 0...|\n",
            "|in this course yo...|[{named_entity, 0...|\n",
            "|the capstone proj...|[{named_entity, 0...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.select(\"text\", \"ner.annotatorType\", \"ner.begin\", \"ner.end\", \"ner.result\", \"ner.metadata\", \"ner.embeddings\")"
      ],
      "metadata": {
        "id": "KL2uGupebgW9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "result = result.withColumn(\"text\", col(\"text\").cast(\"string\")) \\\n",
        ".withColumn(\"annotatorType\", col(\"annotatorType\").cast(\"string\")) \\\n",
        ".withColumn(\"begin\", col(\"begin\").cast(\"string\")) \\\n",
        ".withColumn(\"end\", col(\"end\").cast(\"string\")) \\\n",
        ".withColumn(\"result\", col(\"result\").cast(\"string\")) \\\n",
        ".withColumn(\"metadata\", col(\"metadata\").cast(\"string\")) \\\n",
        ".withColumn(\"embeddings\", col(\"embeddings\").cast(\"string\"))"
      ],
      "metadata": {
        "id": "hninazD9blgz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('text','ner_span.result','ner_span').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWJRmjUgbnH-",
        "outputId": "3f323fb2-64be-49a8-d636-66bbdf64747b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|                text|              result|            ner_span|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|software testing ...|[software testing...|[{chunk, 0, 15, s...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.withColumn(\"ner\", col(\"ner\").cast(\"string\")) \\\n",
        ".withColumn(\"document\", col(\"document\").cast(\"string\")) \\\n",
        ".withColumn(\"token\", col(\"token\").cast(\"string\"))"
      ],
      "metadata": {
        "id": "h_fXMF3gqzQT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.write.options(header='True', delimiter=',', quotes = '\"') \\\n",
        " .csv(\"/content/drive/MyDrive/Data_Science/thesis/result\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tejyJqRgboGz",
        "outputId": "620f9821-7edc-49f0-ba54-8c44f0acba5f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-99a5108004bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Data_Science/thesis/result\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o782.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error accessing configuration file\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:582)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1173)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.nextProviderClass(ServiceLoader.java:1206)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1221)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1265)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1300)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1385)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\n\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\n\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\n\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)\n\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:652)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:852)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:839)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.nio.file.NoSuchFileException: /usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-sql_2.12-3.2.3.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1764)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1259)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:831)\n\tat java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:857)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:846)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:248)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:177)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:350)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1165)\n\t... 36 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('text','token.annotatorType','token.begin','token.end','token.result','token.metadata', 'token.embeddings').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvBzB_CqsGyC",
        "outputId": "e647c919-454a-48f0-94a2-6a3b1afd8e2c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|       annotatorType|               begin|                 end|              result|            metadata|          embeddings|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|machine learning ...|[token, token, to...|[0, 8, 17, 20, 24...|[6, 15, 18, 22, 3...|[machine, learnin...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|in an age now dri...|[token, token, to...|[0, 3, 6, 10, 14,...|[1, 4, 8, 12, 19,...|[in, an, age, now...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|in this course, y...|[token, token, to...|[0, 3, 8, 14, 16,...|[1, 6, 13, 14, 18...|[in, this, course...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|explain the princ...|[token, token, to...|[0, 8, 12, 23, 26...|[6, 10, 21, 24, 2...|[explain, the, pr...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|in the first cour...|[token, token, to...|[0, 3, 7, 13, 20,...|[1, 5, 11, 18, 21...|[in, the, first, ...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|define data scien...|[token, token, to...|[0, 7, 12, 20, 24...|[5, 10, 18, 22, 2...|[define, data, sc...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|this mooc ‚Äì a joi...|[token, token, to...|[0, 5, 10, 12, 14...|[3, 8, 10, 12, 18...|[this, mooc, ‚Äì, a...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|describe the use ...|[token, token, to...|[0, 9, 13, 17, 20...|[7, 11, 15, 18, 2...|[describe, the, u...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|while telling sto...|[token, token, to...|[0, 6, 14, 22, 27...|[4, 12, 20, 25, 3...|[while, telling, ...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|in this course, y...|[token, token, to...|[0, 3, 8, 14, 16,...|[1, 6, 13, 14, 18...|[in, this, course...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|properly identify...|[token, token, to...|[0, 9, 18, 26, 31...|[7, 16, 24, 29, 3...|[properly, identi...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|development of an...|[token, token, to...|[0, 12, 15, 18, 2...|[10, 13, 16, 25, ...|[development, of,...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|describe differen...|[token, token, to...|[0, 9, 19, 24, 31...|[7, 17, 22, 30, 3...|[describe, differ...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|distinguish betwe...|[token, token, to...|[0, 12, 20, 28, 3...|[10, 18, 26, 32, ...|[distinguish, bet...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|apply tidyverse f...|[token, token, to...|[0, 6, 16, 26, 29...|[4, 14, 24, 27, 3...|[apply, tidyverse...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|describe differen...|[token, token, to...|[0, 9, 19, 25, 28...|[7, 17, 23, 26, 3...|[describe, differ...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|articulate differ...|[token, token, to...|[0, 11, 21, 27, 3...|[9, 19, 25, 28, 3...|[articulate, diff...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|build an investme...|[token, token, to...|[0, 6, 9, 20, 27,...|[4, 7, 18, 25, 31...|[build, an, inves...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|in this course yo...|[token, token, to...|[0, 3, 8, 15, 22,...|[1, 6, 13, 20, 26...|[in, this, course...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "|the capstone proj...|[token, token, to...|[0, 4, 13, 21, 28...|[2, 11, 19, 26, 3...|[the, capstone, p...|[{sentence -> 0},...|[[], [], [], [], ...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('token.result','ner.result').show(truncate=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROSkNKDwRFCH",
        "outputId": "988d9719-44ce-43fb-8309-f9cb077559d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                              result|                                                                                              result|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|[machine, learning, is, the, science, of, getting, computers, to, act, without, being, explicitly...|[B-KNOW, I-KNOW, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-KNOW, I-KNOW, O, O, O, O...|\n",
            "|[in, an, age, now, driven, by, \", big, data, \",, we, need, to, cut, through, the, noise, and, pre...|[O, O, O, O, O, O, O, B-KNOW, I-KNOW, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O,...|\n",
            "|[in, this, course, ,, you, will, learn, how, to, analyze, map, data, using, different, data, type...|[O, O, O, O, O, O, O, O, O, B-KNOW, I-KNOW, I-KNOW, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...|\n",
            "|[explain, the, principles, of, data, structures, &amp, ;, how, they, are, used, ., create, progra...|[O, O, O, O, B-KNOW, I-KNOW, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O,...|\n",
            "|[in, the, first, course, of, the, deep, learning, specialization, ,, you, will, study, the, found...|[O, O, O, O, O, O, B-KNOW, I-KNOW, O, O, O, O, O, O, O, O, O, B-KNOW, I-KNOW, O, B-KNOW, I-KNOW, ...|\n",
            "|[define, data, science, and, its, importance, in, today‚Äôs, data-driven, world, ., describe, the, ...|[O, B-KNOW, I-KNOW, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-KNOW, I-KNOW, O, O...|\n",
            "|[this, mooc, ‚Äì, a, joint, initiative, between, eit, digital, ,, universit√©, de, nice, sophia-anti...|[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-KNOW, O, O, O, O, O, O...|\n",
            "|[describe, the, use, of, data, visualizations, to, talk, about, data, and, the, results, of, data...|[O, O, O, O, B-KNOW, I-KNOW, O, O, O, O, O, O, O, O, B-KNOW, I-KNOW, O, O, B-TOOL, O, O, B-KNOW, ...|\n",
            "|[while, telling, stories, with, data, has, been, part, of, the, news, practice, since, its, earli...|[O, B-KNOW, I-KNOW, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O,...|\n",
            "|[in, this, course, ,, you, learn, about, the, data, structure, needed, for, geographic, mapping, ...|[O, O, O, O, O, O, O, O, B-KNOW, I-KNOW, O, O, O, O, O, O, O, O, O, O, B-TOOL, I-TOOL, I-TOOL, O,...|\n",
            "|[properly, identify, various, data, types, and, understand, the, different, uses, for, each, ., c...|[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-KNOW, I-KNOW, O, B-KNOW, I-KNOW, O, B-LANG, O, O, O,...|\n",
            "|[development, of, an, analytic, mindset, for, approaching, business, problems, ., the, ability, t...|[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-KNOW, O, B-KNOW, I-KNOW, ...|\n",
            "|[describe, different, data, formats, ., apply, tidyverse, functions, to, import, data, into, r, f...|           [O, O, O, O, O, O, B-FRAM, O, O, O, O, O, B-LANG, O, O, O, O, O, O, O, O, B-KNOW, I-KNOW]|\n",
            "|[distinguish, between, various, types, of, plots, and, their, uses, ., use, the, ggplot2, r, pack...|[O, O, O, O, O, O, O, O, O, O, O, O, B-FRAM, I-FRAM, O, O, O, B-KNOW, I-KNOW, O, O, O, O, O, O, O...|\n",
            "|[apply, tidyverse, functions, to, transform, non-tidy, data, to, tidy, data, ., conduct, basic, e...|                   [O, B-FRAM, O, O, O, O, O, O, O, O, O, O, O, O, B-KNOW, I-KNOW, O, O, O, O, O, O]|\n",
            "|[describe, different, types, of, data, analytic, questions, ., conduct, hypothesis, tests, of, yo...|[O, O, O, O, B-KNOW, I-KNOW, O, O, O, B-KNOW, I-KNOW, O, O, O, O, O, B-KNOW, I-KNOW, O, O, O, O, ...|\n",
            "|[articulate, different, forms, of, clinical, and, population, level, data, ., describe, the, data...|[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-KNOW, I-KNOW, O, B-KNOW, ...|\n",
            "|[build, an, investment, factor, model, using, regression, methodology, ., employ, optimization, a...|                                [O, O, O, O, O, O, O, O, O, O, O, O, O, B-LANG, O, O, O, O, O, O, O]|\n",
            "|[in, this, course, you‚Äôll, focus, on, how, constant, data, collection, and, big, data, analysis, ...|[O, O, O, O, O, O, O, O, O, O, O, B-KNOW, I-KNOW, I-KNOW, O, O, O, O, O, O, O, O, O, O, O, O, O, ...|\n",
            "|[the, capstone, project, offers, qualified, learners, to, the, opportunity, to, apply, their, kno...|[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "result_df = result.select(F.explode(F.arrays_zip(result.token.result,\n",
        "                                                 result.ner.result, \n",
        "                                                 result.ner.metadata)).alias(\"cols\"))\\\n",
        "                  .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "                          F.expr(\"cols['1']\").alias(\"ner_label\"),\n",
        "                          F.expr(\"cols['2']['confidence']\").alias(\"confidence\"))\n",
        "\n",
        "result_df.show(50, truncate=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_Pvi94VRW3T",
        "outputId": "2e01a7ba-b8dd-4b0a-8335-affb424cc716"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------+----------+\n",
            "|        token|ner_label|confidence|\n",
            "+-------------+---------+----------+\n",
            "|      machine|   B-KNOW|      null|\n",
            "|     learning|   I-KNOW|      null|\n",
            "|           is|        O|      null|\n",
            "|          the|        O|      null|\n",
            "|      science|        O|      null|\n",
            "|           of|        O|      null|\n",
            "|      getting|        O|      null|\n",
            "|    computers|        O|      null|\n",
            "|           to|        O|      null|\n",
            "|          act|        O|      null|\n",
            "|      without|        O|      null|\n",
            "|        being|        O|      null|\n",
            "|   explicitly|        O|      null|\n",
            "|   programmed|        O|      null|\n",
            "|            .|        O|      null|\n",
            "|           in|        O|      null|\n",
            "|          the|        O|      null|\n",
            "|         past|        O|      null|\n",
            "|       decade|        O|      null|\n",
            "|            ,|        O|      null|\n",
            "|      machine|   B-KNOW|      null|\n",
            "|     learning|   I-KNOW|      null|\n",
            "|          has|        O|      null|\n",
            "|        given|        O|      null|\n",
            "|           us|        O|      null|\n",
            "| self-driving|        O|      null|\n",
            "|         cars|        O|      null|\n",
            "|            ,|        O|      null|\n",
            "|    practical|        O|      null|\n",
            "|       speech|        O|      null|\n",
            "|  recognition|        O|      null|\n",
            "|            ,|        O|      null|\n",
            "|    effective|        O|      null|\n",
            "|          web|        O|      null|\n",
            "|       search|        O|      null|\n",
            "|            ,|        O|      null|\n",
            "|          and|        O|      null|\n",
            "|            a|        O|      null|\n",
            "|       vastly|        O|      null|\n",
            "|     improved|        O|      null|\n",
            "|understanding|        O|      null|\n",
            "|           of|        O|      null|\n",
            "|          the|        O|      null|\n",
            "|        human|        O|      null|\n",
            "|       genome|        O|      null|\n",
            "|            .|        O|      null|\n",
            "|      machine|   B-KNOW|      null|\n",
            "|     learning|   I-KNOW|      null|\n",
            "|           is|        O|      null|\n",
            "|           so|        O|      null|\n",
            "+-------------+---------+----------+\n",
            "only showing top 50 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('token.result', 'ner.result').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4j7Uq-kR8qY",
        "outputId": "a3896642-a757-4304-db2d-8aa0bf451549"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|              result|              result|\n",
            "+--------------------+--------------------+\n",
            "|[machine, learnin...|[B-KNOW, I-KNOW, ...|\n",
            "|[in, an, age, now...|[O, O, O, O, O, O...|\n",
            "|[in, this, course...|[O, O, O, O, O, O...|\n",
            "|[explain, the, pr...|[O, O, O, O, B-KN...|\n",
            "|[in, the, first, ...|[O, O, O, O, O, O...|\n",
            "|[define, data, sc...|[O, B-KNOW, I-KNO...|\n",
            "|[this, mooc, ‚Äì, a...|[O, O, O, O, O, O...|\n",
            "|[describe, the, u...|[O, O, O, O, B-KN...|\n",
            "|[while, telling, ...|[O, B-KNOW, I-KNO...|\n",
            "|[in, this, course...|[O, O, O, O, O, O...|\n",
            "|[properly, identi...|[O, O, O, O, O, O...|\n",
            "|[development, of,...|[O, O, O, O, O, O...|\n",
            "|[describe, differ...|[O, O, O, O, O, O...|\n",
            "|[distinguish, bet...|[O, O, O, O, O, O...|\n",
            "|[apply, tidyverse...|[O, B-FRAM, O, O,...|\n",
            "|[describe, differ...|[O, O, O, O, B-KN...|\n",
            "|[articulate, diff...|[O, O, O, O, O, O...|\n",
            "|[build, an, inves...|[O, O, O, O, O, O...|\n",
            "|[in, this, course...|[O, O, O, O, O, O...|\n",
            "|[the, capstone, p...|[O, O, O, O, O, O...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "last = result.select(F.explode(F.arrays_zip(\"ner_span.result\",\"ner_span.metadata\")).alias(\"entities\")) \\\n",
        ".select(F.expr(\"entities['0']\").alias(\"chunk\"), \n",
        "        F.expr(\"entities['1'].entity\").alias(\"entity\")). \\\n",
        "        show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "MzSjADwPd8jm",
        "outputId": "ee80bca3-51e1-4136-9455-c4d9b4e6ecfc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-3ebcd30ba788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner_span.result\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ner_span.metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m .select(F.expr(\"entities['0']\").alias(\"chunk\"), \n\u001b[1;32m      5\u001b[0m         F.expr(\"entities['1'].entity\").alias(\"entity\")). \\\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: No such struct field 0 in result, metadata"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last.select(F.expr(\"entities['0']\").alias(\"chunk\"), \n",
        "            F.expr(\"entities['1'].entity\").alias(\"entity\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "D6xSUTDziKyu",
        "outputId": "f3d2ce48-6ebf-4422-bd76-8adbb389faeb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-0aa798ae2c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entities['0']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entities['1'].entity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last.select(F.explode(F.expr(\"entities\").alias(\"chunk\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "tlQ2DCUti4wW",
        "outputId": "0610243c-c700-459e-ad38-ecdc7cc3b0d5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-900daa12dfbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "transformers",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "59794f394f79a45d9851d6706177d59b9a5e9d735b0369dbae4b76bccf016251"
      }
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}