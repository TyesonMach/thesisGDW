{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/worker2/.ivy2/cache\n",
      "The jars for the packages stored in: /home/worker2/.ivy2/jars\n",
      "com.microsoft.sqlserver#mssql-jdbc added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e9c739f5-fde0-4042-9757-a5076754ee2d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.sqlserver#mssql-jdbc;9.4.0.jre8 in central\n",
      ":: resolution report :: resolve 374ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.microsoft.sqlserver#mssql-jdbc;9.4.0.jre8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e9c739f5-fde0-4042-9757-a5076754ee2d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/10ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"12g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.sqlserver:mssql-jdbc:9.4.0.jre8\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTOR_TYPE='com.microsoft.sqlserver.jdbc.SQLServerDriver'\n",
    "SQL_USERNAME='tyeson'\n",
    "SQL_PASSWORD='Canhtoan1509'\n",
    "SQL_SERVERNAME='tyeson.database.windows.net'\n",
    "SQL_DBNAME='DW'\n",
    "CONNECTION_STRING='jdbc:sqlserver://tyeson.database.windows.net:1433;databaseName=DW;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header','true').csv('/home/worker2/Desktop/thesis/K18/File_K18/sample_crawl_dataset/Coursera_DataScience.csv',inferSchema=True, escape = '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df = df.withColumn(\"enroll\", f.regexp_replace(f.col(\"enroll\"), \",\", \"\").alias(\"enroll\")) \\\n",
    "    .withColumn(\"time\", f.regexp_replace(f.col(\"time\"), \" hours\", \"\").alias(\"time\")) \\\n",
    "    .withColumn(\"time\", f.regexp_replace(f.col(\"time\"), \" hour\", \"\").alias(\"time\")) \\\n",
    "    .withColumn(\"fee\", f.regexp_replace(f.col(\"fee\"), \"[a-zA-Z]+\", \"\").alias(\"fee\"))\n",
    "    #.withColumn(\"fee\", f.regexp_replace(f.col(\"fee\"), \"Enroll\", \"\").alias(\"fee\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, LongType, IntegerType\n",
    "\n",
    "df = df \\\n",
    "  .withColumn(\"rating\" ,\n",
    "              df[\"rating\"]\n",
    "              .cast(FloatType()))   \\\n",
    "  .withColumn(\"enroll\",\n",
    "              df[\"enroll\"]\n",
    "              .cast(LongType())) \\\n",
    "  .withColumn(\"time\",df['time'].cast(IntegerType())) \\\n",
    "  .withColumn(\"fee\", df['fee'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,when\n",
    "df = df.na.fill(value=0, subset=['rating','enroll', 'fee','time'])\n",
    "df = df.na.fill(value='None', subset=['link','instructor', 'levelrequirement', 'skillrequirement', 'SkillWillLearn','SkillGain', 'Subject', 'organization', 'program', 'RelationInsOrg', 'Subtitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,lower\n",
    "df = df.withColumn('levelrequirement', lower('levelrequirement'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"levelrequirement\", when((df.levelrequirement != \"expert\") & (df.levelrequirement != \"advanced\") & (df.levelrequirement != 'intermediate') & (df.levelrequirement != 'beginner'),\"beginner\") \\\n",
    "                                        .when(df.levelrequirement == \"advanced\",\"expert\")\n",
    "                                        .otherwise(df.levelrequirement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col, monotonically_increasing_id\n",
    "\n",
    "df2 = df.select(col('name'),col('link'),col('rating'),col('enroll'),col('instructor'),col('time'),col('levelrequirement'),col('skillrequirement'),\n",
    "                col('SkillWillLearn'),col('SkillGain'),col('Subject'),col('organization'),col('fee'),col('program'),col('RelationInsOrg'),split(col(\"Subtitle\"),\", \").alias(\"Subtitle\"))\n",
    "#df2 = df2.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"instructor\", f.regexp_replace(f.col(\"instructor\"), \", PhD\", \"\").alias(\"instructor\")) \\\n",
    "    .withColumn(\"instructor\", f.regexp_replace(f.col(\"instructor\"), \", Ph.D.\", \"\").alias(\"instructor\")) \\\n",
    "    .withColumn(\"instructor\", f.regexp_replace(f.col(\"instructor\"), chr(34), \"\").alias(\"instructor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex2 = r\"^https?://.*\"\n",
    "df2 = df2.filter(col(\"link\").rlike(regex2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor = df2.select(split(col(\"instructor\"),\",\").alias(\"instructor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "instructorNode = instructor.select(explode(\"instructor\").alias(\"instructor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructorNode = instructorNode.dropDuplicates(['instructor']).select(col('instructor').alias('instructorName'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col, monotonically_increasing_id\n",
    "instructorNode = instructorNode.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructorNode = instructorNode.select(col('id').alias('instructorId'),'instructorName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "instructorNode.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','dim_instructor')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = df2.select(col('levelrequirement').alias('levelName'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = level.dropDuplicates(['levelName']).select('levelName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = level.withColumn(\"levelId\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "level.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','dim_level')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle = df2.select(explode(col(\"Subtitle\")).alias(\"language\"))\n",
    "subtitle = subtitle.withColumn('language', lower(col('language'))) \\\n",
    "            .withColumn(\"language\", f.regexp_replace(f.col(\"language\"), \"subtitles: \", \"\").alias(\"language\")) \\\n",
    "            .withColumn(\"language\", f.regexp_replace(f.col(\"language\"), \"subtitles:\", \"\").alias(\"language\")) \\\n",
    "            .withColumn(\"language\", f.regexp_replace(f.col(\"language\"), \"none\", \"english\").alias(\"language\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitleNode = subtitle.dropDuplicates(['language']).select(col('language').alias('languageName'))\n",
    "subtitleNode = subtitleNode.withColumn(\"languageId\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "subtitleNode.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','dim_language')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "regex = r\"(?:https?:\\/\\/)?(?:[^@\\n]+@)?(?:www\\.)?([^:\\/\\n?]+)\"\n",
    "regex2 = r\"^https?://.*\"\n",
    "website = df2.select(df2.link)\n",
    "website = website.filter(col(\"link\").rlike(regex2))\n",
    "website = website.withColumn(\"website\", regexp_extract(df[\"link\"], regex, 1))\n",
    "website= website.dropDuplicates(['website']).select(col('website').alias('websiteName'))\n",
    "df2 = df2.withColumn(\"website\", regexp_extract(df[\"link\"], regex, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = website.withColumn(\"websiteId\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "website.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','dim_website')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"timestamp\", f.current_timestamp()).alias(\"timestamp\")\n",
    "course = df2.select(col('name').alias('courseName'), col('enroll') ,col('rating'), col('fee'),col('time'), col('link'), col('levelrequirement').alias('level'), col('website'),col('timestamp').alias('createDate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_joined_df = course.join(level, col(\"level\") == col(\"levelName\"), \"left\").select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_joined_df = level_joined_df.drop('levelName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_joined_df = level_joined_df.join(website, col(\"website\") == col(\"websiteName\"), \"left\").select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "course = website_joined_df.withColumn('expiredDate', lit('2099-01-01')) \\\n",
    "                .withColumn('rowStatus', lit(1)) \\\n",
    "                .withColumn('courseKey', monotonically_increasing_id()) \\\n",
    "                .withColumn('courseId',  monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "course = course.drop('level', 'websiteName', 'website')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "course = course.select(col('courseKey'), col('courseId'), col('courseName'), col('enroll') ,col('rating'), col('fee'),col('time'), col('link'), col('levelId'), col('websiteId'), col('createDate'), col('expiredDate'), col('rowStatus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "course.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','dim_course')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor = instructor.withColumn('courseKey', monotonically_increasing_id())\n",
    "instructor = instructor.select(col('courseKey'), explode(\"instructor\").alias(\"instructor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor_joined_df = instructor.join(instructorNode, col(\"instructor\") == col(\"instructorName\"), \"left\").select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor_joined_df = instructor_joined_df.select('courseKey', 'instructorId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "instructor_joined_df.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','bridge_instructor_course')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn('id', monotonically_increasing_id())\n",
    "subtitle = df3.select(col('id').alias('courseKey'), explode(col(\"Subtitle\")).alias(\"language\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle = subtitle.select(col('courseKey'), lower(col('language')).alias('language'))\n",
    "subtitle = subtitle.withColumn('language', lower(col('language'))) \\\n",
    "            .withColumn(\"language\", f.regexp_replace(f.col(\"language\"), \"subtitles: \", \"\").alias(\"language\")) \\\n",
    "            .withColumn(\"language\", f.regexp_replace(f.col(\"language\"), \"subtitles:\", \"\").alias(\"language\")) \\\n",
    "            .withColumn(\"language\", f.regexp_replace(f.col(\"language\"), \"none\", \"english\").alias(\"language\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_joined_df = subtitle.join(subtitleNode, col(\"language\") == col(\"languageName\"), \"left\").select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_joined_df = subtitle_joined_df.select('languageId', 'courseKey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "subtitle_joined_df.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','bridge_language_course')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "#from transformers import BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 20:11:32.961746: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: /home/worker2/Desktop/thesis/NER/NERModel_config/converting/saved_model/1\n",
      "2023-02-26 20:11:33.028150: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }\n",
      "2023-02-26 20:11:33.028226: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: /home/worker2/Desktop/thesis/NER/NERModel_config/converting/saved_model/1\n",
      "2023-02-26 20:11:33.028417: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-26 20:11:33.262610: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2023-02-26 20:11:33.290132: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2096000000 Hz\n",
      "2023-02-26 20:11:39.473821: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /home/worker2/Desktop/thesis/NER/NERModel_config/converting/saved_model/1\n",
      "2023-02-26 20:11:39.691859: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 6730194 microseconds.\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "MODEL_NAME = '/home/worker2/Desktop/thesis/NER/NERModel_config'\n",
    "bert = BertForTokenClassification.loadSavedModel(\n",
    "     '{}/converting/saved_model/1'.format(MODEL_NAME),\n",
    "     spark\n",
    " )\\\n",
    " .setInputCols([\"document\",'token'])\\\n",
    " .setOutputCol(\"ner\")\\\n",
    " .setCaseSensitive(True)\\\n",
    " .setMaxSentenceLength(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bert.write().overwrite().save(\"./{}\".format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenClassifier_loaded = BertForTokenClassification.load(\"./{}\".format(MODEL_NAME))\\\n",
    "  .setInputCols([\"document\",'token'])\\\n",
    "  .setOutputCol(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenClassifier_loaded.getClasses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('description') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "converter = NerConverter()\\\n",
    "    .setInputCols([\"document\",\"token\",\"ner\"])\\\n",
    "    .setOutputCol(\"ner_span\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    tokenizer,\n",
    "    tokenClassifier_loaded,\n",
    "    converter\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "def ftfy_udf(text):\n",
    "    return ftfy.fix_text(text)\n",
    "\n",
    "fix_text = udf(ftfy_udf, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "from pyspark.sql.functions import trim,ltrim,rtrim\n",
    "import pyspark.sql.functions as f\n",
    "df4 = df3.select(df.SkillWillLearn.alias('description'))\n",
    "df4 = df4.withColumn('description', lower(col('description'))) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \"[\\n\\r\\b\\f\\t]\", \"\")) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \"®\", \"\")) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \"'s\", \"\")) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \";\", \" \")) \\\n",
    "        .withColumn('description', fix_text(col('description'))) \\\n",
    "        .withColumn('description', f.decode('description','UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit(df4).transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col, monotonically_increasing_id\n",
    "result = result.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "result_last = result.select(F.explode(F.arrays_zip(\"ner_span.result\",\"ner_span.metadata\")).alias(\"entities\"), 'id') \\\n",
    ".select(F.expr(\"entities['result']\").alias(\"name\"), \n",
    "        F.expr(\"entities['metadata'].entity\").alias(\"entity\"), 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_last = result_last.orderBy(\"id\").where(result_last.entity != 'EP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "competency = result_last.select(col('name').alias('competencyName'), col('entity').alias('type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "competency = competency.dropDuplicates(['competencyName', 'type']).select('competencyName','type')\n",
    "competency = competency.withColumn('competencyId',monotonically_increasing_id())\n",
    "competency = competency.select('competencyId', 'competencyName', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "competency.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','dim_competency')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_course = result_last.dropDuplicates(['name','entity','id']).select(col('id').alias('courseKey'), col('name').alias('competency'), col('entity')).orderBy('courseKey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_course = fact_course.join(competency, (col('competency') == col('competencyName')) & (col('entity') == col('type')), 'left').select('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_course = fact_course.select('courseKey', 'competencyId')\n",
    "fact_course = fact_course.withColumn('is_require', lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fact_course.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','fact_course')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('description') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "converter = NerConverter()\\\n",
    "    .setInputCols([\"document\",\"token\",\"ner\"])\\\n",
    "    .setOutputCol(\"ner_span\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    tokenizer,\n",
    "    tokenClassifier_loaded,\n",
    "    converter\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.select(df3.skillrequirement.alias('description'))\n",
    "df4 = df4.withColumn('description', lower(col('description'))) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \"[\\n\\r\\b\\f\\t]\", \"\")) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \"®\", \"\")) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \"'s\", \"\")) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \";\", \" \")) \\\n",
    "        .withColumn('description', fix_text(col('description'))) \\\n",
    "        .withColumn('description', f.decode('description','UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "require_result = pipeline.fit(df4).transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "require_result = require_result.withColumn(\"id\", monotonically_increasing_id())\n",
    "require_last = require_result.select(F.explode(F.arrays_zip(\"ner_span.result\",\"ner_span.metadata\")).alias(\"entities\"), 'id') \\\n",
    ".select(F.expr(\"entities['result']\").alias(\"name\"), \n",
    "        F.expr(\"entities['metadata'].entity\").alias(\"entity\"), 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "require_last = require_last.orderBy(\"id\").where(require_last.entity != 'EP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "competency_require = require_last.select(col('name').alias('competencyName'), col('entity').alias('type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "competency_require = competency_require.dropDuplicates(['competencyName', 'type']).select('competencyName','type')\n",
    "competency_require = competency_require.withColumn('competencyId',monotonically_increasing_id())\n",
    "competency_require = competency_require.select('competencyId', 'competencyName', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "max_key = competency.select(max('competencyId').alias('maxkey')).collect()[0][0]\n",
    "max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_skill_learn = competency.select('competencyName', 'type')\n",
    "set_skill_require = competency_require.select('competencyName', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = set_skill_require.exceptAll(set_skill_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.withColumn('competencyId', max_key + 1 + monotonically_increasing_id())\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.select('competencyId','competencyName', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','dim_competency')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_course_require = require_last.dropDuplicates(['name','entity','id']).select(col('id').alias('courseKey'), col('name').alias('competency'), col('entity')).orderBy('courseKey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_course_require = fact_course_require.join(competency_require, (col('competency') == col('competencyName')) & (col('entity') == col('type')), 'left').select('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_course_require = fact_course_require.select('courseKey', 'competencyId')\n",
    "fact_course_require = fact_course_require.withColumn('is_require', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fact_course_require.write.format('jdbc')\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", CONNECTION_STRING)\\\n",
    "    .option('driver',CONNECTOR_TYPE) \\\n",
    "    .option('dbtable','fact_course')\\\n",
    "    .option(\"user\", SQL_USERNAME)\\\n",
    "    .option('password', SQL_PASSWORD)\\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
