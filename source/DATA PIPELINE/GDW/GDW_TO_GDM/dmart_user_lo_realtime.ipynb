{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "surl = 'neo4j://localhost:7692'\n",
    "spassword = 'password'\n",
    "turl='neo4j+s://061a1cbc.databases.neo4j.io'\n",
    "tpassword = 'KuK8K1aTC6_LkCeyBHVvq_JZxA4EC7ooqeNk8LYH8o8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint1 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint1'\n",
    "checkpoint2 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint2'\n",
    "checkpoint3 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint3'\n",
    "checkpoint4 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint4'\n",
    "checkpoint5 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint5'\n",
    "checkpoint6 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint6'\n",
    "checkpoint7 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint7'\n",
    "checkpoint8 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint8'\n",
    "checkpoint9 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint9'\n",
    "checkpoint10 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint10'\n",
    "checkpoint11 = '/home/worker2/Desktop/thesis/dmart_checkpoint/checkpoint11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", surl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", spassword)\\\n",
    "  .option(\"streaming.from\", \"ALL\") \\\n",
    "  .option(\"streaming.property.name\", \"id\") \\\n",
    "  .option(\"streaming.query.offset\", \"MATCH (p:user) RETURN p\") \\\n",
    "  .option(\"query\", \"match (c:User)-[HAVE_PL]->(pl:ProgrammingLanguage) return c.id as id, c.username as username, c.email as email, pl.name as name\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = df.dropDuplicates([\"id\",\"username\",'email']).select(df.id,df.username,df.email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query1 = user.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "    .option(\"checkpointLocation\", checkpoint1) \\\n",
    "    .option(\"node.keys\", \"id\")\\\n",
    "    .option(\"labels\",\":User\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "pl = df.dropDuplicates([\"name\"]).select(df.name)\n",
    "query2 = pl.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "      .option(\"checkpointLocation\", checkpoint2) \\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":ProgrammingLanguage\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "user_pl = df.select(df.id, df.name)\n",
    "query3 = user_pl.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "    .option(\"checkpointLocation\", checkpoint3) \\\n",
    "    .option(\"relationship\", 'HAVE_PL')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':User')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':ProgrammingLanguage')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.readStream.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", surl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", spassword)\\\n",
    "  .option(\"streaming.from\", \"ALL\") \\\n",
    "  .option(\"streaming.property.name\", \"id\") \\\n",
    "  .option(\"streaming.query.offset\", \"MATCH (p:user) RETURN p\") \\\n",
    "  .option(\"query\", \"match (c:User)-[HAVE_SKILL]->(t:Tool) return c.id as id, t.name as name\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "tool = df2.dropDuplicates([\"name\"]).select(df2.name)\n",
    "query4 = tool.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "    .option(\"checkpointLocation\", checkpoint4) \\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":Tool\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "user_tool = df2.select(df2.id, df2.name)\n",
    "query5 = user_tool.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "    .option(\"checkpointLocation\", checkpoint5) \\\n",
    "    .option(\"relationship\", 'HAVE_TOOL')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':User')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Tool')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.readStream.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", surl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", spassword)\\\n",
    "  .option(\"streaming.from\", \"ALL\") \\\n",
    "  .option(\"streaming.property.name\", \"id\") \\\n",
    "  .option(\"streaming.query.offset\", \"MATCH (p:user) RETURN p\") \\\n",
    "  .option(\"query\", \"match (c:User)-[HAVE_SKILL]->(t:Framework) return c.id as id, t.name as name\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "fram = df3.dropDuplicates([\"name\"]).select(df3.name)\n",
    "query6 = fram.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "    .option(\"checkpointLocation\", checkpoint6) \\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":Framework\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "user_fram = df3.select(df3.id, df3.name)\n",
    "query7 = user_fram.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "  .option(\"checkpointLocation\", checkpoint7) \\\n",
    "    .option(\"relationship\", 'HAVE_FRAM')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':User')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Framework')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.readStream.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", surl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", spassword)\\\n",
    "  .option(\"streaming.from\", \"ALL\") \\\n",
    "  .option(\"streaming.property.name\", \"id\") \\\n",
    "  .option(\"streaming.query.offset\", \"MATCH (p:user) RETURN p\") \\\n",
    "  .option(\"query\", \"match (c:User)-[HAVE_SKILL]->(t:Platform) return c.id as id, t.name as name\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "plat = df4.dropDuplicates([\"name\"]).select(df4.name)\n",
    "query8 = plat.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "    .option(\"checkpointLocation\", checkpoint8) \\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":Platform\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "user_plat = df4.select(df4.id, df4.name)\n",
    "query9 = user_plat.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "  .option(\"checkpointLocation\", checkpoint9) \\\n",
    "    .option(\"relationship\", 'HAVE_PLAT')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':User')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Platform')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = spark.readStream.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", surl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", spassword)\\\n",
    "  .option(\"streaming.from\", \"ALL\") \\\n",
    "  .option(\"streaming.property.name\", \"id\") \\\n",
    "  .option(\"streaming.query.offset\", \"MATCH (p:user) RETURN p\") \\\n",
    "  .option(\"query\", \"match (c:User)-[HAVE_KNOW]->(t:Knowledge) return c.id as id, t.name as name\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "know = df5.dropDuplicates([\"name\"]).select(df5.name)\n",
    "query10 = know.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "    .option(\"checkpointLocation\", checkpoint10) \\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":Knowledge\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "user_know = df5.select(df5.id, df5.name)\n",
    "query11 = user_know.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "  .option(\"url\", turl)\\\n",
    "  .option(\"authentication.type\", \"basic\")\\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "  .option(\"authentication.basic.password\", tpassword)\\\n",
    "    .option(\"checkpointLocation\", checkpoint11) \\\n",
    "    .option(\"relationship\", 'HAVE_KNOW')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':User')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Knowledge')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>(55 + 6) / 200][Stage 27:> (0 + 0) / 200][Stage 29:> (0 + 0) / 200]  \r"
     ]
    }
   ],
   "source": [
    "query1.awaitTermination()\n",
    "query2.awaitTermination()\n",
    "query3.awaitTermination()\n",
    "query4.awaitTermination()\n",
    "query5.awaitTermination()\n",
    "query6.awaitTermination()\n",
    "query7.awaitTermination()\n",
    "query8.awaitTermination()\n",
    "query9.awaitTermination()\n",
    "query10.awaitTermination()\n",
    "query11.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
