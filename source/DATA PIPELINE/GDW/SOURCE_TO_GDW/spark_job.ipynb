{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QJrYiDDzUJ3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"12g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "url='neo4j://localhost:7692'\n",
    "password = 'password'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CfbE7I_vQAXJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header','true').csv('/home/worker2/Desktop/thesis/K18/File_K18/sample_crawl_dataset/Dice_12JobTitle.csv',inferSchema=True, escape = '\"')\n",
    "#df = spark.read.option('header','true').csv('/content/drive/MyDrive/Data Science/thesis/K18/File K18/sample_crawl_dataset/Coursera_DataScience.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import trim,ltrim,rtrim\n",
    "from pyspark.sql.functions import split, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import lower, col\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id()).withColumn('name', lower(col('name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"senior\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"senior', \", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"sr.\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \", senior\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"sr. \", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"jr.\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"sr\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"mid-senior\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"lead\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"with\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"remote\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"onsite\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"mid Level\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"associate\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \", consultant\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"senior Consultant\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"iii\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"ii\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \" , \", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \" intern \", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \" intern\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"junior \", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \" junior\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"campaign \", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"contract: \", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"\\(.*?\\)\", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", f.regexp_replace(f.col(\"name\"), \"sr \", \"\").alias(\"name\")) \\\n",
    "    .withColumn(\"name\", trim(col(\"name\"))).alias(\"name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "career = df.select(df.name)\n",
    "career = career.dropDuplicates(['name']).select('name')\n",
    "career = career.withColumn(\"name\", ltrim(col(\"name\"))).alias(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "career.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":Career\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"timestamp\", f.current_timestamp()).alias(\"timestamp\")\n",
    "jobpost = df.select(df.id, df.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jobpost.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"node.keys\", \"id\")\\\n",
    "    .option(\"labels\",\":JobPosting\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"relationship\", 'ABOUT')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':JobPosting')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Career')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 01:20:22.825377: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-19 01:20:23.540882: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-19 01:20:23.540911: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-19 01:20:25.526377: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-19 01:20:25.526898: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-19 01:20:25.526925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from transformers import BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 01:20:28.941536: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: /home/worker2/Desktop/thesis/NER/NERModel_config/converting/saved_model/1\n",
      "2023-02-19 01:20:28.991887: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }\n",
      "2023-02-19 01:20:28.991970: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: /home/worker2/Desktop/thesis/NER/NERModel_config/converting/saved_model/1\n",
      "2023-02-19 01:20:28.992078: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-19 01:20:29.108181: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2023-02-19 01:20:29.123286: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2096000000 Hz\n",
      "2023-02-19 01:20:29.548276: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2023-02-19 01:20:30.852847: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /home/worker2/Desktop/thesis/NER/NERModel_config/converting/saved_model/1\n",
      "2023-02-19 01:20:31.045447: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 2103935 microseconds.\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "MODEL_NAME = '/home/worker2/Desktop/thesis/NER/NERModel_config'\n",
    "bert = BertForTokenClassification.loadSavedModel(\n",
    "     '{}/converting/saved_model/1'.format(MODEL_NAME),\n",
    "     spark\n",
    " )\\\n",
    " .setInputCols([\"document\",'token'])\\\n",
    " .setOutputCol(\"ner\")\\\n",
    " .setCaseSensitive(True)\\\n",
    " .setMaxSentenceLength(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bert.write().overwrite().save(\"./{}\".format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 01:20:48.182173: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "tokenClassifier_loaded = BertForTokenClassification.load(\"./{}\".format(MODEL_NAME))\\\n",
    "  .setInputCols([\"document\",'token'])\\\n",
    "  .setOutputCol(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-TOOL',\n",
       " 'B-TOOL',\n",
       " 'I-KNOW',\n",
       " '[SEP]',\n",
       " 'B-LANG',\n",
       " 'I-LANG',\n",
       " 'B-FRAM',\n",
       " 'I-FRAM',\n",
       " 'B-KNOW',\n",
       " 'I-PLAT',\n",
       " '[CLS]',\n",
       " 'O',\n",
       " 'B-PLAT']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenClassifier_loaded.getClasses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('description') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "converter = NerConverter()\\\n",
    "    .setInputCols([\"document\",\"token\",\"ner\"])\\\n",
    "    .setOutputCol(\"ner_span\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    tokenizer,\n",
    "    tokenClassifier_loaded,\n",
    "    converter\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(df.description.alias('description'))\n",
    "df = df.withColumn('description', lower(col('description'))) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \"[\\n\\r]\", \"\")) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \"®\", \"\")) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \"'s\", \"\")) \\\n",
    "        .withColumn('description', f.regexp_replace(col(\"description\"), \";\", \" \")) \\\n",
    "        .withColumn('description', f.decode('description','UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col, monotonically_increasing_id\n",
    "result = result.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "result_last = result.select(F.explode(F.arrays_zip(\"ner_span.result\",\"ner_span.metadata\")).alias(\"entities\"), 'id') \\\n",
    ".select(F.expr(\"entities['result']\").alias(\"name\"), \n",
    "        F.expr(\"entities['metadata'].entity\").alias(\"entity\"), 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_last = result_last.orderBy(\"id\").where(result_last.entity != 'EP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "programmingLanguage = result_last.distinct().select(result_last.name).where(result_last.entity == 'LANG')\n",
    "programmingLanguage.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":ProgrammingLanguage\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tool = result_last.distinct().select(result_last.name).where(result_last.entity == 'TOOL')\n",
    "tool.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":Tool\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "framework = result_last.distinct().select(result_last.name).where(result_last.entity == 'FRAM')\n",
    "framework.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":Framework\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "platform = result_last.distinct().select(result_last.name).where(result_last.entity == 'PLAT')\n",
    "platform.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":Platform\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "knowledge = result_last.distinct().select(result_last.name).where(result_last.entity == 'KNOW')\n",
    "knowledge.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"node.keys\", \"name\")\\\n",
    "    .option(\"labels\",\":Knowledge\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobpost_lo = result_last.distinct().select(result_last.name, result_last.entity,result_last.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jobpost_pl = jobpost_lo.select(jobpost_lo.name, jobpost_lo.id).where(jobpost_lo.entity == 'LANG')\n",
    "jobpost_pl.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"relationship\", 'NEED_PL')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':JobPosting')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':ProgrammingLanguage')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jobpost_tool = jobpost_lo.select(jobpost_lo.name, jobpost_lo.id).where(jobpost_lo.entity == 'TOOL')\n",
    "jobpost_tool.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"relationship\", 'NEED_TOOL')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':JobPosting')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Tool')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jobpost_fram = jobpost_lo.select(jobpost_lo.name, jobpost_lo.id).where(jobpost_lo.entity == 'FRAM')\n",
    "jobpost_fram.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"relationship\", 'NEED_FRAM')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':JobPosting')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Framework')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jobpost_plat = jobpost_lo.select(jobpost_lo.name, jobpost_lo.id).where(jobpost_lo.entity == 'PLAT')\n",
    "jobpost_plat.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"relationship\", 'NEED_PLAT')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':JobPosting')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Platform')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jobpost_know = jobpost_lo.select(jobpost_lo.name, jobpost_lo.id).where(jobpost_lo.entity == 'KNOW')\n",
    "jobpost_know.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"relationship\", 'NEED_KNOW')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':JobPosting')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Knowledge')\\\n",
    "    .option('relationship.target.node.keys','name:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4506.5916039943695 seconds\n"
     ]
    }
   ],
   "source": [
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
