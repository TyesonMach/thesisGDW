{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[ WARN] Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='neo4j://localhost:7692'\n",
    "password = 'password'\n",
    "checkpoint1 = \"/home/worker2/Desktop/thesis/K18/File_K18/sample_crawl_dataset/checkpoint/checkpoint1\"\n",
    "checkpoint2 = \"/home/worker2/Desktop/thesis/K18/File_K18/sample_crawl_dataset/checkpoint/checkpoint2\"\n",
    "checkpoint3 = \"/home/worker2/Desktop/thesis/K18/File_K18/sample_crawl_dataset/checkpoint/checkpoint3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"id INT,username STRING,email STRING,career STRING,skills STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.option('header','true') \\\n",
    "    .format('csv') \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option('inferSchema', 'True') \\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .load('/home/worker2/Desktop/thesis/K18/File_K18/sample_crawl_dataset/user/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = user_info.writeStream.format(\"console\").start()\n",
    "#query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "user_info = df.select(df.id,df.username, df.email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "query1 = user_info.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"checkpointLocation\", checkpoint1) \\\n",
    "    .option(\"node.keys\", \"id\")\\\n",
    "    .option(\"labels\",\":User\")\\\n",
    "    .start()\n",
    "#query1.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_favor = df.select(df.id, df.career)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query2 =user_favor.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"checkpointLocation\", checkpoint2) \\\n",
    "    .option(\"relationship\", 'FAVOR_IN')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':User')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Career')\\\n",
    "    .option('relationship.target.node.keys','career:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_skills= df.select(df.id, split(df.skills,\",\").alias(\"skills\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_skills = user_skills.select(user_skills.id, explode(user_skills.skills).alias(\"skills\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARN] spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query3 = user_skills.writeStream.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"save.mode\", \"Overwrite\") \\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"checkpointLocation\", checkpoint3) \\\n",
    "    .option(\"relationship\", 'HAVE_SKILL')\\\n",
    "    .option('relationship.save.strategy','keys')\\\n",
    "    .option('relationship.source.labels',':User')\\\n",
    "    .option('relationship.source.node.keys','id:id')\\\n",
    "    .option('relationship.source.save.mode','Match')\\\n",
    "    .option('relationship.target.labels',':Competency')\\\n",
    "    .option('relationship.target.node.keys','skills:name')\\\n",
    "    .option('relationship.target.save.mode','Match')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query1.awaitTermination()\n",
    "query2.awaitTermination()\n",
    "query3.awaitTermination()\n",
    "\n",
    "#spark.streams.awaitAnyTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
