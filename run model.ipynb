{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vXYNX2lQROB"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BertForTokenClassification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zva6MvJyLeWi"
      },
      "source": [
        "## Import BertForTokenClassification models from HuggingFace 🤗  into Spark NLP 🚀 \n",
        "\n",
        "Let's keep in mind a few things before we start 😊 \n",
        "\n",
        "- This feature is only in `Spark NLP 3.2.x` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import BERT models trained/fine-tuned for token classification via `BertForTokenClassification` or `TFBertForTokenClassification`. These models are usually under `Token Classification` category and have `bert` in their labels\n",
        "- Reference: [TFBertForTokenClassification](https://huggingface.co/transformers/model_doc/bert.html#tfbertfortokenclassification)\n",
        "- Some [example models](https://huggingface.co/models?filter=bert&pipeline_tag=token-classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzxB-Nq6cxOA"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNQkhyMHMgkE"
      },
      "source": [
        "- Let's install `HuggingFace` and `TensorFlow`. You don't need `TensorFlow` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock TensorFlow on `2.11.0` version and Transformers on `4.25.1`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hHXgqiWpMfCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0ddfaa-52b3-45e7-959b-206caa44318e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.8/dist-packages (0.19.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "fatal: destination path 'onnx-tensorflow' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "Installing collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for pytorch\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Running setup.py install for pytorch ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
            "\u001b[31m╰─>\u001b[0m pytorch\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.13.1+cu116)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.25.1 tensorflow==2.11.0\n",
        "!pip install tensorflow-addons\n",
        "!git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow && pip install -e . \n",
        "!pip install pytorch\n",
        "!pip install torchvision\n",
        "!pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect ggdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSJIEn9lk8BE",
        "outputId": "3066505b-461d-43f9-8c8f-e054da86282d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3AM6bj4P3NS"
      },
      "source": [
        "- HuggingFace comes with a native `saved_model` feature inside `save_pretrained` function for TensorFlow based models. We will use that to save it as TF `SavedModel`.\n",
        "- We'll use [dslim/bert-base-NER](https://huggingface.co/dslim/bert-base-NER) model from HuggingFace as an example\n",
        "- In addition to `TFBertForTokenClassification` we also need to save the `BertTokenizer`. This is the same for every model, these are assets needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pprint import pprint\n",
        "import string    \n",
        "import random\n",
        "import json\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "#from transformers import BertTokenizer, BertForTokenClassification\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TFAutoModelForTokenClassification"
      ],
      "metadata": {
        "id": "VXuGsv8Q1ibh"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_address = '/content/drive/MyDrive/Data_Science/thesis/ML_NER/NERModel_config'\n",
        "#save_model = BertForTokenClassification.from_pretrained(save_model_address, num_labels=20)\n",
        "#tokenizer = BertTokenizer.from_pretrained(save_model_address,do_lower_case=True)\n",
        "\n",
        "save_model = TFAutoModelForTokenClassification.from_pretrained(save_model_address, from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_model_address, do_lower_case=True, model_max_length=256)\n",
        "\n",
        "nlp = pipeline(\"ner\", model=save_model, tokenizer=tokenizer, aggregation_strategy='simple',ignore_labels =['X','O'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fsAsozAFfsg",
        "outputId": "2321f5a5-8f59-42db-dc03-b9af2ab92d70"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test cau dai\n",
        "orig_string = '''Learn and Master Software Testing Quickly from the experts - GUARANTEED! THE IN-DEPTH SOFTWARE TESTING TRAINING - By SoftwareTestingHelp Team. \"TOP STUDENT PICK\" on Udemy in the Software Testing category! 26+ hours of HD content. Value for money! DON'T settle for other basic courses of less thanhours! Few Student reviews from hundreds ofstar reviews: \"The course is an eye opener into the world of IT. Theophilus. \"Money well spent, excellent delivery. Very informative and practical. Would highly recommend to anyone interested in pursuing software testing as a career. Olanrewaju. \"Truly the best software testing training I have come across both in dept and in substance. Kingsley. \"This is really \"The Best Software Training Course\". I hardly know anything regarding testing, instructor had taken utmost care in providing the knowledge starting from basics, the terminology etc...I am very much satisfied with this course. I strongly recommend this course. Vijaya. \"Great tutorials ..in detail ...learned a lot ...must see tutorial for all testers. Masud. \"The instructor is just a perfect lecturer! Entire course is very informative and useful for software testers as beginners with a lot of practical examples. Who wants to understand principles of testing and main techniques of it - enroll in this course. Oleksii. \"The instructor according to me.....God has gifted her a real talent to be one of the best tutors in this world. Biju. Introducing the Most Practical, Precise and Inexpensive Software Testing Course. It is going to include everything there is to know for you to become a perfect Software Tester. This software testing QA training course is designed by working professionals in a way that, course it will progress from introducing you to the basics of software testing to advanced topics like Software configuration management, creating a test plan, test estimations etc along with introduction and familiarity with Automation testing and test management tools like QTP (intro), QC, JIRA, and Bugzilla. Course Benefits: Syllabus: We came up with a unique list of topics that will help you gradually work your way into the testing world. Practice sessions: Assignments in a way that you will get to apply the theory you learnt immediately. Video sessions of Instructor led live training sessions. Practical learning experienc e with live project work and examples. Support: Our Team is going to be available to you via email or the website for you to reach out to us. Over Lectures and more than+ hours of HD content! Learn Software Testing and Automation basics from a professional trainer from your own desk. Information packed practical training starting from basics to advanced testing techniques. Best suitable for beginners to advanced level users and who learn faster when demonstrated. Get â€œCertificate of completion. LIVE PROJECT End to End Software Testing Training Included. Learn Software Testing and Automation basics from a professional trainer from your own desk. Information packed practical training starting from basics to advanced testing techniques. Best suitable for beginners to advanced level users and who learn faster when demonstrated. Course content designed by considering current software testing technology and the job market. Practical assignments at the end of every session. Practical learning experience with live project work and examples. Lifetime enrollment - Pay one time fee and access video training sessions as many times as you want. Resume Preparation Guidance for Testers Included. Software Testing Interview Questions and Preparation Tips Included. Download Real Software Testing Templates like Test Plan, Test Cases and other important Templates. Software Testing Certification Guidance. Learn Test Management Tools like JIRA, and Bugzilla. Get all future course updates free!'''\n",
        "#results = nlp(sentences)\n",
        "#results\n",
        "#len(tokenizer.tokenize(sentences, truncation=True))\n",
        "list_of_lines = []\n",
        "max_length = 256*4\n",
        "while len(orig_string) > max_length:\n",
        "    line_length = max(orig_string[:max_length].rfind(i) for i in \".!?,\")\n",
        "    list_of_lines.append(orig_string[:line_length])\n",
        "    orig_string = orig_string[line_length + 1:]\n",
        "list_of_lines.append(orig_string)\n",
        "list_of_lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR0JRrEU1Wmj",
        "outputId": "3ca108e7-34f9-48a5-cb42-9dc6ea560f63"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Learn and Master Software Testing Quickly from the experts - GUARANTEED! THE IN-DEPTH SOFTWARE TESTING TRAINING - By SoftwareTestingHelp Team. \"TOP STUDENT PICK\" on Udemy in the Software Testing category! 26+ hours of HD content. Value for money! DON\\'T settle for other basic courses of less thanhours! Few Student reviews from hundreds ofstar reviews: \"The course is an eye opener into the world of IT. Theophilus. \"Money well spent, excellent delivery. Very informative and practical. Would highly recommend to anyone interested in pursuing software testing as a career. Olanrewaju. \"Truly the best software testing training I have come across both in dept and in substance. Kingsley. \"This is really \"The Best Software Training Course\". I hardly know anything regarding testing, instructor had taken utmost care in providing the knowledge starting from basics, the terminology etc...I am very much satisfied with this course. I strongly recommend this course. Vijaya. \"Great tutorials ..in detail ...learned a lot ..',\n",
              " 'must see tutorial for all testers. Masud. \"The instructor is just a perfect lecturer! Entire course is very informative and useful for software testers as beginners with a lot of practical examples. Who wants to understand principles of testing and main techniques of it - enroll in this course. Oleksii. \"The instructor according to me.....God has gifted her a real talent to be one of the best tutors in this world. Biju. Introducing the Most Practical, Precise and Inexpensive Software Testing Course. It is going to include everything there is to know for you to become a perfect Software Tester. This software testing QA training course is designed by working professionals in a way that, course it will progress from introducing you to the basics of software testing to advanced topics like Software configuration management, creating a test plan, test estimations etc along with introduction and familiarity with Automation testing and test management tools like QTP (intro), QC, JIRA, and Bugzilla',\n",
              " ' Course Benefits: Syllabus: We came up with a unique list of topics that will help you gradually work your way into the testing world. Practice sessions: Assignments in a way that you will get to apply the theory you learnt immediately. Video sessions of Instructor led live training sessions. Practical learning experienc e with live project work and examples. Support: Our Team is going to be available to you via email or the website for you to reach out to us. Over Lectures and more than+ hours of HD content! Learn Software Testing and Automation basics from a professional trainer from your own desk. Information packed practical training starting from basics to advanced testing techniques. Best suitable for beginners to advanced level users and who learn faster when demonstrated. Get â€œCertificate of completion. LIVE PROJECT End to End Software Testing Training Included. Learn Software Testing and Automation basics from a professional trainer from your own desk',\n",
              " ' Information packed practical training starting from basics to advanced testing techniques. Best suitable for beginners to advanced level users and who learn faster when demonstrated. Course content designed by considering current software testing technology and the job market. Practical assignments at the end of every session. Practical learning experience with live project work and examples. Lifetime enrollment - Pay one time fee and access video training sessions as many times as you want. Resume Preparation Guidance for Testers Included. Software Testing Interview Questions and Preparation Tips Included. Download Real Software Testing Templates like Test Plan, Test Cases and other important Templates. Software Testing Certification Guidance. Learn Test Management Tools like JIRA, and Bugzilla. Get all future course updates free!']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix ky tu bang thu vien fix that for you\n",
        "import ftfy\n",
        "results = nlp(ftfy.fix_text(list_of_lines[0]))\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA8Tq5eADEiv",
        "outputId": "c78e6889-43c0-4884-e69d-407c9f6daf54"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'B-KNOW',\n",
              "  'score': 0.91182387,\n",
              "  'index': 4,\n",
              "  'word': 'software',\n",
              "  'start': 17,\n",
              "  'end': 25},\n",
              " {'entity': 'I-KNOW',\n",
              "  'score': 0.89576316,\n",
              "  'index': 5,\n",
              "  'word': 'testing',\n",
              "  'start': 26,\n",
              "  'end': 33},\n",
              " {'entity': 'B-KNOW',\n",
              "  'score': 0.8580251,\n",
              "  'index': 17,\n",
              "  'word': 'software',\n",
              "  'start': 86,\n",
              "  'end': 94},\n",
              " {'entity': 'I-KNOW',\n",
              "  'score': 0.8224982,\n",
              "  'index': 18,\n",
              "  'word': 'testing',\n",
              "  'start': 95,\n",
              "  'end': 102},\n",
              " {'entity': 'B-KNOW',\n",
              "  'score': 0.81303245,\n",
              "  'index': 116,\n",
              "  'word': 'software',\n",
              "  'start': 543,\n",
              "  'end': 551},\n",
              " {'entity': 'I-KNOW',\n",
              "  'score': 0.9022264,\n",
              "  'index': 117,\n",
              "  'word': 'testing',\n",
              "  'start': 552,\n",
              "  'end': 559},\n",
              " {'entity': 'B-KNOW',\n",
              "  'score': 0.8579681,\n",
              "  'index': 132,\n",
              "  'word': 'software',\n",
              "  'start': 601,\n",
              "  'end': 609},\n",
              " {'entity': 'I-KNOW',\n",
              "  'score': 0.8925069,\n",
              "  'index': 133,\n",
              "  'word': 'testing',\n",
              "  'start': 610,\n",
              "  'end': 617}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fixed = ftfy.fix_text(list_of_lines[0])\n",
        "fixed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "Nyz_r2XvJx6a",
        "outputId": "7d06f1b3-5038-40d6-820f-7347dd527474"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Learn and Master Software Testing Quickly from the experts - GUARANTEED! THE IN-DEPTH SOFTWARE TESTING TRAINING - By SoftwareTestingHelp Team. \"TOP STUDENT PICK\" on Udemy in the Software Testing category! 26+ hours of HD content. Value for money! DON\\'T settle for other basic courses of less thanhours! Few Student reviews from hundreds ofstar reviews: \"The course is an eye opener into the world of IT. Theophilus. \"Money well spent, excellent delivery. Very informative and practical. Would highly recommend to anyone interested in pursuing software testing as a career. Olanrewaju. \"Truly the best software testing training I have come across both in dept and in substance. Kingsley. \"This is really \"The Best Software Training Course\". I hardly know anything regarding testing, instructor had taken utmost care in providing the knowledge starting from basics, the terminology etc...I am very much satisfied with this course. I strongly recommend this course. Vijaya. \"Great tutorials ..in detail ...learned a lot ..'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\n",
        "    fixed, \n",
        "    return_attention_mask=False,\n",
        "    truncation=True,\n",
        "    return_special_tokens_mask=True,\n",
        "    return_offsets_mapping=tokenizer.is_fast,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6b1vY1GNgO0",
        "outputId": "9fa9f5d0-85c7-4692-b0fb-1f1623cf9aab"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  4553,  1998,  3040,  4007,  5604,  2855,  2013,  1996,  8519,\n",
              "          1011, 12361,   999,  1996,  1999,  1011,  5995,  4007,  5604,  2731,\n",
              "          1011,  2011,  4007, 22199,  2075, 16001,  2361,  2136,  1012,  1000,\n",
              "          2327,  3076,  4060,  1000,  2006, 20904, 26662,  1999,  1996,  4007,\n",
              "          5604,  4696,   999,  2656,  1009,  2847,  1997, 10751,  4180,  1012,\n",
              "          3643,  2005,  2769,   999,  2123,  1005,  1056,  7392,  2005,  2060,\n",
              "          3937,  5352,  1997,  2625,  2084,  6806,  9236,   999,  2261,  3076,\n",
              "          4391,  2013,  5606,  1997, 14117,  4391,  1024,  1000,  1996,  2607,\n",
              "          2003,  2019,  3239, 16181,  2046,  1996,  2088,  1997,  2009,  1012,\n",
              "         14833, 21850,  7393,  1012,  1000,  2769,  2092,  2985,  1010,  6581,\n",
              "          6959,  1012,  2200, 12367,  8082,  1998,  6742,  1012,  2052,  3811,\n",
              "         16755,  2000,  3087,  4699,  1999, 11828,  4007,  5604,  2004,  1037,\n",
              "          2476,  1012, 19330,  2319, 15603, 13006,  2226,  1012,  1000,  5621,\n",
              "          1996,  2190,  4007,  5604,  2731,  1045,  2031,  2272,  2408,  2119,\n",
              "          1999, 29466,  1998,  1999,  9415,  1012, 22819,  1012,  1000,  2023,\n",
              "          2003,  2428,  1000,  1996,  2190,  4007,  2731,  2607,  1000,  1012,\n",
              "          1045,  6684,  2113,  2505,  4953,  5604,  1010,  9450,  2018,  2579,\n",
              "         27917,  2729,  1999,  4346,  1996,  3716,  3225,  2013, 24078,  1010,\n",
              "          1996, 18444,  4385,  1012,  1012,  1012,  1045,  2572,  2200,  2172,\n",
              "          8510,  2007,  2023,  2607,  1012,  1045,  6118, 16755,  2023,  2607,\n",
              "          1012, 17027,  2050,  1012,  1000,  2307, 14924, 26340,  1012,  1012,\n",
              "          1999,  6987,  1012,  1012,  1012,  4342,  1037,  2843,  1012,  1012,\n",
              "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 1]]), 'offset_mapping': tensor([[[   0,    0],\n",
              "         [   0,    5],\n",
              "         [   6,    9],\n",
              "         [  10,   16],\n",
              "         [  17,   25],\n",
              "         [  26,   33],\n",
              "         [  34,   41],\n",
              "         [  42,   46],\n",
              "         [  47,   50],\n",
              "         [  51,   58],\n",
              "         [  59,   60],\n",
              "         [  61,   71],\n",
              "         [  71,   72],\n",
              "         [  73,   76],\n",
              "         [  77,   79],\n",
              "         [  79,   80],\n",
              "         [  80,   85],\n",
              "         [  86,   94],\n",
              "         [  95,  102],\n",
              "         [ 103,  111],\n",
              "         [ 112,  113],\n",
              "         [ 114,  116],\n",
              "         [ 117,  125],\n",
              "         [ 125,  129],\n",
              "         [ 129,  132],\n",
              "         [ 132,  135],\n",
              "         [ 135,  136],\n",
              "         [ 137,  141],\n",
              "         [ 141,  142],\n",
              "         [ 143,  144],\n",
              "         [ 144,  147],\n",
              "         [ 148,  155],\n",
              "         [ 156,  160],\n",
              "         [ 160,  161],\n",
              "         [ 162,  164],\n",
              "         [ 165,  167],\n",
              "         [ 167,  170],\n",
              "         [ 171,  173],\n",
              "         [ 174,  177],\n",
              "         [ 178,  186],\n",
              "         [ 187,  194],\n",
              "         [ 195,  203],\n",
              "         [ 203,  204],\n",
              "         [ 205,  207],\n",
              "         [ 207,  208],\n",
              "         [ 209,  214],\n",
              "         [ 215,  217],\n",
              "         [ 218,  220],\n",
              "         [ 221,  228],\n",
              "         [ 228,  229],\n",
              "         [ 230,  235],\n",
              "         [ 236,  239],\n",
              "         [ 240,  245],\n",
              "         [ 245,  246],\n",
              "         [ 247,  250],\n",
              "         [ 250,  251],\n",
              "         [ 251,  252],\n",
              "         [ 253,  259],\n",
              "         [ 260,  263],\n",
              "         [ 264,  269],\n",
              "         [ 270,  275],\n",
              "         [ 276,  283],\n",
              "         [ 284,  286],\n",
              "         [ 287,  291],\n",
              "         [ 292,  296],\n",
              "         [ 296,  298],\n",
              "         [ 298,  301],\n",
              "         [ 301,  302],\n",
              "         [ 303,  306],\n",
              "         [ 307,  314],\n",
              "         [ 315,  322],\n",
              "         [ 323,  327],\n",
              "         [ 328,  336],\n",
              "         [ 337,  339],\n",
              "         [ 339,  343],\n",
              "         [ 344,  351],\n",
              "         [ 351,  352],\n",
              "         [ 353,  354],\n",
              "         [ 354,  357],\n",
              "         [ 358,  364],\n",
              "         [ 365,  367],\n",
              "         [ 368,  370],\n",
              "         [ 371,  374],\n",
              "         [ 375,  381],\n",
              "         [ 382,  386],\n",
              "         [ 387,  390],\n",
              "         [ 391,  396],\n",
              "         [ 397,  399],\n",
              "         [ 400,  402],\n",
              "         [ 402,  403],\n",
              "         [ 404,  408],\n",
              "         [ 408,  411],\n",
              "         [ 411,  414],\n",
              "         [ 414,  415],\n",
              "         [ 416,  417],\n",
              "         [ 417,  422],\n",
              "         [ 423,  427],\n",
              "         [ 428,  433],\n",
              "         [ 433,  434],\n",
              "         [ 435,  444],\n",
              "         [ 445,  453],\n",
              "         [ 453,  454],\n",
              "         [ 455,  459],\n",
              "         [ 460,  466],\n",
              "         [ 466,  471],\n",
              "         [ 472,  475],\n",
              "         [ 476,  485],\n",
              "         [ 485,  486],\n",
              "         [ 487,  492],\n",
              "         [ 493,  499],\n",
              "         [ 500,  509],\n",
              "         [ 510,  512],\n",
              "         [ 513,  519],\n",
              "         [ 520,  530],\n",
              "         [ 531,  533],\n",
              "         [ 534,  542],\n",
              "         [ 543,  551],\n",
              "         [ 552,  559],\n",
              "         [ 560,  562],\n",
              "         [ 563,  564],\n",
              "         [ 565,  571],\n",
              "         [ 571,  572],\n",
              "         [ 573,  575],\n",
              "         [ 575,  577],\n",
              "         [ 577,  580],\n",
              "         [ 580,  582],\n",
              "         [ 582,  583],\n",
              "         [ 583,  584],\n",
              "         [ 585,  586],\n",
              "         [ 586,  591],\n",
              "         [ 592,  595],\n",
              "         [ 596,  600],\n",
              "         [ 601,  609],\n",
              "         [ 610,  617],\n",
              "         [ 618,  626],\n",
              "         [ 627,  628],\n",
              "         [ 629,  633],\n",
              "         [ 634,  638],\n",
              "         [ 639,  645],\n",
              "         [ 646,  650],\n",
              "         [ 651,  653],\n",
              "         [ 654,  658],\n",
              "         [ 659,  662],\n",
              "         [ 663,  665],\n",
              "         [ 666,  675],\n",
              "         [ 675,  676],\n",
              "         [ 677,  685],\n",
              "         [ 685,  686],\n",
              "         [ 687,  688],\n",
              "         [ 688,  692],\n",
              "         [ 693,  695],\n",
              "         [ 696,  702],\n",
              "         [ 703,  704],\n",
              "         [ 704,  707],\n",
              "         [ 708,  712],\n",
              "         [ 713,  721],\n",
              "         [ 722,  730],\n",
              "         [ 731,  737],\n",
              "         [ 737,  738],\n",
              "         [ 738,  739],\n",
              "         [ 740,  741],\n",
              "         [ 742,  748],\n",
              "         [ 749,  753],\n",
              "         [ 754,  762],\n",
              "         [ 763,  772],\n",
              "         [ 773,  780],\n",
              "         [ 780,  781],\n",
              "         [ 782,  792],\n",
              "         [ 793,  796],\n",
              "         [ 797,  802],\n",
              "         [ 803,  809],\n",
              "         [ 810,  814],\n",
              "         [ 815,  817],\n",
              "         [ 818,  827],\n",
              "         [ 828,  831],\n",
              "         [ 832,  841],\n",
              "         [ 842,  850],\n",
              "         [ 851,  855],\n",
              "         [ 856,  862],\n",
              "         [ 862,  863],\n",
              "         [ 864,  867],\n",
              "         [ 868,  879],\n",
              "         [ 880,  883],\n",
              "         [ 883,  884],\n",
              "         [ 884,  885],\n",
              "         [ 885,  886],\n",
              "         [ 886,  887],\n",
              "         [ 888,  890],\n",
              "         [ 891,  895],\n",
              "         [ 896,  900],\n",
              "         [ 901,  910],\n",
              "         [ 911,  915],\n",
              "         [ 916,  920],\n",
              "         [ 921,  927],\n",
              "         [ 927,  928],\n",
              "         [ 929,  930],\n",
              "         [ 931,  939],\n",
              "         [ 940,  949],\n",
              "         [ 950,  954],\n",
              "         [ 955,  961],\n",
              "         [ 961,  962],\n",
              "         [ 963,  968],\n",
              "         [ 968,  969],\n",
              "         [ 969,  970],\n",
              "         [ 971,  972],\n",
              "         [ 972,  977],\n",
              "         [ 978,  983],\n",
              "         [ 983,  987],\n",
              "         [ 988,  989],\n",
              "         [ 989,  990],\n",
              "         [ 990,  992],\n",
              "         [ 993,  999],\n",
              "         [1000, 1001],\n",
              "         [1001, 1002],\n",
              "         [1002, 1003],\n",
              "         [1003, 1010],\n",
              "         [1011, 1012],\n",
              "         [1013, 1016],\n",
              "         [1017, 1018],\n",
              "         [1018, 1019],\n",
              "         [   0,    0]]])}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.is_fast:\n",
        "    offset_mapping = tokens.pop(\"offset_mapping\").cpu().numpy()[0]\n",
        "elif offset_mappings:\n",
        "    offset_mapping = offset_mappings[i]\n",
        "else:\n",
        "    offset_mapping = None\n",
        "\n",
        "special_tokens_mask = tokens.pop(\"special_tokens_mask\").cpu().numpy()[0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    entities = save_model(**tokens)[0][0].cpu().numpy()\n",
        "    input_ids = tokens[\"input_ids\"].cpu().numpy()[0]\n"
      ],
      "metadata": {
        "id": "RwYvSV-JNrs8"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.onnx.export(save_model, **tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "b1w755NbCqrC",
        "outputId": "c1213f68-1af3-4c27-d4fe-f2d52bd3189e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-45f2d3eff90b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: export() got an unexpected keyword argument 'input_ids'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "MODEL_NAME = '/content/drive/MyDrive/Data_Science/thesis/ML_NER/NERModel_config'\n",
        "\n",
        "# Define TF Signature\n",
        "@tf.function(\n",
        "  input_signature=[\n",
        "      {\n",
        "          \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n",
        "          \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n",
        "          \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n",
        "      }\n",
        "  ]\n",
        ")\n",
        "def serving_fn(input):\n",
        "    return save_model(input)\n",
        "\n",
        "save_model.save_pretrained(\"{}/converting\".format(MODEL_NAME), saved_model=True, signatures={\"serving_default\": serving_fn})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arqtR23l1380",
        "outputId": "a2a7995a-f07f-4601-d6ac-30aab25957c7"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, LayerNorm_layer_call_fn while saving (showing 5 of 416). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tree -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNhE3V7g88WR",
        "outputId": "86f58ea2-4d16-4b04-9444-520225045b18"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 43.0 kB of archives.\n",
            "After this operation, 115 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tree amd64 1.8.0-1 [43.0 kB]\n",
            "Fetched 43.0 kB in 0s (94.8 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 129499 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.8.0-1_amd64.deb ...\n",
            "Unpacking tree (1.8.0-1) ...\n",
            "Setting up tree (1.8.0-1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree {MODEL_NAME}/converting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkPcuCBg8z6G",
        "outputId": "df277f93-4982-41c6-b2de-4f744c75bf4e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/drive/MyDrive/Data_Science/thesis/ML_NER/NERModel_config/converting\u001b[00m\n",
            "├── config.json\n",
            "├── \u001b[01;34msaved_model\u001b[00m\n",
            "│   └── \u001b[01;34m1\u001b[00m\n",
            "│       ├── \u001b[01;34massets\u001b[00m\n",
            "│       ├── fingerprint.pb\n",
            "│       ├── keras_metadata.pb\n",
            "│       ├── saved_model.pb\n",
            "│       └── \u001b[01;34mvariables\u001b[00m\n",
            "│           ├── variables.data-00000-of-00001\n",
            "│           └── variables.index\n",
            "└── tf_model.h5\n",
            "\n",
            "4 directories, 7 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree {MODEL_NAME}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxkmPxl0VPpv",
        "outputId": "a30c54ed-cb14-4db3-ffa2-05a0073cd561"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/drive/MyDrive/Data_Science/thesis/ML_NER/NERModel_config\u001b[00m\n",
            "├── config.json\n",
            "├── \u001b[01;34mconverting\u001b[00m\n",
            "│   ├── config.json\n",
            "│   ├── \u001b[01;34msaved_model\u001b[00m\n",
            "│   │   └── \u001b[01;34m1\u001b[00m\n",
            "│   │       ├── \u001b[01;34massets\u001b[00m\n",
            "│   │       ├── fingerprint.pb\n",
            "│   │       ├── keras_metadata.pb\n",
            "│   │       ├── saved_model.pb\n",
            "│   │       └── \u001b[01;34mvariables\u001b[00m\n",
            "│   │           ├── variables.data-00000-of-00001\n",
            "│   │           └── variables.index\n",
            "│   └── tf_model.h5\n",
            "├── eval_results.txt\n",
            "├── \u001b[01;34mmy_model_tf\u001b[00m\n",
            "│   └── \u001b[01;34msaved_model\u001b[00m\n",
            "│       └── \u001b[01;34m1\u001b[00m\n",
            "│           ├── \u001b[01;34massets\u001b[00m\n",
            "│           │   ├── labels.txt\n",
            "│           │   └── vocab.txt\n",
            "│           ├── saved_model.pd\n",
            "│           └── \u001b[01;34mvariables\u001b[00m\n",
            "├── pytorch_model.bin\n",
            "└── vocab.txt\n",
            "\n",
            "10 directories, 14 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r {MODEL_NAME}/my_model_tf/saved_model/1/assets {MODEL_NAME}/converting/saved_model/1"
      ],
      "metadata": {
        "id": "Q_yqGgWKVRRk"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tree {MODEL_NAME}/converting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDPcaHZN3I2W",
        "outputId": "2d340fbe-af26-44f1-8b08-57da9ab684c4"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/drive/MyDrive/Data_Science/thesis/ML_NER/NERModel_config/converting\u001b[00m\n",
            "├── config.json\n",
            "├── \u001b[01;34msaved_model\u001b[00m\n",
            "│   └── \u001b[01;34m1\u001b[00m\n",
            "│       ├── \u001b[01;34massets\u001b[00m\n",
            "│       │   ├── labels.txt\n",
            "│       │   └── vocab.txt\n",
            "│       ├── fingerprint.pb\n",
            "│       ├── keras_metadata.pb\n",
            "│       ├── saved_model.pb\n",
            "│       └── \u001b[01;34mvariables\u001b[00m\n",
            "│           ├── variables.data-00000-of-00001\n",
            "│           └── variables.index\n",
            "└── tf_model.h5\n",
            "\n",
            "4 directories, 9 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFcmW0fdVghJ",
        "outputId": "474f4a94-98c3-4a88-ea27-990e1a18c680"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing PySpark 3.2.3 and Spark NLP 4.2.8\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 4.2.8\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "# let's start Spark with Spark NLP\n",
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "NRM3cr3QVioV"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import *\n",
        "\n",
        "bert = BertForTokenClassification.loadSavedModel(\n",
        "     '{}/converting/saved_model/1'.format(MODEL_NAME),\n",
        "     spark\n",
        " )\\\n",
        " .setInputCols([\"document\",'token'])\\\n",
        " .setOutputCol(\"ner\")\\\n",
        " .setCaseSensitive(True)\\\n",
        " .setMaxSentenceLength(128)"
      ],
      "metadata": {
        "id": "Iz1p_xME3Q2F"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert.write().overwrite().save(\"./{}\".format(MODEL_NAME))"
      ],
      "metadata": {
        "id": "5RZIx0Y7X0mR"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {MODEL_NAME}_tokenizer {MODEL_NAME}"
      ],
      "metadata": {
        "id": "5vI0v-w6YLL5"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -l {MODEL_NAME}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCo5ZJcjYb3J",
        "outputId": "796a32c6-5521-4052-99f2-0d72bb331726"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 425716\n",
            "-rw------- 1 root root      1136 Sep 26 01:35 config.json\n",
            "drwx------ 3 root root      4096 Jan 30 20:32 converting\n",
            "-rw------- 1 root root       554 Sep 26 01:35 eval_results.txt\n",
            "drwx------ 3 root root      4096 Jan 30 20:32 my_model_tf\n",
            "-rw------- 1 root root 435689969 Sep 26 01:35 pytorch_model.bin\n",
            "-rw------- 1 root root    231508 Sep 26 01:35 vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenClassifier_loaded = BertForTokenClassification.load(\"./{}\".format(MODEL_NAME))\\\n",
        "  .setInputCols([\"document\",'token'])\\\n",
        "  .setOutputCol(\"ner\")"
      ],
      "metadata": {
        "id": "XwP2iZHQZ2Xz"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenClassifier_loaded.getClasses()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSf7AsOtZ-uI",
        "outputId": "9bd89695-d57a-48d7-8790-66733b7ad10a"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I-TOOL',\n",
              " 'B-TOOL',\n",
              " 'I-KNOW',\n",
              " '[SEP]',\n",
              " 'B-LANG',\n",
              " 'I-LANG',\n",
              " 'B-FRAM',\n",
              " 'I-FRAM',\n",
              " 'B-KNOW',\n",
              " 'I-PLAT',\n",
              " '[CLS]',\n",
              " 'O',\n",
              " 'B-PLAT']"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import *\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol('text') \\\n",
        "    .setOutputCol('document')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(['document']) \\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler, \n",
        "    tokenizer,\n",
        "    tokenClassifier_loaded    \n",
        "])\n",
        "\n",
        "# couple of simple examples\n",
        "example = spark.createDataFrame([[\"Learn and Master software testing Quickly from the experts - GUARANTEED! THE IN-DEPTH software testing TRAINING - By SoftwareTestingHelp Team. on Udemy in the software testing category!\"], ['My name is Clara and I live in Berkeley, California.']]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(example).transform(example)\n",
        "\n",
        "# result is a DataFrame\n",
        "result.select(\"text\", \"ner.result\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6eI7HX1aErb",
        "outputId": "804e9f41-3c9f-4ac1-cd9c-3bc920fd3ba0"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|              result|\n",
            "+--------------------+--------------------+\n",
            "|Learn and Master ...|[O, O, O, B-KNOW,...|\n",
            "|My name is Clara ...|[O, O, O, O, O, O...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "transformers",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "59794f394f79a45d9851d6706177d59b9a5e9d735b0369dbae4b76bccf016251"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}